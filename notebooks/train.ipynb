{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ftfy regex tqdm spacy ultralytics\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuela/mambaforge/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Tuple, OrderedDict, Callable\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import pickle\n",
    "import json\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor, tensor, device\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import box_iou, box_convert, generalized_box_iou_loss\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from clip import clip\n",
    "from clip.model import CLIP, ModifiedResNet\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from optuna.visualization import plot_optimization_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset and save under data/raw/ only if not already downloaded\n",
    "url = \"https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\"\n",
    "if not os.path.exists(\"data/refcocog.tar.gz\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(url=url, output=\"data/raw/\", quiet=False, resume=True)\n",
    "if not os.path.exists(\"data/refcocog/\"):\n",
    "    print(\"Extracting dataset...\")\n",
    "    !tar -xf data/raw/refcocog.tar.gz -C data/raw/ --verbose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility classes definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Sample:\n",
    "    image_path: str\n",
    "    caption: str\n",
    "    bounding_box: Tensor\n",
    "\n",
    "\n",
    "class BatchSample:\n",
    "    def __init__(self, image: Tensor, caption: Tensor) -> None:\n",
    "        self.image: Tensor = image\n",
    "        self.caption: Tensor = caption\n",
    "\n",
    "    def to(self, device: device | str) -> Any:\n",
    "        return self.__class__(self.image.to(device), self.caption.to(device))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BatchSample(image={self.image.shape}, caption={self.caption.shape})\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Result:\n",
    "    bounding_box: Tensor\n",
    "    score: Tensor\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BboxType(Enum):\n",
    "    XYXY = \"xyxy\"\n",
    "    XYWH = \"xywh\"\n",
    "    CXCWH = \"cxcwh\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return super().__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Singleton:\n",
    "    def __init__(self, decorated_class: Any) -> None:\n",
    "        self._decorated = decorated_class\n",
    "\n",
    "    def get_instance(self) -> Any:\n",
    "        \"\"\"\n",
    "        Returns the singleton instance. Upon its first call, it creates a\n",
    "        new instance of the decorated class and calls its `__init__` method.\n",
    "        On all subsequent calls, the already created instance is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._instance  # type: ignore\n",
    "        except AttributeError:\n",
    "            self._instance = self._decorated()\n",
    "            return self._instance\n",
    "\n",
    "    def __call__(self) -> None:\n",
    "        raise TypeError(\"Singletons must be accessed through get_instance() method.\")\n",
    "\n",
    "    def __instancecheck__(self, inst: Any) -> bool:\n",
    "        return isinstance(inst, self._decorated)\n",
    "\n",
    "\n",
    "# All configurations are stored ina json file and loaded here only once\n",
    "# All other times the class is called the same instance is returned\n",
    "\n",
    "\n",
    "@Singleton\n",
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        with open(file=\"config.json\", mode=\"r\") as fp:\n",
    "            cfg: Dict[str, Any] = json.load(fp=fp)\n",
    "        for k, v in cfg.items():\n",
    "            setattr(self, k, v)\n",
    "        # self.__dict__.update(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item of the dataset is a tuple of (BatchSample, Tensor) where the tensor is the ground truth bounding box\n",
    "class VGDataset(Dataset[Tuple[BatchSample, Tensor]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        split: Split,\n",
    "        output_bbox_type: BboxType,\n",
    "        transform_image: Any = None,\n",
    "        dependencies: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dir_path: str = dir_path\n",
    "        self.split: Split = split\n",
    "        self.output_bbox_type: BboxType = output_bbox_type\n",
    "        self.transform_image = transform_image\n",
    "        self.text_processor = spacy.load(name=\"en_core_web_lg\")\n",
    "        self.device: device = torch.device(\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.samples: List[Sample] = self.get_samples(dependencies=dependencies)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, ref_id: int) -> Tuple[BatchSample, Tensor]:\n",
    "        caption: Tensor = clip.tokenize(self.samples[ref_id].caption)\n",
    "\n",
    "        if self.transform_image is not None:\n",
    "            image_trans, bbox_trans = self.transform_image(\n",
    "                Image.open(self.samples[ref_id].image_path),\n",
    "                self.samples[ref_id].bounding_box,\n",
    "                device=self.device,\n",
    "            )\n",
    "            sample_trans: BatchSample = BatchSample(image_trans, caption).to(\n",
    "                self.device\n",
    "            )\n",
    "            return sample_trans, bbox_trans\n",
    "        else:\n",
    "            image: Tensor = read_image(self.samples[ref_id].image_path)\n",
    "            bbox: Tensor = self.samples[ref_id].bounding_box.to(device=self.device)\n",
    "            sample: BatchSample = BatchSample(image, caption).to(self.device)\n",
    "            return sample, bbox\n",
    "\n",
    "    def get_samples(self, dependencies: bool = False) -> List[Sample]:\n",
    "        with open(self.dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "            self.dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "        ) as refs:\n",
    "            instances = json.load(inst)\n",
    "            references = pickle.load(refs)\n",
    "        samples: List[Sample] = []\n",
    "        for ref in references:\n",
    "            if self.split.value == ref[\"split\"]:\n",
    "                image_path = self.get_image_path(ref[\"image_id\"], instances)\n",
    "                caption = self.get_caption(ref[\"sentences\"], dependencies)\n",
    "                bbox = self.get_bounding_box(ref[\"ann_id\"], instances)\n",
    "                samples.append(Sample(image_path, caption, bbox))\n",
    "        return samples\n",
    "\n",
    "    def get_image_path(self, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "        image_name = next(\n",
    "            image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "        )\n",
    "        path = self.dir_path + \"images/\" + image_name\n",
    "        return path\n",
    "\n",
    "    # Longest caption inserted into the prompt\n",
    "    def get_caption(self, captions: List[Dict[str, Any]], dependencies: bool) -> str:\n",
    "        longest_caption = captions[0]\n",
    "        for caption in captions:\n",
    "            if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "                longest_caption = caption\n",
    "        if dependencies:\n",
    "            return self.get_relevant_caption(\n",
    "                doc=self.text_processor(longest_caption[\"sent\"])\n",
    "            )\n",
    "        return f\"find the region that corresponds to the description {longest_caption['sent']}\"\n",
    "\n",
    "    # Bounding boxed converted to format compatible with yolo or torchvision\n",
    "    def get_bounding_box(self, ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
    "        bbox = next(\n",
    "            ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id\n",
    "        )\n",
    "        bounding_box: Tensor = tensor([])\n",
    "        match self.output_bbox_type:\n",
    "            case BboxType.XYXY:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "                )\n",
    "            case BboxType.XYWH:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
    "                )\n",
    "            case BboxType.CXCWH:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
    "                )\n",
    "\n",
    "        return bounding_box\n",
    "\n",
    "    # We tried extracting the most relevant part of the caption to use in the baseline implementation\n",
    "    # However, the results were not better than simply using the whole caption\n",
    "    def get_relevant_caption(self, doc: Doc) -> str:\n",
    "        # for chunck in doc.noun_chunks:\n",
    "        #         return chunck.text\n",
    "\n",
    "        # Mainly -ing verbs\n",
    "        for token in doc:\n",
    "            if \"acl\" in token.dep_:\n",
    "                subtree = list(token.subtree)\n",
    "                end = subtree[0].i\n",
    "                sent = doc[0:end]\n",
    "                if len(sent) > 1:\n",
    "                    return str(sent)\n",
    "\n",
    "        # subject which/that something\n",
    "        for token in doc:\n",
    "            if \"relcl\" in token.dep_:\n",
    "                subtree = list(token.subtree)\n",
    "                end: int = subtree[0].i\n",
    "                sent: Span = doc[0:end]\n",
    "                if len(sent) > 1:\n",
    "                    return str(sent)\n",
    "\n",
    "        # Subjects\n",
    "        for token in doc:\n",
    "            if \"subj\" in token.dep_:\n",
    "                subtree = list(token.subtree)\n",
    "                start: int = subtree[0].i\n",
    "                end = subtree[-1].i + 1\n",
    "                sent_subj: Span = doc[start:end]\n",
    "                if len(sent_subj) > 1:\n",
    "                    return str(sent_subj)\n",
    "        return str(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset / Dataloader utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the batches\n",
    "# Each batch is a tuple with a list of BatchSample and a tensor with the bounding boxes\n",
    "# (since the tensors have all the same size we can stack them in a tensor instead of a list)\n",
    "def custom_collate(\n",
    "    batch: List[Tuple[BatchSample, torch.Tensor]]\n",
    ") -> Tuple[List[BatchSample], torch.Tensor]:\n",
    "    bboxes: List[torch.Tensor] = []\n",
    "    samples: List[BatchSample] = []\n",
    "    for sample, bbox in batch:\n",
    "        samples.append(BatchSample(sample.image, sample.caption))\n",
    "        bboxes.append(bbox)\n",
    "    return samples, torch.stack(bboxes)\n",
    "\n",
    "\n",
    "# Function to resize the image and the corresponding bounding box correctly\n",
    "# Assumes bounding box is in format xyxy format\n",
    "\n",
    "\n",
    "def transform_sample(\n",
    "    image: Image.Image,\n",
    "    box: Tensor,\n",
    "    target_size: int = 224,\n",
    "    device: device = torch.device(\"cpu\"),\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    x, y = image.size[0], image.size[1]\n",
    "\n",
    "    x_scale = target_size / x\n",
    "    y_scale = target_size / y\n",
    "\n",
    "    trans = T.Compose(\n",
    "        transforms=[\n",
    "            T.Resize((target_size, target_size)),\n",
    "            T.CenterCrop(target_size),\n",
    "            T.PILToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    image_tensor: Tensor = trans(image).to(device)  # type: ignore\n",
    "\n",
    "    xmin, ymin, xmax, ymax = box.squeeze(0)\n",
    "\n",
    "    xmin = np.round(xmin * x_scale)\n",
    "    ymin = np.round(ymin * y_scale)\n",
    "    xmax = np.round(xmax * x_scale)\n",
    "    ymax = np.round(ymax * y_scale)\n",
    "\n",
    "    bbox_tensor: Tensor = torch.tensor([xmin, ymin, xmax, ymax], device=device)\n",
    "    return image_tensor, bbox_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple forward pass on the pretrained CLIP text encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: CLIP = clip.load(\"RN50\", device=self.device)[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, tokenized_caption: Tensor) -> Tensor:\n",
    "        tokenized_caption = tokenized_caption.int()\n",
    "        out: Tensor = self.pretrained_model.encode_text(\n",
    "            tokenized_caption.to(torch.IntTensor())\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP backbone is a modified ResNet with an attention layer for global pooling\n",
    "# We get all layers as outputs and project them to the same dimension as the text encodings\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: ModifiedResNet = clip.load(\"RN50\", device=self.device)[\n",
    "            0\n",
    "        ].visual  # type: ignore\n",
    "        assert isinstance(self.pretrained_model, ModifiedResNet)\n",
    "\n",
    "        # Freeze the backbone\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Register hooks to get the output of all layers\n",
    "        self.layers_outputs: OrderedDict[str, Tensor] = OrderedDict()\n",
    "        self.pretrained_model.layer1.register_forward_hook(self.hook_fn(\"layer1\"))  # type: ignore\n",
    "        self.pretrained_model.layer2.register_forward_hook(self.hook_fn(\"layer2\"))  # type: ignore\n",
    "        self.pretrained_model.layer3.register_forward_hook(self.hook_fn(\"layer3\"))  # type: ignore\n",
    "        self.pretrained_model.layer4.register_forward_hook(self.hook_fn(\"layer4\"))  # type: ignore\n",
    "\n",
    "        # Project the output of each layer to the same dimensionality as the text features\n",
    "        cfg = Config.get_instance().visual_encoder  # type: ignore\n",
    "        resnet_resolution = cfg[\"resnet_resolution\"]\n",
    "\n",
    "        # Each layer is projected through a different sequence of layers since the initial dimensionality is different\n",
    "        self.layers_projections: List[nn.Sequential] = []\n",
    "        for _ in range(4):\n",
    "            resnet_resolution //= 2\n",
    "            layer_projection: nn.Sequential = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(resnet_resolution),\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.LazyLinear(cfg[\"output_dim\"], device=self.device),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.layers_projections.append(layer_projection)\n",
    "\n",
    "    def forward(self, batch: Tensor) -> OrderedDict[str, Tensor]:\n",
    "        # Reset the dictionary\n",
    "        self.layers_outputs = OrderedDict()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out: Tensor = self.pretrained_model(batch)\n",
    "\n",
    "        for idx, (layer_name, layer_output) in enumerate(self.layers_outputs.items()):\n",
    "            self.layers_outputs[layer_name] = self.layers_projections[idx](layer_output)\n",
    "        self.layers_outputs[\"output\"] = out\n",
    "\n",
    "        return self.layers_outputs\n",
    "\n",
    "    def hook_fn(self, layer: str) -> Callable[[nn.Module, Tensor, Tensor], None]:\n",
    "        def hook(module: nn.Module, input: Tensor, output: Tensor) -> None:\n",
    "            self.layers_outputs[layer] = output\n",
    "\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is composed of the visual encoder and the text encoder described above\n",
    "# The attention layer is used to compute the attention between the the text and each layer of visual features\n",
    "# The attentions are then concatenated and passed through a regression head to predict the bounding box\n",
    "class VGModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlp_hidden_dim_1: int,\n",
    "        mlp_hidden_dim_2: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        cfg = Config.get_instance().model  # type: ignore\n",
    "        emb_dim = cfg[\"emb_dim\"]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.visual_backbone: VisualEncoder = VisualEncoder().to(self.device)\n",
    "        self.text_encoder: TextEncoder = TextEncoder().to(self.device)\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=emb_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.reg_head: MLP = MLP(\n",
    "            emb_dim * 5, 4, hidden_dim_1=mlp_hidden_dim_1, hidden_dim_2=mlp_hidden_dim_2\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, batch: List[BatchSample]) -> Tensor:\n",
    "        captions: Tensor = torch.stack([sample.caption for sample in batch]).squeeze(1)\n",
    "        text_features: Tensor = self.text_encoder(captions)\n",
    "\n",
    "        images: Tensor = torch.stack([sample.image for sample in batch])\n",
    "        visual_features: OrderedDict[str, Tensor] = self.visual_backbone(images)\n",
    "\n",
    "        attended_features: List[Tensor] = []\n",
    "        for feature in visual_features.values():\n",
    "            attention: Tensor = self.attention_layer(\n",
    "                feature, text_features, text_features\n",
    "            )\n",
    "            attended_features.append(attention[0])\n",
    "\n",
    "        aggregated_features: Tensor = torch.cat(attended_features, dim=1)\n",
    "        return self.reg_head(aggregated_features)\n",
    "\n",
    "# Simple 2-layer MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, output_dim: int, hidden_dim_1: int, hidden_dim_2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_2, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, l: float) -> None:\n",
    "        self.l1_loss = nn.L1Loss(reduction=\"mean\")\n",
    "        self.giou_loss = generalized_box_iou_loss\n",
    "        self.l = l\n",
    "        self.loss: Tensor\n",
    "\n",
    "    def compute(self, out: Tensor, bbox: Tensor) -> Tensor:\n",
    "        self.loss = self.giou_loss(out, bbox, reduction=\"mean\") + self.l * self.l1_loss(\n",
    "            out, bbox\n",
    "        )\n",
    "        return self.loss\n",
    "\n",
    "    def to_float(self) -> float:\n",
    "        return self.loss.detach().cpu().item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: Trial) -> float:\n",
    "    config = Config.get_instance()  # type: ignore\n",
    "    train_dataset: VGDataset = VGDataset(\n",
    "        dir_path=config.dataset_path,\n",
    "        split=Split.TRAIN,\n",
    "        output_bbox_type=BboxType.XYXY,\n",
    "        transform_image=transform_sample,\n",
    "    )\n",
    "\n",
    "    val_dataset: VGDataset = VGDataset(\n",
    "        dir_path=config.dataset_path,\n",
    "        split=Split.VAL,\n",
    "        output_bbox_type=BboxType.XYXY,\n",
    "        transform_image=transform_sample,\n",
    "    )\n",
    "\n",
    "    batch_size = trial.suggest_int(\n",
    "        \"batch_size\",\n",
    "        1,\n",
    "        10,\n",
    "    )\n",
    "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Loss is the weighted sum of the smooth l1 loss and the GIoU\n",
    "    l = trial.suggest_float(\"l\", 0.0, 1.0)\n",
    "    loss = Loss(l)\n",
    "    losses: List[float] = []\n",
    "    accuracies: List[float] = []\n",
    "\n",
    "    hidden_dim_1 = trial.suggest_int(\"hidden_dim_1\", 512, 2048)\n",
    "    hidden_dim_2 = trial.suggest_int(\"hidden_dim_2\", 128, 512)\n",
    "    if config.logging[\"resume\"]:\n",
    "        checkpoint: Dict[str, Any] = torch.load(config.logging[\"path\"] + \"model.pth\")\n",
    "        model = VGModel(hidden_dim_1, hidden_dim_2).to(device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=config.model[\"gamma\"]\n",
    "        )\n",
    "        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n",
    "        start_epoch: int = checkpoint[\"epoch\"]\n",
    "        losses.append(checkpoint[\"loss\"])\n",
    "    else:\n",
    "        model = VGModel(hidden_dim_1, hidden_dim_2).train()\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=config.model[\"gamma\"]\n",
    "        )\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, config.epochs), desc=\"Epochs\"):\n",
    "        print(\"-------------------- Training --------------------------\")\n",
    "        loss_epoch = train_one_epoch(train_dataloader, model, loss, optimizer)\n",
    "        losses.append(loss_epoch.cpu().item())\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set for hyperparameter tuning\n",
    "        print(\"-------------------- Validation ------------------------\")\n",
    "        accuracy = validate(val_dataloader, model)\n",
    "        accuracies.append(accuracy)\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Early stopping for non promising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        # Save model after each epoch\n",
    "        if config.logging[\"save_model\"]:\n",
    "            dir: str = config.logging[\"path\"]\n",
    "            if not os.path.exists(dir):\n",
    "                os.makedirs(dir)\n",
    "\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "                    \"loss\": loss_epoch,\n",
    "                },\n",
    "                f=f\"{config.logging['path']}model.pth\",\n",
    "            )\n",
    "\n",
    "        torch.clear_autocast_cache()\n",
    "\n",
    "    return sum(accuracies) / len(accuracies)\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    loss: Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> Tensor:\n",
    "    # As loss we take smooth_l1 + GIoU\n",
    "    train_loss: List[Tensor] = []\n",
    "\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        # Loss and metrics\n",
    "        batch_loss: Tensor = loss.compute(out, bbox)\n",
    "        train_loss.append(batch_loss)\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return torch.stack(train_loss).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]], model: VGModel\n",
    ") -> float:\n",
    "    # As accuracy we take the average IoU\n",
    "    accuracy: List[Tensor] = []\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        accuracy.append(torch.diagonal(box_iou(out, bbox)).mean())\n",
    "\n",
    "    return torch.stack(accuracy).mean().cpu().item()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "    study_name = \"test\"\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=f\"sqlite:///{study_name}\",\n",
    "        direction=\"maximize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=3, timeout=600)\n",
    "\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best hyperparameters: {trial.params}\")\n",
    "    fig = plot_optimization_history(study)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
