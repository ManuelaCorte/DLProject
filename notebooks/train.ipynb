{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ftfy regex tqdm ultralytics optuna\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple, OrderedDict, Callable\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import pickle\n",
    "import json\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor, tensor, device\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import box_iou, box_convert, generalized_box_iou_loss\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from clip import clip\n",
    "from clip.model import CLIP, ModifiedResNet\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from optuna.visualization import plot_optimization_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset and save under data/raw/ only if not already downloaded\n",
    "url = \"https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\"\n",
    "if not os.path.exists(\"data/refcocog.tar.gz\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(url=url, output=\"data/\", quiet=False, resume=True)\n",
    "if not os.path.exists(\"data/refcocog/\"):\n",
    "    print(\"Extracting dataset...\")\n",
    "    !tar -xf data/refcocog.tar.gz -C data/ --verbose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed samples can be downloaded from Google Drive by executing the following cell. Otherwise, the preprocessing wil be done saving the file only temporarly in the Colab environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download preprocessed dataset\n",
    "url = \"https://drive.google.com/drive/folders/1jaJV40dneOckZn7WHMQyd2jBh7A8534N\"\n",
    "gdown.download_folder(url=url, output=\"data/\", quiet=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility classes definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Sample:\n",
    "    image_path: str\n",
    "    caption: str\n",
    "    bounding_box: Tensor\n",
    "\n",
    "\n",
    "class BatchSample:\n",
    "    def __init__(self, image: Tensor, caption: Tensor) -> None:\n",
    "        self.image: Tensor = image\n",
    "        self.caption: Tensor = caption\n",
    "\n",
    "    def to(self, device: device | str) -> Any:\n",
    "        return self.__class__(self.image.to(device), self.caption.to(device))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BatchSample(image={self.image.shape}, caption={self.caption.shape})\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "\n",
    "# Used in the baseline implementation\n",
    "@dataclass(frozen=True)\n",
    "class Result:\n",
    "    bounding_box: Tensor\n",
    "    score: Tensor\n",
    "\n",
    "\n",
    "# XYXY: top left and bottom right corners\n",
    "# XYWH: top left corner, width and height\n",
    "# CXCWH: center coordinates, width and height\n",
    "@dataclass(frozen=True)\n",
    "class BboxType(Enum):\n",
    "    XYXY = \"xyxy\"\n",
    "    XYWH = \"xywh\"\n",
    "    CXCWH = \"cxcwh\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return super().__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Singleton:\n",
    "    def __init__(self, decorated_class: Any) -> None:\n",
    "        self._decorated = decorated_class\n",
    "\n",
    "    def get_instance(self) -> Any:\n",
    "        \"\"\"\n",
    "        Returns the singleton instance. Upon its first call, it creates a\n",
    "        new instance of the decorated class and calls its `__init__` method.\n",
    "        On all subsequent calls, the already created instance is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._instance  # type: ignore\n",
    "        except AttributeError:\n",
    "            self._instance = self._decorated()\n",
    "            return self._instance\n",
    "\n",
    "    def __call__(self) -> None:\n",
    "        raise TypeError(\"Singletons must be accessed through get_instance() method.\")\n",
    "\n",
    "    def __instancecheck__(self, inst: Any) -> bool:\n",
    "        return isinstance(inst, self._decorated)\n",
    "\n",
    "\n",
    "# All configurations are stored in a json file and loaded here only once\n",
    "# All other times the class is called the same instance is returned\n",
    "@Singleton\n",
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        with open(file=\"config.json\", mode=\"r\") as fp:\n",
    "            cfg: Dict[str, Any] = json.load(fp=fp)\n",
    "        for k, v in cfg.items():\n",
    "            setattr(self, k, v)\n",
    "        # self.__dict__.update(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(dir_path: str) -> Tuple[List[Sample], List[Sample], List[Sample]]:\n",
    "    with open(dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "        dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "    ) as refs:\n",
    "        instances = json.load(inst)\n",
    "        references = pickle.load(refs)\n",
    "    train_samples: List[Sample] = []\n",
    "    val_samples: List[Sample] = []\n",
    "    test_samples: List[Sample] = []\n",
    "    for ref in tqdm(references, desc=f\"Processing dataset\"):\n",
    "        image_path = get_image_path(dir_path, ref[\"image_id\"], instances)\n",
    "        caption = get_caption(ref[\"sentences\"])\n",
    "        bbox = get_bounding_box(ref[\"ann_id\"], instances)\n",
    "        split = ref[\"split\"]\n",
    "        # print(split)\n",
    "        match split:\n",
    "            case Split.TRAIN.value:\n",
    "                train_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.VAL.value:\n",
    "                val_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.TEST.value:\n",
    "                test_samples.append(Sample(image_path, caption, bbox))\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid split: {split}\")\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "def get_image_path(dir_path: str, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "    image_name = next(\n",
    "        image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "    )\n",
    "    path = dir_path + \"images/\" + image_name\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_caption(captions: List[Dict[str, Any]]) -> str:\n",
    "    longest_caption = captions[0]\n",
    "    for caption in captions:\n",
    "        if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "            longest_caption = caption\n",
    "    return longest_caption[\"sent\"]\n",
    "\n",
    "\n",
    "# Bounding boxed converted to format compatible with yolo or torchvision\n",
    "def get_bounding_box(ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
    "    bbox = next(ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id)\n",
    "    bounding_box: Tensor = tensor([])\n",
    "    bounding_box = box_convert(\n",
    "        tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "    )\n",
    "    return bounding_box\n",
    "\n",
    "\n",
    "# If the files already exist, don't preprocess again\n",
    "def preprocess(in_path: str, out_path: str) -> None:\n",
    "    if (\n",
    "        os.path.exists(f\"{out_path}train_samples.p\")\n",
    "        and os.path.exists(f\"{out_path}val_samples.p\")\n",
    "        and os.path.exists(f\"{out_path}test_samples.p\")\n",
    "    ):\n",
    "        return\n",
    "    train_samples, val_samples, test_samples = get_samples(in_path)\n",
    "\n",
    "    pickle.dump(train_samples, open(f\"{out_path}train_samples.p\", \"wb\"))\n",
    "\n",
    "    pickle.dump(val_samples, open(f\"{out_path}val_samples.p\", \"wb\"))\n",
    "\n",
    "    pickle.dump(test_samples, open(f\"{out_path}test_samples.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset contains samples with an image with a bounding box and a caption associated with the bounding box.\n",
    "class VGDataset(Dataset[Tuple[BatchSample, Tensor]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        split: Split,\n",
    "        output_bbox_type: BboxType,\n",
    "        transform_image: Any = None,\n",
    "        preprocessed: bool = False,\n",
    "        preprocessed_path: str = \"data/processed/\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dir_path: str = dir_path\n",
    "        self.split: Split = split\n",
    "        self.output_bbox_type: BboxType = output_bbox_type\n",
    "        self.transform_image: Any = transform_image\n",
    "        self.device: device = torch.device(\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        if preprocessed:\n",
    "            preprocess(dir_path, preprocessed_path)\n",
    "            with open(\n",
    "                preprocessed_path + f\"{self.split.value}_samples.p\", \"rb\"\n",
    "            ) as samples:\n",
    "                self.samples: List[Sample] = pickle.load(samples)\n",
    "        else:\n",
    "            self.samples: List[Sample] = self.get_samples()  # type: ignore\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, ref_id: int) -> Tuple[BatchSample, Tensor]:\n",
    "        caption: Tensor = clip.tokenize(self.samples[ref_id].caption)\n",
    "\n",
    "        if self.transform_image is not None:\n",
    "            image_trans, bbox_trans = self.transform_image(\n",
    "                Image.open(self.samples[ref_id].image_path),\n",
    "                self.samples[ref_id].bounding_box,\n",
    "                device=self.device,\n",
    "            )\n",
    "            sample_trans: BatchSample = BatchSample(image_trans, caption).to(\n",
    "                self.device\n",
    "            )\n",
    "            return sample_trans, bbox_trans\n",
    "        else:\n",
    "            image: Tensor = read_image(self.samples[ref_id].image_path)\n",
    "            bbox: Tensor = self.samples[ref_id].bounding_box.to(device=self.device)\n",
    "            sample: BatchSample = BatchSample(image, caption).to(self.device)\n",
    "            return sample, bbox\n",
    "\n",
    "    def get_samples(self) -> List[Sample]:\n",
    "        with open(self.dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "            self.dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "        ) as refs:\n",
    "            instances = json.load(inst)\n",
    "            references = pickle.load(refs)\n",
    "        samples: List[Sample] = []\n",
    "        for ref in references:\n",
    "            if self.split.value == ref[\"split\"]:\n",
    "                image_path = self.get_image_path(ref[\"image_id\"], instances)\n",
    "                caption = self.get_caption(ref[\"sentences\"])\n",
    "                bbox = self.get_bounding_box(ref[\"ann_id\"], instances)\n",
    "                samples.append(Sample(image_path, caption, bbox))\n",
    "        return samples\n",
    "\n",
    "    def get_image_path(self, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "        image_name = next(\n",
    "            image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "        )\n",
    "        path = self.dir_path + \"images/\" + image_name\n",
    "        return path\n",
    "\n",
    "    def get_caption(self, captions: List[Dict[str, Any]]) -> str:\n",
    "        longest_caption = captions[0]\n",
    "        for caption in captions:\n",
    "            if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "                longest_caption = caption\n",
    "        return f\"find the region that corresponds to the description {longest_caption['sent']}\"\n",
    "\n",
    "    # Bounding boxed converted to format compatible with yolo or torchvision\n",
    "    def get_bounding_box(self, ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
    "        bbox = next(\n",
    "            ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id\n",
    "        )\n",
    "        bounding_box: Tensor = tensor([])\n",
    "        match self.output_bbox_type:\n",
    "            case BboxType.XYXY:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "                )\n",
    "            case BboxType.XYWH:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
    "                )\n",
    "            case BboxType.CXCWH:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
    "                )\n",
    "\n",
    "        return bounding_box"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset / Dataloader utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the batches\n",
    "# Each batch is a tuple with a list of BatchSample and a tensor with the bounding boxes\n",
    "# (since the tensors have all the same size we can stack them in a tensor instead of a list)\n",
    "def custom_collate(\n",
    "    batch: List[Tuple[BatchSample, torch.Tensor]]\n",
    ") -> Tuple[List[BatchSample], torch.Tensor]:\n",
    "    bboxes: List[torch.Tensor] = []\n",
    "    samples: List[BatchSample] = []\n",
    "    for sample, bbox in batch:\n",
    "        samples.append(BatchSample(sample.image, sample.caption))\n",
    "        bboxes.append(bbox)\n",
    "    return samples, torch.stack(bboxes)\n",
    "\n",
    "\n",
    "# Function to resize the image and the corresponding bounding box correctly\n",
    "# Bounding box already in the correct format\n",
    "def transform_sample(\n",
    "    image: Image.Image,\n",
    "    box: Tensor,\n",
    "    target_size: int = 224,\n",
    "    device: device = torch.device(\"cpu\"),\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    x: int\n",
    "    y: int\n",
    "    x, y = image.size[0], image.size[1]\n",
    "\n",
    "    x_scale: float = target_size / x\n",
    "    y_scale: float = target_size / y\n",
    "\n",
    "    # Same transformation as in the paper\n",
    "    trans = T.Compose(\n",
    "        transforms=[\n",
    "            T.Resize(\n",
    "                size=target_size,\n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                max_size=None,\n",
    "                antialias=\"warn\",\n",
    "            ),\n",
    "            T.CenterCrop(target_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                std=(0.26862954, 0.26130258, 0.27577711),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image_tensor: Tensor = trans(image).to(device)  # type: ignore\n",
    "    if image_tensor.shape[0] == 1:\n",
    "        image_tensor = image_tensor.repeat(3, 1, 1)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = box.squeeze(0)\n",
    "\n",
    "    xmin = np.round(xmin * x_scale)\n",
    "    ymin = np.round(ymin * y_scale)\n",
    "    xmax = np.round(xmax * x_scale)\n",
    "    ymax = np.round(ymax * y_scale)\n",
    "\n",
    "    bbox_tensor: Tensor = torch.tensor([xmin, ymin, xmax, ymax], device=device)\n",
    "    return image_tensor, bbox_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple forward pass on the pretrained CLIP text encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: CLIP = clip.load(\"RN50\", device=self.device)[0]\n",
    "        self.pretrained_model.float()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, tokenized_caption: Tensor) -> Tensor:\n",
    "        out: Tensor = self.pretrained_model.encode_text(tokenized_caption).to(\n",
    "            self.device\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that gets output for all layes of the backbone\n",
    "# CLIP backbone is a modified ResNet with an attention layer for global pooling\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: ModifiedResNet = clip.load(\"RN50\", device=self.device)[\n",
    "            0\n",
    "        ].visual  # type: ignore\n",
    "        self.pretrained_model.float()\n",
    "        assert isinstance(self.pretrained_model, ModifiedResNet)\n",
    "\n",
    "        # Freeze the backbone\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Register hooks to get the output of all layers\n",
    "        self.layers_outputs: OrderedDict[str, Tensor] = OrderedDict()\n",
    "        self.pretrained_model.layer1.register_forward_hook(self.hook_fn(\"layer1\"))  # type: ignore\n",
    "        self.pretrained_model.layer2.register_forward_hook(self.hook_fn(\"layer2\"))  # type: ignore\n",
    "        self.pretrained_model.layer3.register_forward_hook(self.hook_fn(\"layer3\"))  # type: ignore\n",
    "        self.pretrained_model.layer4.register_forward_hook(self.hook_fn(\"layer4\"))  # type: ignore\n",
    "\n",
    "        # Project the output of each layer to the same dimensionality as the text features\n",
    "        cfg = Config.get_instance().visual_encoder  # type: ignore\n",
    "        resnet_resolution: int = cfg[\"resnet_resolution\"]\n",
    "        resnet_channels: int = cfg[\"resnet_channels\"]\n",
    "\n",
    "        self.layers_projections: List[nn.Sequential] = []\n",
    "        for _ in range(4):\n",
    "            resnet_resolution //= 2\n",
    "            in_features: int = resnet_channels * resnet_resolution * resnet_resolution\n",
    "            resnet_channels *= 2\n",
    "            layer_projection: nn.Sequential = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(resnet_resolution),\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Linear(in_features, cfg[\"output_dim\"], device=self.device),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.layers_projections.append(layer_projection)\n",
    "\n",
    "    def forward(self, batch: Tensor) -> OrderedDict[str, Tensor]:\n",
    "        # Reset the dictionary\n",
    "        self.layers_outputs = OrderedDict()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out: Tensor = self.pretrained_model(batch)\n",
    "\n",
    "        for idx, (layer_name, layer_output) in enumerate(self.layers_outputs.items()):\n",
    "            self.layers_outputs[layer_name] = self.layers_projections[idx](layer_output)\n",
    "        self.layers_outputs[\"output\"] = out\n",
    "\n",
    "        return self.layers_outputs\n",
    "\n",
    "    def hook_fn(self, layer: str) -> Callable[[nn.Module, Tensor, Tensor], None]:\n",
    "        def hook(module: nn.Module, input: Tensor, output: Tensor) -> None:\n",
    "            # print(f\"Module: {[module for  module in module.modules()]}\")\n",
    "            self.layers_outputs[layer] = output\n",
    "\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is composed of the visual encoder and the text encoder described above\n",
    "# The attention layer is used to compute the attention between the the text and each layer of visual features\n",
    "# The attentions are then concatenated and passed through a regression head to predict the bounding box\n",
    "class VGModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlp_hidden_dim_1: int,\n",
    "        mlp_hidden_dim_2: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        cfg = Config.get_instance().model  # type: ignore\n",
    "        emb_dim = cfg[\"emb_dim\"]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.visual_backbone: VisualEncoder = VisualEncoder().to(self.device)\n",
    "        self.text_encoder: TextEncoder = TextEncoder().to(self.device)\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=emb_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.reg_head: MLP = MLP(\n",
    "            emb_dim * 5, 4, hidden_dim_1=mlp_hidden_dim_1, hidden_dim_2=mlp_hidden_dim_2\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, batch: List[BatchSample]) -> Tensor:\n",
    "        captions: Tensor = torch.stack([sample.caption for sample in batch]).squeeze(1)\n",
    "        text_features: Tensor = self.text_encoder(captions)\n",
    "\n",
    "        images: Tensor = torch.stack([sample.image for sample in batch])\n",
    "        visual_features: OrderedDict[str, Tensor] = self.visual_backbone(images)\n",
    "\n",
    "        attended_features: List[Tensor] = []\n",
    "        for feature in visual_features.values():\n",
    "            attention: Tensor = self.attention_layer(\n",
    "                feature, text_features, text_features\n",
    "            )\n",
    "            attended_features.append(attention[0])\n",
    "\n",
    "        aggregated_features: Tensor = torch.cat(attended_features, dim=1)\n",
    "        return self.reg_head(aggregated_features)\n",
    "\n",
    "\n",
    "# Simple 2-layer MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, output_dim: int, hidden_dim_1: int, hidden_dim_2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_2, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, l: float) -> None:\n",
    "        self.l1_loss = nn.L1Loss(reduction=\"mean\")\n",
    "        self.giou_loss = generalized_box_iou_loss\n",
    "        self.l = l\n",
    "        self.loss: Tensor\n",
    "\n",
    "    def compute(self, out: Tensor, bbox: Tensor) -> Tensor:\n",
    "        self.loss = self.giou_loss(out, bbox, reduction=\"mean\") + self.l * self.l1_loss(\n",
    "            out, bbox\n",
    "        )\n",
    "        return self.loss\n",
    "\n",
    "    def to_float(self) -> float:\n",
    "        return self.loss.detach().cpu().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    loss: Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tensor:\n",
    "    # As loss we take smooth_l1 + GIoU\n",
    "    epoch_loss_list: List[Tensor] = []\n",
    "\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        # Loss and metrics\n",
    "        batch_loss: Tensor = loss.compute(out, bbox)\n",
    "        epoch_loss_list.append(batch_loss)\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return torch.stack(epoch_loss_list).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    # As accuracy we take the average IoU\n",
    "    accuracy_list: List[Tensor] = []\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        accuracy_list.append(torch.diagonal(box_iou(out, bbox)).mean())\n",
    "\n",
    "    return torch.stack(accuracy_list).mean().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: Trial) -> float:\n",
    "    cfg = Config.get_instance()  # type: ignore\n",
    "    train_dataset: VGDataset = VGDataset(\n",
    "        dir_path=cfg.dataset_path,\n",
    "        split=Split.TRAIN,\n",
    "        output_bbox_type=BboxType.XYXY,\n",
    "        transform_image=transform_sample,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Train dataset created. Dataset length \", len(train_dataset))\n",
    "\n",
    "    val_dataset: VGDataset = VGDataset(\n",
    "        dir_path=cfg.dataset_path,\n",
    "        split=Split.VAL,\n",
    "        output_bbox_type=BboxType.XYXY,\n",
    "        transform_image=transform_sample,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Validation dataset created. Dataset length: \", len(val_dataset))\n",
    "\n",
    "    batch_size = trial.suggest_int(\n",
    "        \"batch_size\",\n",
    "        1,\n",
    "        10,\n",
    "    )\n",
    "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Loss is the weighted sum of the smooth l1 loss and the GIoU\n",
    "    l = trial.suggest_float(\"l\", 0.0, 1.0)\n",
    "    loss_func = Loss(l)\n",
    "    losses_list: List[float] = []\n",
    "    accuracies_list: List[float] = []\n",
    "\n",
    "    hidden_dim_1 = trial.suggest_int(\"hidden_dim_1\", 512, 2048)\n",
    "    hidden_dim_2 = trial.suggest_int(\"hidden_dim_2\", 128, 512)\n",
    "    if cfg.logging[\"resume\"]:\n",
    "        checkpoint: Dict[str, Any] = torch.load(cfg.logging[\"path\"] + \"model.pth\")\n",
    "        model = VGModel(hidden_dim_1, hidden_dim_2).to(device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=cfg.model[\"gamma\"]\n",
    "        )\n",
    "        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n",
    "        start_epoch: int = checkpoint[\"epoch\"]\n",
    "        losses_list.append(checkpoint[\"loss\"])\n",
    "    else:\n",
    "        model = VGModel(hidden_dim_1, hidden_dim_2).train()\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=cfg.model[\"gamma\"]\n",
    "        )\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, cfg.epochs), desc=\"Epochs\"):\n",
    "        print(\"-------------------- Training --------------------------\")\n",
    "        epoch_loss = train_one_epoch(\n",
    "            train_dataloader, model, loss_func, optimizer, device\n",
    "        )\n",
    "        losses_list.append(epoch_loss.cpu().item())\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set for hyperparameter tuning\n",
    "        print(\"-------------------- Validation ------------------------\")\n",
    "        accuracy = validate(val_dataloader, model, device)\n",
    "        accuracies_list.append(accuracy)\n",
    "        trial.report(accuracy, epoch)\n",
    "        print(f\"Accuracy: {accuracy} at epoch {epoch}\")\n",
    "\n",
    "        # Early stopping for non promising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        # Save model after each epoch\n",
    "        if cfg.logging[\"save_model\"]:\n",
    "            dir: str = cfg.logging[\"path\"]\n",
    "            if not os.path.exists(dir):\n",
    "                os.makedirs(dir)\n",
    "\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "                    \"loss\": epoch_loss,\n",
    "                },\n",
    "                f=f\"{cfg.logging['path']}model.pth\",\n",
    "            )\n",
    "\n",
    "        torch.clear_autocast_cache()\n",
    "\n",
    "    return sum(accuracies_list) / len(accuracies_list)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "    study_name = \"train.db\"\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=f\"sqlite:///{study_name}\",\n",
    "        direction=\"maximize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=1, timeout=600)\n",
    "\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best hyperparameters: {trial.params}\")\n",
    "    fig = plot_optimization_history(study)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
