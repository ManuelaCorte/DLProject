{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ftfy regex tqdm ultralytics optuna albumentations torchviz\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Any, Callable, Dict, List, OrderedDict, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from clip import clip\n",
    "from clip.model import CLIP, ModifiedResNet\n",
    "from optuna.trial import Trial\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from PIL import Image\n",
    "from torch import Tensor, device, tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops import box_convert, box_iou, generalized_box_iou_loss\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset and save under data/raw/ only if not already downloaded\n",
    "%cd /content/\n",
    "url = \"https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\"\n",
    "if not os.path.exists(\"data/raw/refcocog.tar.gz\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(url=url, output=\"data/raw/\", quiet=False, resume=True)\n",
    "if not os.path.exists(\"data/raw/refcocog/\"):\n",
    "    print(\"Extracting dataset...\")\n",
    "    !tar -xf data/raw/refcocog.tar.gz -C data/raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed samples can be downloaded from Google Drive by executing the following cell. Otherwise, the preprocessing wil be done saving the file only temporarly in the Colab environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download preprocessed dataset\n",
    "url = \"https://drive.google.com/drive/folders/1jaJV40dneOckZn7WHMQyd2jBh7A8534N\"\n",
    "gdown.download_folder(url=url, output=\"data/\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy configuration file from repository\n",
    "%cd /content/\n",
    "!wget https://raw.githubusercontent.com/ManuelaCorte/DLProject/master/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move forlder simply for consistentcy with repository implementation\n",
    "%mkdir src\n",
    "%cd src"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility classes definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, image_path: str, caption: str, bounding_box: Tensor) -> None:\n",
    "        self.image_path = image_path\n",
    "        self.caption = caption\n",
    "        self.bounding_box = bounding_box\n",
    "\n",
    "    def as_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"image_path\": self.image_path,\n",
    "            \"caption\": self.caption,\n",
    "            \"bounding_box\": self.bounding_box.tolist(),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def fromJSON(json: dict[str, Any]) -> Any:\n",
    "        return Sample(json[\"image_path\"], json[\"caption\"], Tensor(json[\"bounding_box\"]))\n",
    "\n",
    "\n",
    "class BatchSample:\n",
    "    def __init__(self, image: Tensor, caption: Tensor) -> None:\n",
    "        self.image: Tensor = image\n",
    "        self.caption: Tensor = caption\n",
    "\n",
    "    def to(self, device: device | str) -> Any:\n",
    "        return self.__class__(self.image.to(device), self.caption.to(device))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BatchSample(image={self.image.shape}, caption={self.caption.shape})\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "\n",
    "# Used in the baseline implementation\n",
    "@dataclass(frozen=True)\n",
    "class Result:\n",
    "    bounding_box: Tensor\n",
    "    score: Tensor\n",
    "\n",
    "\n",
    "# XYXY: top left and bottom right corners\n",
    "# XYWH: top left corner, width and height\n",
    "# CXCWH: center coordinates, width and height\n",
    "@dataclass(frozen=True)\n",
    "class BboxType(Enum):\n",
    "    XYXY = \"xyxy\"\n",
    "    XYWH = \"xywh\"\n",
    "    CXCWH = \"cxcwh\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return super().__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Singleton:\n",
    "    def __init__(self, decorated_class: Any) -> None:\n",
    "        self._decorated = decorated_class\n",
    "\n",
    "    def get_instance(self) -> Any:\n",
    "        \"\"\"\n",
    "        Returns the singleton instance. Upon its first call, it creates a\n",
    "        new instance of the decorated class and calls its `__init__` method.\n",
    "        On all subsequent calls, the already created instance is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._instance  # type: ignore\n",
    "        except AttributeError:\n",
    "            self._instance = self._decorated()\n",
    "            return self._instance\n",
    "\n",
    "    def __call__(self) -> None:\n",
    "        raise TypeError(\"Singletons must be accessed through get_instance() method.\")\n",
    "\n",
    "    def __instancecheck__(self, inst: Any) -> bool:\n",
    "        return isinstance(inst, self._decorated)\n",
    "\n",
    "\n",
    "# All configurations are stored in a json file and loaded here only once\n",
    "# All other times the class is called the same instance is returned\n",
    "@Singleton\n",
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        with open(file=\"../config.json\", mode=\"r\") as fp:\n",
    "            cfg: Dict[str, Any] = json.load(fp=fp)\n",
    "        for k, v in cfg.items():\n",
    "            setattr(self, k, v)\n",
    "        # self.__dict__.update(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(dir_path: str) -> Tuple[List[Sample], List[Sample], List[Sample]]:\n",
    "    with open(dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "        dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "    ) as refs:\n",
    "        instances = json.load(inst)\n",
    "        references = pickle.load(refs)\n",
    "    train_samples: List[Sample] = []\n",
    "    val_samples: List[Sample] = []\n",
    "    test_samples: List[Sample] = []\n",
    "    for ref in tqdm(references, desc=f\"Processing dataset\"):\n",
    "        image_path = get_image_path(dir_path, ref[\"image_id\"], instances)\n",
    "        caption = get_caption(ref[\"sentences\"])\n",
    "        bbox = get_bounding_box(ref[\"ann_id\"], instances)\n",
    "        split = ref[\"split\"]\n",
    "        # print(split)\n",
    "        match split:\n",
    "            case Split.TRAIN.value:\n",
    "                train_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.VAL.value:\n",
    "                val_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.TEST.value:\n",
    "                test_samples.append(Sample(image_path, caption, bbox))\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid split: {split}\")\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "def get_image_path(dir_path: str, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "    image_name = next(\n",
    "        image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "    )\n",
    "    path = dir_path + \"images/\" + image_name\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_caption(captions: List[Dict[str, Any]]) -> str:\n",
    "    longest_caption = captions[0]\n",
    "    for caption in captions:\n",
    "        if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "            longest_caption = caption\n",
    "    return longest_caption[\"sent\"]\n",
    "\n",
    "\n",
    "# Bounding boxed converted to format compatible with yolo or torchvision\n",
    "def get_bounding_box(ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
    "    bbox = next(ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id)\n",
    "    bounding_box: Tensor = tensor([])\n",
    "    bounding_box = box_convert(\n",
    "        tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "    )\n",
    "    return bounding_box\n",
    "\n",
    "\n",
    "# If the files already exist, don't preprocess again\n",
    "def preprocess(in_path: str, out_path: str) -> None:\n",
    "    if (\n",
    "        os.path.exists(f\"{out_path}train_samples.json\")\n",
    "        and os.path.exists(f\"{out_path}val_samples.json\")\n",
    "        and os.path.exists(f\"{out_path}test_samples.json\")\n",
    "    ):\n",
    "        return\n",
    "    train_samples, val_samples, test_samples = get_samples(in_path)\n",
    "\n",
    "    json.dump(\n",
    "        train_samples,\n",
    "        open(f\"{out_path}train_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "    json.dump(\n",
    "        val_samples,\n",
    "        open(f\"{out_path}val_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "    json.dump(\n",
    "        test_samples,\n",
    "        open(f\"{out_path}test_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset contains samples with an image with a bounding box and a caption associated with the bounding box.\n",
    "\n",
    "\n",
    "class VGDataset(Dataset[Tuple[BatchSample, Tensor]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        split: Split,\n",
    "        output_bbox_type: BboxType,\n",
    "        augment: bool,\n",
    "        transform: bool = True,\n",
    "        preprocessed: bool = False,\n",
    "        preprocessed_path: str = \"../data/processed/\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dir_path: str = dir_path\n",
    "        self.split: Split = split\n",
    "        self.output_bbox_type: BboxType = output_bbox_type\n",
    "        self.augment: bool = augment\n",
    "        self.transform: bool = transform\n",
    "        self.device: device = torch.device(\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        if preprocessed:\n",
    "            preprocess(dir_path, preprocessed_path)\n",
    "            with open(\n",
    "                preprocessed_path + f\"{self.split.value}_samples.json\", \"rb\"\n",
    "            ) as samples:\n",
    "                self.samples: List[Sample] = json.load(\n",
    "                    samples, object_hook=Sample.fromJSON\n",
    "                )\n",
    "        else:\n",
    "            self.samples: List[Sample] = self.get_samples()  # type: ignore\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, ref_id: int) -> Tuple[BatchSample, Tensor]:\n",
    "        # extended_caption = f\"find the region that corresponds to the description {caption}\"\n",
    "        caption: Tensor = clip.tokenize(self.samples[ref_id].caption)  # type: ignore\n",
    "        if self.transform:\n",
    "            image, bbox = transform_sample(\n",
    "                Image.open(self.samples[ref_id].image_path),\n",
    "                self.samples[ref_id].bounding_box,\n",
    "                self.augment,\n",
    "                device=self.device,\n",
    "            )\n",
    "        else:\n",
    "            image = read_image(self.samples[ref_id].image_path)\n",
    "            bbox = torch.tensor([self.samples[ref_id].bounding_box])\n",
    "        return BatchSample(image, caption), bbox\n",
    "\n",
    "    def get_samples(self) -> List[Sample]:\n",
    "        with open(self.dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "            self.dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "        ) as refs:\n",
    "            instances = json.load(inst)\n",
    "            references = pickle.load(refs)\n",
    "        samples: List[Sample] = []\n",
    "        for ref in references:\n",
    "            if self.split.value == ref[\"split\"]:\n",
    "                image_path = self.get_image_path(ref[\"image_id\"], instances)\n",
    "                caption = self.get_caption(ref[\"sentences\"])\n",
    "                bbox = self.get_bounding_box(ref[\"ann_id\"], instances)\n",
    "                samples.append(Sample(image_path, caption, bbox))\n",
    "        return samples\n",
    "\n",
    "    def get_image_path(self, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "        image_name = next(\n",
    "            image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "        )\n",
    "        path = self.dir_path + \"images/\" + image_name\n",
    "        return path\n",
    "\n",
    "    def get_caption(self, captions: List[Dict[str, Any]]) -> str:\n",
    "        longest_caption = captions[0]\n",
    "        for caption in captions:\n",
    "            if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "                longest_caption = caption\n",
    "        return f\"find the region that corresponds to the description {longest_caption['sent']}\"\n",
    "\n",
    "    # Bounding boxed converted to format compatible with yolo or torchvision\n",
    "    def get_bounding_box(self, ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
    "        bbox = next(\n",
    "            ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id\n",
    "        )\n",
    "        bounding_box: Tensor = tensor([])\n",
    "        match self.output_bbox_type:\n",
    "            case BboxType.XYXY:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "                )\n",
    "            case BboxType.XYWH:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
    "                )\n",
    "            case BboxType.CXCWH:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
    "                )\n",
    "\n",
    "        return bounding_box"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset / Dataloader utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(\n",
    "    batch: List[Tuple[BatchSample, torch.Tensor]]\n",
    ") -> Tuple[List[BatchSample], torch.Tensor]:\n",
    "    bboxes: List[torch.Tensor] = []\n",
    "    samples: List[BatchSample] = []\n",
    "    for sample, bbox in batch:\n",
    "        samples.append(BatchSample(sample.image, sample.caption))\n",
    "        bboxes.append(bbox)\n",
    "    return samples, torch.stack(bboxes)\n",
    "\n",
    "\n",
    "# Transform image according to CLIP preprocess function\n",
    "# Normalize bounding box coordinates to be independent of image size\n",
    "def transform_sample(\n",
    "    image: Image.Image,\n",
    "    box: Tensor,\n",
    "    augment: bool,\n",
    "    device: device,\n",
    "    target_size: int = 224,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Same transformation as in the CLIP preprocess function\n",
    "    if augment:\n",
    "        trans = A.Compose(\n",
    "            transforms=[\n",
    "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
    "                A.CenterCrop(\n",
    "                    target_size,\n",
    "                    target_size,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                    max_pixel_value=255.0,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0),\n",
    "                A.GaussianBlur(p=1),\n",
    "                A.PixelDropout(dropout_prob=0.02),\n",
    "                A.Rotate(limit=20),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[]),\n",
    "        )\n",
    "    else:\n",
    "        trans = A.Compose(\n",
    "            transforms=[\n",
    "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
    "                A.CenterCrop(\n",
    "                    target_size,\n",
    "                    target_size,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                    max_pixel_value=255.0,\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[]),\n",
    "        )\n",
    "\n",
    "    transformed_sample: Dict[str, Any] = trans(\n",
    "        image=np.array(image), bboxes=box.tolist()\n",
    "    )\n",
    "\n",
    "    bbox_tensor: Tensor = (\n",
    "        torch.tensor(transformed_sample[\"bboxes\"][0], requires_grad=True) / target_size\n",
    "    )\n",
    "    # print(bbox_tensor)\n",
    "    return transformed_sample[\"image\"], bbox_tensor.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple forward pass on the pretrained CLIP text encoder\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: CLIP = clip.load(\"RN50\", device=self.device)[0]\n",
    "        self.pretrained_model.float()\n",
    "\n",
    "        # Freeze the backbone\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, tokenized_caption: Tensor) -> Tensor:\n",
    "        out: Tensor = self.pretrained_model.encode_text(tokenized_caption).to(\n",
    "            self.device\n",
    "        )\n",
    "        # .unsqueeze(1)\n",
    "        return out.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that gets output for all layes of the backbone\n",
    "# CLIP backbone is a modified ResNet with an attention layer for global pooling\n",
    "\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: ModifiedResNet = clip.load(\"RN50\", device=self.device)[\n",
    "            0\n",
    "        ].visual  # type: ignore\n",
    "        self.pretrained_model.float()\n",
    "        assert isinstance(self.pretrained_model, ModifiedResNet)\n",
    "\n",
    "        # Freeze the backbone\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Register hooks to get the output of all layers\n",
    "        self.layers_outputs: OrderedDict[str, Tensor] = OrderedDict()\n",
    "        self.pretrained_model.layer1.register_forward_hook(self.hook_fn(\"layer1\"))  # type: ignore\n",
    "        self.pretrained_model.layer2.register_forward_hook(self.hook_fn(\"layer2\"))  # type: ignore\n",
    "        self.pretrained_model.layer3.register_forward_hook(self.hook_fn(\"layer3\"))  # type: ignore\n",
    "        self.pretrained_model.layer4.register_forward_hook(self.hook_fn(\"layer4\"))  # type: ignore\n",
    "\n",
    "        # Project the output of each layer to the same dimensionality as the text features\n",
    "        cfg = Config.get_instance().visual_encoder  # type: ignore\n",
    "        resnet_resolution: int = cfg[\"resnet_resolution\"]\n",
    "        resnet_channels: int = cfg[\"resnet_channels\"]\n",
    "\n",
    "        self.layers_projections = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            resnet_resolution //= 2\n",
    "            in_features: int = resnet_channels * resnet_resolution * resnet_resolution\n",
    "            resnet_channels *= 2\n",
    "            layer_projection: nn.Sequential = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(resnet_resolution),\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Linear(in_features, cfg[\"output_dim\"], device=self.device),\n",
    "                # nn.LayerNorm(\n",
    "                #     cfg[\"output_dim\"], eps=1e-3, device=self.device\n",
    "                # ),\n",
    "                # nn.ReLU(),\n",
    "            )\n",
    "            self.layers_projections.append(layer_projection)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, batch: Tensor) -> OrderedDict[str, Tensor]:\n",
    "        # Reset the dictionary\n",
    "        self.layers_outputs = OrderedDict()\n",
    "\n",
    "        out: Tensor = self.pretrained_model(batch)\n",
    "        # .unsqueeze(1)\n",
    "\n",
    "        for idx, (layer_name, layer_output) in enumerate(self.layers_outputs.items()):\n",
    "            self.layers_outputs[layer_name] = self.layers_projections[idx](layer_output)\n",
    "            # .unsqueeze(1)\n",
    "        self.layers_outputs[\"output\"] = out\n",
    "\n",
    "        return self.layers_outputs\n",
    "\n",
    "    def hook_fn(self, layer: str) -> Callable[[nn.Module, Tensor, Tensor], None]:\n",
    "        def hook(module: nn.Module, input: Tensor, output: Tensor) -> None:\n",
    "            # print(f\"Module: {[module for  module in module.modules()]}\")\n",
    "            self.layers_outputs[layer] = output.requires_grad_(True)\n",
    "\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is composed of the visual encoder and the text encoder described above\n",
    "# The attention layer is used to compute the attention between the the text and each layer of visual features\n",
    "\n",
    "\n",
    "class VGModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlp_hidden_dim_1: int,\n",
    "        mlp_hidden_dim_2: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        cfg = Config.get_instance().model  # type: ignore\n",
    "        emb_dim: int = cfg[\"emb_dim\"]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.visual_backbone: VisualEncoder = VisualEncoder().to(self.device)\n",
    "        self.text_encoder: TextEncoder = TextEncoder().to(self.device)\n",
    "        self.attention_layers: nn.ModuleList = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(\n",
    "                    embed_dim=emb_dim,\n",
    "                    num_heads=4,\n",
    "                    batch_first=True,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                for _ in range(5)\n",
    "            ]\n",
    "        )\n",
    "        self.reg_head: MLP = MLP(\n",
    "            emb_dim * 5, 4, hidden_dim_1=mlp_hidden_dim_1, hidden_dim_2=mlp_hidden_dim_2\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, batch: List[BatchSample]) -> Tensor:\n",
    "        captions: Tensor = (\n",
    "            torch.stack([sample.caption for sample in batch]).squeeze(1).to(self.device)\n",
    "        )\n",
    "        text_features: Tensor = self.text_encoder(captions).unsqueeze(1)\n",
    "\n",
    "        images: Tensor = torch.stack([sample.image for sample in batch]).to(self.device)\n",
    "        visual_features: OrderedDict[str, Tensor] = self.visual_backbone(images)\n",
    "\n",
    "        attended_features: List[Tensor] = []\n",
    "        for i, visual_feature in enumerate(visual_features.values()):\n",
    "            visual_feature: Tensor = visual_feature.unsqueeze(1)\n",
    "            # print(visual_feature.shape, text_features.shape)\n",
    "            attended_feature: Tensor = self.attention_layers[i](\n",
    "                query=text_features, key=visual_feature, value=visual_feature\n",
    "            )[0].squeeze(1)\n",
    "            attended_features.append(attended_feature)\n",
    "\n",
    "        aggregated_features: Tensor = torch.cat(attended_features, dim=1)\n",
    "\n",
    "        return self.reg_head(aggregated_features)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, output_dim: int, hidden_dim_1: int, hidden_dim_2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_2, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, l: float) -> None:\n",
    "        self.l1_loss = nn.SmoothL1Loss(reduction=\"mean\")\n",
    "        self.giou_loss = generalized_box_iou_loss\n",
    "        self.l: float = l\n",
    "        self.loss: Tensor\n",
    "\n",
    "    def compute(self, out: Tensor, bbox: Tensor) -> Tensor:\n",
    "        self.loss = self.giou_loss(out, bbox, reduction=\"mean\") + self.l * self.l1_loss(\n",
    "            out, bbox\n",
    "        )\n",
    "        return self.loss\n",
    "\n",
    "    def to_float(self) -> float:\n",
    "        return self.loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(samples: List[Sample], predictions: Tensor) -> None:\n",
    "    ncols = 3\n",
    "    nrows = int(len(samples) / ncols)\n",
    "    print(nrows, ncols)\n",
    "    _, ax = plt.subplots(nrows, ncols, figsize=(24, 24))\n",
    "    for i, sample in enumerate(samples):\n",
    "        img: Tensor = read_image(sample.image_path)\n",
    "        # bboxes: Tensor = torch.stack(\n",
    "        #     [\n",
    "        #         unnormalize_bbox(img, sample.bounding_box),\n",
    "        #         unnormalize_bbox(img, predictions[i]),\n",
    "        #     ]\n",
    "        # )\n",
    "        bboxes: Tensor = torch.stack(\n",
    "            [\n",
    "                sample.bounding_box,\n",
    "                predictions[i],\n",
    "            ]\n",
    "        ).squeeze(1)\n",
    "        result: Tensor = draw_bounding_boxes(img, bboxes, width=2, colors=(255, 0, 0))\n",
    "        ax[i // ncols, i % ncols].imshow(result.permute(1, 2, 0))\n",
    "        ax[i // ncols, i % ncols].set_title(sample.caption)\n",
    "        ax[i // ncols, i % ncols].axis(\"off\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def unnormalize_bbox(image: Tensor, bbox: Tensor) -> Tensor:\n",
    "    x: int\n",
    "    y: int\n",
    "    y, x = image.shape[1], image.shape[2]\n",
    "    xmin, ymin, xmax, ymax = bbox.squeeze(0)\n",
    "    xmin_unnorm: float = xmin.item() * x\n",
    "    ymin_unnorm: float = ymin.item() * y\n",
    "    xmax_unnorm: float = xmax.item() * x\n",
    "    ymax_unnorm: float = ymax.item() * y\n",
    "    return torch.tensor([xmin_unnorm, ymin_unnorm, xmax_unnorm, ymax_unnorm])\n",
    "\n",
    "\n",
    "def visualize_network(model: torch.nn.Module, batch: List[BatchSample]) -> None:\n",
    "    output: Tensor = model(batch)\n",
    "    make_dot(\n",
    "        output.mean(), params=dict(model.named_parameters()), show_attrs=True\n",
    "    ).render(\"model_graph\", directory=\"../runs\", format=\"png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    loss: Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tensor:\n",
    "    # As loss we take smooth_l1 + GIoU\n",
    "    epoch_loss_list: List[Tensor] = []\n",
    "\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        # Loss and metrics\n",
    "        batch_loss: Tensor = loss.compute(out, bbox)\n",
    "        epoch_loss_list.append(batch_loss)\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return torch.stack(epoch_loss_list).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    # As accuracy we take the average IoU\n",
    "    model.eval()\n",
    "    accuracy_list: List[Tensor] = []\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        accuracy_list.append(torch.diagonal(box_iou(out, bbox)).mean())\n",
    "\n",
    "    return torch.stack(accuracy_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: Trial) -> float:\n",
    "    cfg = Config.get_instance()  # type: ignore\n",
    "    train_dataset: VGDataset = VGDataset(\n",
    "        dir_path=cfg.dataset_path,\n",
    "        split=Split.TRAIN,\n",
    "        output_bbox_type=BboxType.XYXY,\n",
    "        augment=True,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Train dataset created. Dataset length \", len(train_dataset))\n",
    "\n",
    "    val_dataset: VGDataset = VGDataset(\n",
    "        dir_path=cfg.dataset_path,\n",
    "        split=Split.VAL,\n",
    "        output_bbox_type=BboxType.XYXY,\n",
    "        augment=False,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Validation dataset created. Dataset length: \", len(val_dataset))\n",
    "\n",
    "    batch_size = trial.suggest_int(\n",
    "        \"batch_size\",\n",
    "        1,\n",
    "        10,\n",
    "    )\n",
    "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Loss is the weighted sum of the smooth l1 loss and the GIoU\n",
    "    l = trial.suggest_float(\"l\", 0.0, 1.0)\n",
    "    loss_func = Loss(l)\n",
    "    losses_list: List[float] = []\n",
    "    accuracies_list: List[float] = []\n",
    "\n",
    "    hidden_dim_1 = trial.suggest_int(\"hidden_dim_1\", 512, 2048)\n",
    "    hidden_dim_2 = trial.suggest_int(\"hidden_dim_2\", 128, 512)\n",
    "    if cfg.logging[\"resume\"]:\n",
    "        checkpoint: Dict[str, Any] = torch.load(cfg.logging[\"path\"] + \"model.pth\")\n",
    "        model = VGModel(hidden_dim_1, hidden_dim_2).to(device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=cfg.model[\"gamma\"]\n",
    "        )\n",
    "        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n",
    "        start_epoch: int = checkpoint[\"epoch\"]\n",
    "        losses_list.append(checkpoint[\"loss\"])\n",
    "    else:\n",
    "        model = VGModel(hidden_dim_1, hidden_dim_2).train()\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=cfg.model[\"gamma\"]\n",
    "        )\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, cfg.epochs), desc=\"Epochs\"):\n",
    "        print(\"-------------------- Training --------------------------\")\n",
    "        epoch_loss = train_one_epoch(\n",
    "            train_dataloader, model, loss_func, optimizer, device\n",
    "        )\n",
    "        losses_list.append(epoch_loss.item())\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set for hyperparameter tuning\n",
    "        print(\"-------------------- Validation ------------------------\")\n",
    "        accuracy = validate(val_dataloader, model, device)\n",
    "        accuracies_list.append(accuracy)\n",
    "        trial.report(accuracy, epoch)\n",
    "        print(f\"Accuracy: {accuracy} at epoch {epoch}\")\n",
    "\n",
    "        # Early stopping for non promising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        # Save model after each epoch\n",
    "        if cfg.logging[\"save_model\"]:\n",
    "            dir: str = cfg.logging[\"path\"]\n",
    "            if not os.path.exists(dir):\n",
    "                os.makedirs(dir)\n",
    "\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "                    \"loss\": epoch_loss,\n",
    "                },\n",
    "                f=f\"{cfg.logging['path']}model.pth\",\n",
    "            )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return sum(accuracies_list) / len(accuracies_list)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "    study_name = \"train.db\"\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=f\"sqlite:///{study_name}\",\n",
    "        direction=\"maximize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=1, timeout=600)\n",
    "\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best hyperparameters: {trial.params}\")\n",
    "    fig = plot_optimization_history(study)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
