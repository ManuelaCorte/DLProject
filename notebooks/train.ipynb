{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ho5gQB-WyFD"
      },
      "source": [
        "# Deep Learning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iYWgolAWyFE"
      },
      "source": [
        "Group members:\n",
        "- Astolfi Alex\n",
        "- Corte Pause Manuela\n",
        "- Zocca Guglielmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GcZeFOF6WyFE",
        "outputId": "1e70f3fc-ff06-4d90-f2b1-205f5713f7dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m916.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.163-py3-none-any.whl (610 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m610.8/610.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.8.0.76)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (16.0.6)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=be7ecfbb11586fcdd8acd654fee2dc1f149502fb84f8415397985c9f2ea0b890\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, ftfy, docker-pycreds, gitdb, GitPython, wandb, ultralytics\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 ftfy-6.1.1 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 ultralytics-8.0.163 wandb-0.15.8\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-00ixn5u9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-00ixn5u9\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=eb115c10a12b4c3288b8c20da08e6b57d7ec2195f46094f82bee61fe3e4cd7d1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-le3nvj3h/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install ftfy regex tqdm ultralytics wandb albumentations\n",
        "%pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Trh7NeO-WyFF",
        "outputId": "12079dbf-3d77-4230-fcc6-ce6c1e98ecc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING ⚠️ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
            "WARNING ⚠️ 'ultralytics.yolo.engine' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.engine' instead.\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import gzip\n",
        "import html\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from functools import lru_cache\n",
        "from pprint import pprint\n",
        "from typing import (\n",
        "    Any,\n",
        "    Callable,\n",
        "    Dict,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Optional,\n",
        "    OrderedDict,\n",
        "    Tuple,\n",
        "    Union,\n",
        ")\n",
        "\n",
        "import albumentations as A\n",
        "import clip\n",
        "import cv2\n",
        "import ftfy\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pkg_resources as p\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as FT\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from PIL import Image\n",
        "from torch import Tensor, device, tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops import (\n",
        "    box_convert,\n",
        "    box_iou,\n",
        "    distance_box_iou_loss,\n",
        "    generalized_box_iou_loss,\n",
        ")\n",
        "from tqdm.notebook import tqdm\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.yolo.engine.results import Results\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gLrUwbwwWyFF",
        "outputId": "219ba2bf-6723-454e-c21c-a36d28116823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
            "To: /content/data/raw/refcocog.tar.gz\n",
            "100%|██████████| 13.5G/13.5G [02:10<00:00, 103MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "# Download dataset and save under data/raw/ only if not already downloaded\n",
        "url = \"https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\"\n",
        "if not os.path.exists(\"data/raw/refcocog.tar.gz\"):\n",
        "    print(\"Downloading dataset...\")\n",
        "    gdown.download(url=url, output=\"data/raw/\", quiet=False, resume=True)\n",
        "if not os.path.exists(\"data/raw/refcocog/\"):\n",
        "    print(\"Extracting dataset...\")\n",
        "    !tar -xf data/raw/refcocog.tar.gz -C data/raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dB5jYCKWyFG"
      },
      "source": [
        "Instead of linking the images with the corresponsing captions and bounding boxes each time we need to run the code, we created json files, one of each spilt, that contain the image paths and the corresponding captions and bounding boxes. This way we can load the data faster and easier. If we prefer to create the dataset on the fly, we just need to put the preprocess option in the dataset constructor to False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KfzS01iEWyFG",
        "outputId": "d04de53c-b766-4dd3-8cf9-a55e356d1039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1ln_0a6iAslC391-K3CUg1-6Wn2I9vWTn test_samples.json\n",
            "Processing file 1lpyWzCA3qZi6YDjFQCxIKGSs446K2luL train_samples.json\n",
            "Processing file 1vE8Rmsi3bevdbpPJ9jyM6AVARczV407k val_samples.json\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ln_0a6iAslC391-K3CUg1-6Wn2I9vWTn\n",
            "To: /content/data/processed/test_samples.json\n",
            "100%|██████████| 1.20M/1.20M [00:00<00:00, 78.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lpyWzCA3qZi6YDjFQCxIKGSs446K2luL\n",
            "To: /content/data/processed/train_samples.json\n",
            "100%|██████████| 10.1M/10.1M [00:00<00:00, 49.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vE8Rmsi3bevdbpPJ9jyM6AVARczV407k\n",
            "To: /content/data/processed/val_samples.json\n",
            "100%|██████████| 735k/735k [00:00<00:00, 104MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/processed/test_samples.json',\n",
              " 'data/processed/train_samples.json',\n",
              " 'data/processed/val_samples.json']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Download preprocessed dataset\n",
        "url = \"https://drive.google.com/drive/folders/1jaJV40dneOckZn7WHMQyd2jBh7A8534N\"\n",
        "gdown.download_folder(url=url, output=\"data/\", quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dJiZdTa7WyFG",
        "outputId": "95745b47-b842-4f88-c9a0-a2a20969c378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-27 14:59:50--  https://raw.githubusercontent.com/ManuelaCorte/DLProject/master/config.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600 [text/plain]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "\rconfig.json           0%[                    ]       0  --.-KB/s               \rconfig.json         100%[===================>]     600  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-27 14:59:50 (37.3 MB/s) - ‘config.json’ saved [600/600]\n",
            "\n",
            "--2023-08-27 14:59:50--  https://raw.githubusercontent.com/ManuelaCorte/DLProject/master/sweep_config.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 508 [text/plain]\n",
            "Saving to: ‘sweep_config.json’\n",
            "\n",
            "sweep_config.json   100%[===================>]     508  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-27 14:59:51 (38.6 MB/s) - ‘sweep_config.json’ saved [508/508]\n",
            "\n",
            "--2023-08-27 14:59:51--  https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.38, 13.107.213.38, 2620:1ec:bdf::38, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 255827503 (244M) [application/octet-stream]\n",
            "Saving to: ‘RN50.pt’\n",
            "\n",
            "RN50.pt             100%[===================>] 243.98M   172MB/s    in 1.4s    \n",
            "\n",
            "2023-08-27 14:59:52 (172 MB/s) - ‘RN50.pt’ saved [255827503/255827503]\n",
            "\n",
            "--2023-08-27 14:59:52--  https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/openai/CLIP/main/clip/bpe_simple_vocab_16e6.txt.gz [following]\n",
            "--2023-08-27 14:59:53--  https://raw.githubusercontent.com/openai/CLIP/main/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-08-27 14:59:53 (24.3 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download config files, model and tokenizer\n",
        "!wget https://raw.githubusercontent.com/ManuelaCorte/DLProject/master/config.json\n",
        "!wget https://raw.githubusercontent.com/ManuelaCorte/DLProject/master/sweep_config.json\n",
        "!wget https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\n",
        "!wget https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ySfi3AWyFG"
      },
      "source": [
        "Since the code was originally structured as a [repo](https://github.com/ManuelaCorte/DLProject), the following cell is used simply to recreate the original structure so that the relative imports are still valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qyAxcQfEWyFH",
        "outputId": "00289ae3-f75d-49d5-f3cd-f297c0b6b91a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/src\n"
          ]
        }
      ],
      "source": [
        "%mkdir src\n",
        "%cd src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVoJVVEFWyFH"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "TODO: add description of the RefCoCog dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ElxeimWyFH"
      },
      "source": [
        "Below are some utility classes that define the structure of a dataset sample (i.e. image path, bounding box and caption) and that of a batch sample (i.e. image tensor and tokenized caption).\n",
        "\n",
        "**Sample class**\n",
        "- `image_path`: relative path to the image in the dataset\n",
        "- `caption`: from all sentences describing the image we choose the longest one as it should be the one containing the most information\n",
        "- `bbox`: bounding box of the object of interest in the image in the format [x, y, w, h]\n",
        "\n",
        "**BatchSample class**\n",
        "- `image`: tensor of shape (3, 224, 224) containing the image\n",
        "- `caption`: the caption is first extended the prompt *\"find the region that corresponds to the description {caption}\"* similarly to what was done in [Jiang, 2022](http://arxiv.org/abs/2203.08481) and then turned into a list of integers using CLIP's tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1effLTfBWyFH"
      },
      "outputs": [],
      "source": [
        "class Sample:\n",
        "    def __init__(self, image_path: str, caption: str, bounding_box: Tensor) -> None:\n",
        "        self.image_path = image_path\n",
        "        self.caption = caption\n",
        "        self.bounding_box = bounding_box\n",
        "\n",
        "    def as_dict(self) -> dict[str, Any]:\n",
        "        return {\n",
        "            \"image_path\": self.image_path,\n",
        "            \"caption\": self.caption,\n",
        "            \"bounding_box\": self.bounding_box.tolist(),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def fromJSON(json: dict[str, Any]) -> Any:\n",
        "        return Sample(json[\"image_path\"], json[\"caption\"], Tensor(json[\"bounding_box\"]))\n",
        "\n",
        "\n",
        "class BatchSample:\n",
        "    def __init__(self, image: Tensor, caption: Tensor) -> None:\n",
        "        self.image: Tensor = image\n",
        "        self.caption: Tensor = caption\n",
        "\n",
        "    def to(self, device: device | str) -> Any:\n",
        "        return self.__class__(self.image.to(device), self.caption.to(device))\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"BatchSample(image={self.image.shape}, caption={self.caption.shape})\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Split(Enum):\n",
        "    TRAIN = \"train\"\n",
        "    VAL = \"val\"\n",
        "    TEST = \"test\"\n",
        "\n",
        "\n",
        "# Used in the baseline model\n",
        "# Returns the bounding box with the highest score\n",
        "# among all the bounding boxes detected by YOLO\n",
        "@dataclass(frozen=True)\n",
        "class Result:\n",
        "    bounding_box: Tensor\n",
        "    score: Tensor\n",
        "\n",
        "\n",
        "# XYXY: top left and bottom right corners\n",
        "# XYWH: top left corner, width and height\n",
        "# CXCWH: center coordinates, width and height\n",
        "@dataclass(frozen=True)\n",
        "class BboxType(Enum):\n",
        "    XYXY = \"xyxy\"\n",
        "    XYWH = \"xywh\"\n",
        "    CXCWH = \"cxcwh\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return super().__str__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "spKD6lFbWyFH"
      },
      "outputs": [],
      "source": [
        "# The Dataset contains samples with an image with a bounding box and a caption associated with the bounding box.\n",
        "class VGDataset(Dataset[Tuple[BatchSample, Tensor]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dir_path: str,\n",
        "        split: Split,\n",
        "        output_bbox_type: BboxType,\n",
        "        # Augmentation is only applied to the training set\n",
        "        augment: bool,\n",
        "        # Necessary for using with both our model and the baseline\n",
        "        transform: bool = True,\n",
        "        preprocessed: bool = False,\n",
        "        preprocessed_path: str = \"../data/processed/\",\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.dir_path: str = dir_path\n",
        "        self.split: Split = split\n",
        "        self.output_bbox_type: BboxType = output_bbox_type\n",
        "        self.augment: bool = augment\n",
        "        self.transform: bool = transform\n",
        "        self.device: device = torch.device(\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "        # Load preprocessed dataset\n",
        "        if preprocessed:\n",
        "            preprocess(dir_path, preprocessed_path, output_bbox_type)\n",
        "            with open(\n",
        "                preprocessed_path + f\"{self.split.value}_samples.json\", \"rb\"\n",
        "            ) as samples:\n",
        "                self.samples: List[Sample] = json.load(\n",
        "                    samples, object_hook=Sample.fromJSON\n",
        "                )\n",
        "        else:\n",
        "            self.samples: List[Sample] = self.get_samples()  # type: ignore\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, ref_id: int) -> Tuple[BatchSample, Tensor]:\n",
        "        # add prefix to caption as described in the pseudoQ paper\n",
        "        extended_caption: str = f\"find the region that corresponds to the description {self.samples[ref_id].caption}\"\n",
        "        caption: Tensor = tokenize(extended_caption, truncate=True)  # type: ignore\n",
        "        if self.transform:\n",
        "            image, bbox = transform_sample(\n",
        "                Image.open(self.samples[ref_id].image_path),\n",
        "                self.samples[ref_id].bounding_box,\n",
        "                self.augment,\n",
        "            )\n",
        "        else:\n",
        "            image = read_image(self.samples[ref_id].image_path)\n",
        "            bbox = torch.tensor([self.samples[ref_id].bounding_box])\n",
        "        return BatchSample(image, caption), bbox\n",
        "\n",
        "    def get_samples(self) -> List[Sample]:\n",
        "        with open(self.dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
        "            self.dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
        "        ) as refs:\n",
        "            instances = json.load(inst)\n",
        "            references = pickle.load(refs)\n",
        "\n",
        "        samples: List[Sample] = []\n",
        "        for ref in references:\n",
        "            if self.split.value == ref[\"split\"]:\n",
        "                image_path = self.get_image_path(ref[\"image_id\"], instances)\n",
        "                caption = self.get_caption(ref[\"sentences\"])\n",
        "                bbox = self.get_bounding_box(ref[\"ann_id\"], instances)\n",
        "                samples.append(Sample(image_path, caption, bbox))\n",
        "        return samples\n",
        "\n",
        "    def get_image_path(self, img_id: int, instances: Dict[str, Any]) -> str:\n",
        "        # Itereate over all images until we find the one with the given id\n",
        "        image_name = next(\n",
        "            image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
        "        )\n",
        "        path = self.dir_path + \"images/\" + image_name\n",
        "        return path\n",
        "\n",
        "    def get_caption(self, captions: List[Dict[str, Any]]) -> str:\n",
        "        longest_caption = captions[0]\n",
        "        for caption in captions:\n",
        "            if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
        "                longest_caption = caption\n",
        "        return longest_caption[\"sent\"]\n",
        "\n",
        "    # Bounding boxed converted to different formats\n",
        "    def get_bounding_box(self, ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
        "        bbox = next(\n",
        "            ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id\n",
        "        )\n",
        "        bounding_box: Tensor = tensor([])\n",
        "        match self.output_bbox_type.name:\n",
        "            case BboxType.XYXY.name:\n",
        "                bounding_box = box_convert(\n",
        "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
        "                )\n",
        "            case BboxType.XYWH.name:\n",
        "                bounding_box = box_convert(\n",
        "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
        "                )\n",
        "            case BboxType.CXCWH.name:\n",
        "                bounding_box = box_convert(\n",
        "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
        "                )\n",
        "\n",
        "        return bounding_box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoaP4uQeWyFI"
      },
      "source": [
        "Since the return type of the `__get_item__()` method is a tuple and not a simple tensor we need to define a custom method to collate the samples in a batch. In particular, it takes a list of (BatchSample, bounding_box) tuples and returns a tuple containing a list of BatchSamples and a tensor containing the stacked bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w4IkSORHWyFI"
      },
      "outputs": [],
      "source": [
        "def custom_collate(\n",
        "    batch: List[Tuple[BatchSample, torch.Tensor]]\n",
        ") -> Tuple[List[BatchSample], torch.Tensor]:\n",
        "    bboxes: List[torch.Tensor] = []\n",
        "    samples: List[BatchSample] = []\n",
        "    for sample, bbox in batch:\n",
        "        samples.append(BatchSample(sample.image, sample.caption))\n",
        "        bboxes.append(bbox)\n",
        "    return samples, torch.stack(bboxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5EhQY3SWyFI"
      },
      "source": [
        "Code used to preprocess the dataset which is basically equvalent to the code used in the *VGDataset* class with the addition of saving the results in a json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U-IPbusAWyFI"
      },
      "outputs": [],
      "source": [
        "def get_samples(\n",
        "    dir_path: str, bbox_type: BboxType\n",
        ") -> Tuple[List[Sample], List[Sample], List[Sample]]:\n",
        "    with open(dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
        "        dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
        "    ) as refs:\n",
        "        instances = json.load(inst)\n",
        "        references = pickle.load(refs)\n",
        "    train_samples: List[Sample] = []\n",
        "    val_samples: List[Sample] = []\n",
        "    test_samples: List[Sample] = []\n",
        "    for ref in tqdm(references, desc=f\"Processing dataset\"):\n",
        "        image_path = get_image_path(dir_path, ref[\"image_id\"], instances)\n",
        "        caption = get_caption(ref[\"sentences\"])\n",
        "        bbox = get_bounding_box(ref[\"ann_id\"], instances, bbox_type)\n",
        "        split = ref[\"split\"]\n",
        "\n",
        "        # Iterate over all dataset and divide samples according to the split\n",
        "        match split:\n",
        "            case Split.TRAIN.value:\n",
        "                train_samples.append(Sample(image_path, caption, bbox))\n",
        "            case Split.VAL.value:\n",
        "                val_samples.append(Sample(image_path, caption, bbox))\n",
        "            case Split.TEST.value:\n",
        "                test_samples.append(Sample(image_path, caption, bbox))\n",
        "            case _:\n",
        "                raise ValueError(f\"Invalid split: {split}\")\n",
        "    return train_samples, val_samples, test_samples\n",
        "\n",
        "\n",
        "def get_image_path(dir_path: str, img_id: int, instances: Dict[str, Any]) -> str:\n",
        "    image_name = next(\n",
        "        image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
        "    )\n",
        "    path = dir_path + \"images/\" + image_name\n",
        "    return path\n",
        "\n",
        "\n",
        "def get_caption(captions: List[Dict[str, Any]]) -> str:\n",
        "    longest_caption = captions[0]\n",
        "    for caption in captions:\n",
        "        if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
        "            longest_caption = caption\n",
        "    return longest_caption[\"sent\"]\n",
        "\n",
        "\n",
        "# Bounding boxed converted to format compatible with yolo or torchvision\n",
        "def get_bounding_box(\n",
        "    ann_id: int, instances: Dict[str, Any], bbox_type: BboxType\n",
        ") -> Tensor:\n",
        "    bbox = next(ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id)\n",
        "    bounding_box: Tensor = tensor([])\n",
        "    match bbox_type.name:\n",
        "        case BboxType.XYXY.name:\n",
        "            bounding_box = box_convert(\n",
        "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
        "            )\n",
        "        case BboxType.XYWH.name:\n",
        "            bounding_box = box_convert(\n",
        "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
        "            )\n",
        "        case BboxType.CXCWH.name:\n",
        "            bounding_box = box_convert(\n",
        "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
        "            )\n",
        "\n",
        "    return bounding_box\n",
        "\n",
        "\n",
        "# If the files already exist, don't preprocess again\n",
        "def preprocess(in_path: str, out_path: str, bbox_type: BboxType) -> None:\n",
        "    if (\n",
        "        os.path.exists(f\"{out_path}train_samples.json\")\n",
        "        and os.path.exists(f\"{out_path}val_samples.json\")\n",
        "        and os.path.exists(f\"{out_path}test_samples.json\")\n",
        "    ):\n",
        "        return\n",
        "    train_samples, val_samples, test_samples = get_samples(in_path, bbox_type)\n",
        "\n",
        "    json.dump(\n",
        "        train_samples,\n",
        "        open(f\"{out_path}train_samples.json\", \"w\"),\n",
        "        default=Sample.as_dict,\n",
        "    )\n",
        "\n",
        "    json.dump(\n",
        "        val_samples,\n",
        "        open(f\"{out_path}val_samples.json\", \"w\"),\n",
        "        default=Sample.as_dict,\n",
        "    )\n",
        "\n",
        "    json.dump(\n",
        "        test_samples,\n",
        "        open(f\"{out_path}test_samples.json\", \"w\"),\n",
        "        default=Sample.as_dict,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX33wIpMWyFJ"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OxmLfw1WyFJ"
      },
      "source": [
        "The images are transformed the same way as in the original CLIP [Radford, 2020](http://arxiv.org/abs/2103.00020) in order to make them compatible with the CLIP model. In particular, we resize the images to 224x224, center crop them and normalize them using the mean and standard deviation provided in the paper.\n",
        "\n",
        "Augmenting images for visual grounding isn't as straightforward as augmenting for other tasks such as classification. In fact, we need to make sure that the bounding boxes are still valid after the transformations and that by modifying the image the text is still describing the image. For example, if a caption says \"a red car\" and we augment the image by inverting the colors, the caption will no longer be valid. Thus we decided to use a external library, [Albumentation](https://albumentations.ai/docs/), so that the bouding boxes are automatically transformed when the image is transformed.\n",
        "\n",
        "We used the following transformations as they are the ones that should not invalidate the captions:\n",
        "- `ColorJitter`: by changing the brightness, contrast and saturation but not the hue we can have augmented images that still are valid wrt the captions\n",
        "- `GaussianBlur`\n",
        "- `Rotate`: we limit the rotation up to 20 degrees so that the captions are still valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l9knsVpkWyFJ"
      },
      "outputs": [],
      "source": [
        "# Transform image according to CLIP preprocess function\n",
        "# Normalize bounding box coordinates to be independent of image size\n",
        "def transform_sample(\n",
        "    image: Image.Image,\n",
        "    box: Tensor,\n",
        "    augment: bool,\n",
        "    target_size: int = 224,\n",
        ") -> Tuple[Tensor, Tensor]:\n",
        "    # Deals with grayscale images\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # Same transformation as in the CLIP preprocess function + optional augmentation\n",
        "    # The used augmentation don't invalidate the caption wrt the image\n",
        "    if augment:\n",
        "        trans = A.Compose(\n",
        "            transforms=[\n",
        "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
        "                A.CenterCrop(\n",
        "                    target_size,\n",
        "                    target_size,\n",
        "                    always_apply=True,\n",
        "                ),\n",
        "                A.Normalize(\n",
        "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
        "                    max_pixel_value=255.0,\n",
        "                    always_apply=True,\n",
        "                ),\n",
        "                A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0),\n",
        "                A.GaussianBlur(),\n",
        "                A.PixelDropout(),\n",
        "                A.Rotate(limit=20),\n",
        "                ToTensorV2(),\n",
        "            ],\n",
        "            # coco format is [x_min, y_min, width, height]\n",
        "            bbox_params=A.BboxParams(format=\"coco\", label_fields=[]),\n",
        "        )\n",
        "    else:\n",
        "        trans = A.Compose(\n",
        "            transforms=[\n",
        "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
        "                A.CenterCrop(\n",
        "                    target_size,\n",
        "                    target_size,\n",
        "                    always_apply=True,\n",
        "                ),\n",
        "                A.Normalize(\n",
        "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
        "                    max_pixel_value=255.0,\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ],\n",
        "            # coco format is [x_min, y_min, width, height]\n",
        "            bbox_params=A.BboxParams(format=\"coco\", label_fields=[]),\n",
        "        )\n",
        "\n",
        "    transformed_sample: Dict[str, Any] = trans(\n",
        "        image=np.array(image), bboxes=box.tolist()\n",
        "    )\n",
        "\n",
        "    # Normalize bounding boxes in [0,1]\n",
        "    bbox_tensor: Tensor = torch.tensor(transformed_sample[\"bboxes\"][0]) / target_size\n",
        "\n",
        "    return transformed_sample[\"image\"], bbox_tensor.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBzIHo6sWyFJ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GvqcnFYWyFJ"
      },
      "source": [
        "### Backbone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-1cacUrWyFJ"
      },
      "source": [
        "Below is the code from [CLIP Repository](https://github.com/openai/CLIP). We describe here in details the changes made to the original model:\n",
        "- `Visual Encoder`: the model we used uses as visual encoder a ResNet50 with some minor modifications (TODO: list modifications). <br>\n",
        "\n",
        "We additionally modified the visual encoder that now return the intermediate representations from the different layers of the ResNet50 as well. In particular, we return the output of the second and third residual blocks. We skip the first layer as the information contained in it are too low level to be useful for grouding the final object. As for the final layer, we directly return the modified result from the **Attention Pooling** layer. <br>\n",
        "The **Attention Pooling** layer is modified by adding a residual connection around the attention pooling layer so that we give more importance to the final features from the ResNet50. Moreover, we also need to reshape the positional embeddings and the final output. (TODO: expand explanation)\n",
        "- `Text Encoder`: the original text encoder consisted of a Transformer [Vaswani, 2017](http://arxiv.org/abs/1706.03762) with ?? layers and then returned a global representation by selecting the higher value token for each element in the sequence. The only modification we added was returning the transformet output on top of the global representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nd9Fuv6lWyFJ"
      },
      "outputs": [],
      "source": [
        "@lru_cache()\n",
        "def default_bpe():\n",
        "    return os.path.join(globals()[\"_dh\"][0], \"bpe_simple_vocab_16e6.txt.gz\")\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = (\n",
        "        list(range(ord(\"!\"), ord(\"~\") + 1))\n",
        "        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
        "        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    )\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = default_bpe()):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges_lst = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n",
        "        merges_lst = merges_lst[1 : 49152 - 256 - 2 + 1]\n",
        "        merges = [tuple(merge.split()) for merge in merges_lst]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v + \"</w>\" for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(\"\".join(merge))\n",
        "        vocab.extend([\"<|startoftext|>\", \"<|endoftext|>\"])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {\n",
        "            \"<|startoftext|>\": \"<|startoftext|>\",\n",
        "            \"<|endoftext|>\": \"<|endoftext|>\",\n",
        "        }\n",
        "        self.pat = re.compile(\n",
        "            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\\\p{L}]+|[\\\\p{N}]|[^\\s\\\\p{L}\\\\p{N}]+\"\"\",\n",
        "            re.IGNORECASE,\n",
        "        )\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token + \"</w>\"\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
        "            bpe_tokens.extend(\n",
        "                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n",
        "            )\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = \"\".join([self.decoder[token] for token in tokens])\n",
        "        text = (\n",
        "            bytearray([self.byte_decoder[c] for c in text])\n",
        "            .decode(\"utf-8\", errors=\"replace\")\n",
        "            .replace(\"</w>\", \" \")\n",
        "        )\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ab43OGqPWyFK"
      },
      "outputs": [],
      "source": [
        "_tokenizer = SimpleTokenizer()\n",
        "\n",
        "\n",
        "def tokenize(\n",
        "    texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False\n",
        ") -> Union[torch.IntTensor, torch.LongTensor]:\n",
        "    \"\"\"\n",
        "    Returns the tokenized representation of given input string(s)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : Union[str, List[str]]\n",
        "        An input string or a list of input strings to tokenize\n",
        "\n",
        "    context_length : int\n",
        "        The context length to use; all CLIP models use 77 as the context length\n",
        "\n",
        "    truncate: bool\n",
        "        Whether to truncate the text in case its encoding is longer than the context length\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n",
        "    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.\n",
        "    \"\"\"\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
        "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
        "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
        "\n",
        "    result: Union[torch.IntTensor, torch.LongTensor]\n",
        "    if p.parse_version(torch.__version__) < p.parse_version(\"1.8.0\"):\n",
        "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)  # type: ignore\n",
        "    else:\n",
        "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)  # type: ignore\n",
        "\n",
        "    for i, tokens in enumerate(all_tokens):\n",
        "        if len(tokens) > context_length:\n",
        "            if truncate:\n",
        "                tokens = tokens[:context_length]\n",
        "                tokens[-1] = eot_token\n",
        "            else:\n",
        "                raise RuntimeError(\n",
        "                    f\"Input {texts[i]} is too long for context length {context_length}\"\n",
        "                )\n",
        "        result[i, : len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ln43yy_TWyFK"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Any, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes: int, planes: int, stride: int = 1) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.downsample = None\n",
        "        self.stride = stride\n",
        "\n",
        "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
        "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
        "            self.downsample = nn.Sequential(\n",
        "                OrderedDict(\n",
        "                    [\n",
        "                        (\"-1\", nn.AvgPool2d(stride)),\n",
        "                        (\n",
        "                            \"0\",\n",
        "                            nn.Conv2d(\n",
        "                                inplanes,\n",
        "                                planes * self.expansion,\n",
        "                                1,\n",
        "                                stride=1,\n",
        "                                bias=False,\n",
        "                            ),\n",
        "                        ),\n",
        "                        (\"1\", nn.BatchNorm2d(planes * self.expansion)),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.relu1(self.bn1(self.conv1(x)))\n",
        "        out = self.relu2(self.bn2(self.conv2(out)))\n",
        "        out = self.avgpool(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Class modified by adding a residual connection around the attention block\n",
        "# and reshaping the positional embeddings and the output\n",
        "# The attention block is the same as in the original paper and isn't finetuned\n",
        "class AttentionPool2d(nn.Module):\n",
        "    def __init__(\n",
        "        self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.spacial_dim = spacial_dim\n",
        "        self.positional_embedding = nn.Parameter(\n",
        "            torch.randn(spacial_dim**2 + 1, embed_dim) / embed_dim**0.5\n",
        "        )\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
        "        self.num_heads = num_heads\n",
        "        # residual\n",
        "        self.connect = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, output_dim, 1),\n",
        "            nn.BatchNorm2d(output_dim),\n",
        "        )\n",
        "\n",
        "    def resize_pos_embed(self, pos_embed, input_shape):\n",
        "        pos_h = pos_w = self.spacial_dim\n",
        "        # Skip the first position embedding as it is the CLS token\n",
        "        pos_embed_weight = pos_embed[:, (-1 * pos_h * pos_w) :, :]  # 1 HW C\n",
        "        pos_embed_weight = pos_embed_weight.permute(0, 2, 1)\n",
        "        return pos_embed_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        # Residual connection\n",
        "        residual = self.connect(x)\n",
        "\n",
        "        x = x.reshape(B, C, -1)  # B C HW\n",
        "        pos_embed = self.positional_embedding.unsqueeze(0)\n",
        "        pos_embed = self.resize_pos_embed(pos_embed, (H, W))  # 1 C HW\n",
        "        x = x + pos_embed.to(x.dtype)  # B C HW\n",
        "        x = x.permute(2, 0, 1)  # HW B C (seq_len, batch, embed_dim)\n",
        "        x, _ = F.multi_head_attention_forward(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            embed_dim_to_check=x.shape[-1],\n",
        "            num_heads=self.num_heads,\n",
        "            q_proj_weight=self.q_proj.weight,\n",
        "            k_proj_weight=self.k_proj.weight,\n",
        "            v_proj_weight=self.v_proj.weight,\n",
        "            in_proj_weight=None,\n",
        "            in_proj_bias=torch.cat(\n",
        "                [self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]\n",
        "            ),\n",
        "            bias_k=None,\n",
        "            bias_v=None,\n",
        "            add_zero_attn=False,\n",
        "            dropout_p=0,\n",
        "            out_proj_weight=self.c_proj.weight,\n",
        "            out_proj_bias=self.c_proj.bias,\n",
        "            use_separate_proj_weight=True,\n",
        "            training=self.training,\n",
        "            need_weights=False,\n",
        "        )\n",
        "        x = x.permute(1, 2, 0).reshape(B, -1, H, W)  # B C H W\n",
        "        x = x + residual\n",
        "        x = F.relu(x, True)\n",
        "\n",
        "        return x\n",
        "\n",
        "# The only modification done is returning the intermediate features from the residual layers\n",
        "class ModifiedResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
        "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
        "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
        "    - The final pooling layer is a QKV attention instead of an average pool\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers: List[int] | Tuple[int, int, int, int],\n",
        "        output_dim: int,\n",
        "        heads: int,\n",
        "        input_resolution: int = 224,\n",
        "        width: int = 64,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.input_resolution = input_resolution\n",
        "\n",
        "        # the 3-layer stem\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, width // 2, kernel_size=3, stride=2, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            width // 2, width // 2, kernel_size=3, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(width)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(2)\n",
        "\n",
        "        # residual layers\n",
        "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
        "        self.layer1 = self._make_layer(width, layers[0])\n",
        "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
        "\n",
        "        embed_dim = width * 32  # the ResNet feature dimension\n",
        "        self.attnpool = AttentionPool2d(\n",
        "            input_resolution // 32, embed_dim, heads, output_dim\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, planes: int, blocks: int, stride: int = 1) -> nn.Sequential:\n",
        "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
        "\n",
        "        self._inplanes = planes * Bottleneck.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(Bottleneck(self._inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        with torch.no_grad():\n",
        "\n",
        "            def stem(x: Tensor) -> Tensor:\n",
        "                x = self.relu1(self.bn1(self.conv1(x)))\n",
        "                x = self.relu2(self.bn2(self.conv2(x)))\n",
        "                x = self.relu3(self.bn3(self.conv3(x)))\n",
        "                x = self.avgpool(x)\n",
        "                return x\n",
        "\n",
        "            x = x.type(self.conv1.weight.dtype)\n",
        "            x_stem: Tensor = stem(x)\n",
        "            x1: Tensor = self.layer1(x_stem)\n",
        "            x2: Tensor = self.layer2(x1)\n",
        "            x3: Tensor = self.layer3(x2)\n",
        "            x4: Tensor = self.layer4(x3)\n",
        "\n",
        "        x_pooled: Tensor = self.attnpool(x4)\n",
        "\n",
        "        return (\n",
        "            x2,\n",
        "            x3,\n",
        "            x_pooled,\n",
        "        )  # B 512 H/8 W/8 (B, 1024, H/16, W/16) (B, 1024, H/32, W/32)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.LayerNorm):\n",
        "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        orig_type = input.dtype\n",
        "        ret = super().forward(input.type(torch.float32))\n",
        "        return ret.type(orig_type)\n",
        "\n",
        "\n",
        "class QuickGELU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "\n",
        "class ResidualAttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_model: int, n_head: int, attn_mask: Optional[torch.Tensor] = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
        "        self.ln_1 = LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
        "                    (\"gelu\", QuickGELU()),\n",
        "                    (\"c_proj\", nn.Linear(d_model * 4, d_model)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "        self.ln_2 = LayerNorm(d_model)\n",
        "        self.attn_mask = attn_mask\n",
        "\n",
        "    def attention(self, x: torch.Tensor):\n",
        "        self.attn_mask = (\n",
        "            self.attn_mask.to(dtype=x.dtype, device=x.device)\n",
        "            if self.attn_mask is not None\n",
        "            else None\n",
        "        )\n",
        "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x + self.attention(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        width: int,\n",
        "        layers: int,\n",
        "        heads: int,\n",
        "        attn_mask: Optional[torch.Tensor] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.width = width\n",
        "        self.layers = layers\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.resblocks(x)\n",
        "\n",
        "# Modifications:\n",
        "# - The encode_text method returns the transformer output on top of the global features\n",
        "# - Initialization of the residual connection added in the attention pooling\n",
        "class CLIP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        # vision\n",
        "        image_resolution: int,\n",
        "        vision_layers: Tuple[int, int, int, int],\n",
        "        vision_width: int,\n",
        "        # text\n",
        "        context_length: int,\n",
        "        vocab_size: int,\n",
        "        transformer_width: int,\n",
        "        transformer_heads: int,\n",
        "        transformer_layers: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.context_length = context_length\n",
        "\n",
        "        vision_heads = vision_width * 32 // 64\n",
        "        self.visual = ModifiedResNet(\n",
        "            layers=vision_layers,\n",
        "            output_dim=embed_dim,\n",
        "            heads=vision_heads,\n",
        "            input_resolution=image_resolution,\n",
        "            width=vision_width,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            width=transformer_width,\n",
        "            layers=transformer_layers,\n",
        "            heads=transformer_heads,\n",
        "            attn_mask=self.build_attention_mask(),\n",
        "        )\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "        self.positional_embedding = nn.Parameter(\n",
        "            torch.empty(self.context_length, transformer_width)\n",
        "        )\n",
        "        self.ln_final = LayerNorm(transformer_width)\n",
        "\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "\n",
        "        if isinstance(self.visual, ModifiedResNet):\n",
        "            if self.visual.attnpool is not None:\n",
        "                std = self.visual.attnpool.c_proj.in_features**-0.5\n",
        "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
        "                nn.init.kaiming_uniform_(self.visual.attnpool.connect[0].weight)\n",
        "\n",
        "            for resnet_block in [\n",
        "                self.visual.layer1,\n",
        "                self.visual.layer2,\n",
        "                self.visual.layer3,\n",
        "                self.visual.layer4,\n",
        "            ]:\n",
        "                for name, param in resnet_block.named_parameters():\n",
        "                    if name.endswith(\"bn3.weight\"):\n",
        "                        nn.init.zeros_(param)\n",
        "\n",
        "        proj_std = (self.transformer.width**-0.5) * (\n",
        "            (2 * self.transformer.layers) ** -0.5\n",
        "        )\n",
        "        attn_std = self.transformer.width**-0.5\n",
        "        fc_std = (2 * self.transformer.width) ** -0.5\n",
        "        for block in self.transformer.resblocks:\n",
        "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
        "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
        "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
        "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
        "\n",
        "        if self.text_projection is not None:\n",
        "            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n",
        "\n",
        "    def build_attention_mask(self):\n",
        "        # lazily create causal attention mask, with full attention between the vision tokens\n",
        "        # pytorch uses additive attention mask; fill with -inf\n",
        "        mask = torch.empty(self.context_length, self.context_length)\n",
        "        mask.fill_(float(\"-inf\"))\n",
        "        mask.triu_(1)  # zero out the lower diagonal\n",
        "        return mask\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.visual.conv1.weight.dtype\n",
        "\n",
        "    def encode_image(self, image) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        return self.visual(image.type(self.dtype))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_text(self, text) -> Tuple[Tensor, Tensor]:\n",
        "        x = self.token_embedding(text).type(\n",
        "            self.dtype\n",
        "        )  # B L D (batch size, sequence length=77, embed dim=512)\n",
        "\n",
        "        x = x + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # L B D\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # B L D\n",
        "        x = self.ln_final(x).type(self.dtype)  # B L D\n",
        "\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        # Embedding corresponding to the higher value token for each element in the sequence (eot_token)\n",
        "        global_repr: Tensor = (\n",
        "            x[torch.arange(x.shape[0]), text.argmax(dim=-1), :] @ self.text_projection\n",
        "        )  # [B, d_model=1024] @ [d_model, D] = [B, D]\n",
        "\n",
        "        # Transformer output, EOT embeddings\n",
        "        return x, global_repr\n",
        "\n",
        "    def forward(self, image, text) -> Tuple[Tensor, Tensor]:\n",
        "        image_features: Tensor = self.encode_image(image)[2]\n",
        "        text_features: Tensor = self.encode_text(text)[1]\n",
        "\n",
        "        # normalized features\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        # shape = [global_batch_size, global_batch_size]\n",
        "        return logits_per_image, logits_per_text\n",
        "\n",
        "\n",
        "def convert_weights(model: nn.Module):\n",
        "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
        "\n",
        "    def _convert_weights_to_fp16(l: nn.Module) -> None:\n",
        "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
        "            l.weight.data = l.weight.data.half()\n",
        "            if l.bias is not None:\n",
        "                l.bias.data = l.bias.data.half()\n",
        "\n",
        "        if isinstance(l, nn.MultiheadAttention):\n",
        "            for attr in [\n",
        "                *[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]],\n",
        "                \"in_proj_bias\",\n",
        "                \"bias_k\",\n",
        "                \"bias_v\",\n",
        "            ]:\n",
        "                tensor = getattr(l, attr)\n",
        "                if tensor is not None:\n",
        "                    tensor.data = tensor.data.half()\n",
        "\n",
        "        for name in [\"text_projection\", \"proj\"]:\n",
        "            if hasattr(l, name):\n",
        "                attr_module: nn.Module = getattr(l, name)\n",
        "                if attr_module is not None:\n",
        "                    attr_module.data = attr_module.data.half()\n",
        "\n",
        "    model.apply(_convert_weights_to_fp16)\n",
        "\n",
        "\n",
        "def build_model(state_dict: dict[str, Any]):\n",
        "    \"\"\"Build a CLIP model from a state dict\"\"\"\n",
        "    counts: List[int] = [\n",
        "        len(\n",
        "            set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))\n",
        "        )\n",
        "        for b in [1, 2, 3, 4]\n",
        "    ]\n",
        "    vision_layers: Tuple[int, int, int, int] = tuple(counts)  # type: ignore\n",
        "    vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
        "    output_width = round(\n",
        "        (state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5\n",
        "    )\n",
        "    assert (\n",
        "        output_width**2 + 1\n",
        "        == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
        "    )\n",
        "    image_resolution = output_width * 32\n",
        "\n",
        "    embed_dim: int = state_dict[\"text_projection\"].shape[1]\n",
        "    context_length: int = state_dict[\"positional_embedding\"].shape[0]\n",
        "    vocab_size: int = state_dict[\"token_embedding.weight\"].shape[0]\n",
        "    transformer_width: int = state_dict[\"ln_final.weight\"].shape[0]\n",
        "    transformer_heads: int = transformer_width // 64\n",
        "    transformer_layers: int = len(\n",
        "        set(\n",
        "            k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    model = CLIP(\n",
        "        embed_dim,\n",
        "        image_resolution,\n",
        "        vision_layers,\n",
        "        vision_width,\n",
        "        context_length,\n",
        "        vocab_size,\n",
        "        transformer_width,\n",
        "        transformer_heads,\n",
        "        transformer_layers,\n",
        "    )\n",
        "\n",
        "    # Add the new residual connection to the state dict\n",
        "    state_dict.update(\n",
        "        model.visual.attnpool.connect.state_dict(prefix=\"visual.attnpool.connect.\")\n",
        "    )\n",
        "\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "\n",
        "    convert_weights(model)\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQg-z_9IWyFL"
      },
      "source": [
        "### Fusion Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY0JDI1sWyFL"
      },
      "source": [
        "This architecture is heavily inspired by the one used in [Wang, 2022](http://arxiv.org/abs/2111.15174)\n",
        "\n",
        "The **FusionModule** allows to fuse the features obtained from the backbone both in the same modality than across them. In particular, both the final visual and text features are projected and then fused together by multplying them and then feeding them to a *norm_layer* (i.e. BatchNorm and Relu)\n",
        "$$ f_{fusion} = norm(W_{visual} \\cdot f_{visual} \\cdot W_{text} \\cdot f_{text}) $$\n",
        "where $f_{visual}$ and $f_{text}$ are the visual and text features respectively and $W_{visual}$ and $W_{text}$ are the projection layers for the visual and text features respectively.\n",
        "\n",
        "The obtained features are then concatenated with the features from the third residual block and fed to a convolutional layer and then again the same thing is done with the features from the second residual block. The features obtained fron the different layers are then concatenated and fed to a final convolutional layer to obtain an aggregate representation.\n",
        "$$ f_{l4} = upsample(f_{fusion}) $$\n",
        "$$ f_{l3} = conv(concat(f_{l4}, f_{l3})) $$\n",
        "$$ f_{l2} = maxpool(f_{l2}) $$\n",
        "$$ f_{l2} = conv(concat(f_{l3}, f_{l2})) $$\n",
        "$$ f_{agg} = conv(concat(f_{l4}, f_{l3}, f_{l2})) $$\n",
        "where $f_{l3}$ and $f_{l2}$ are the features from the third and second residual blocks respectively, $f_{agg}$ is the final aggregate representation and $conv$ is a sequence of a convolutional, batch normalization and relu layers.\n",
        "\n",
        "As described in [Liu, 2018](https://arxiv.org/abs/1807.03247) and [Wang, 2022](http://arxiv.org/abs/2111.15174), we additionally add a **CoordConv** module consisitng of a fixed 2D tensor containing the coordinates of the pixels. This way the model can learn to ignore the translation invariance of the image (i.e. it might help to recognize the same object on the left or on the right as different objects for visual grounding).\n",
        "$$ f_{final} = coordconv(f_{agg}) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "laQH462LWyFL"
      },
      "outputs": [],
      "source": [
        "class FusionModule(nn.Module):\n",
        "    def __init__(self, emb_dim: int, clip_emb_dim: int, proj_img_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(in_features=clip_emb_dim, out_features=clip_emb_dim),\n",
        "            nn.BatchNorm1d(clip_emb_dim, device=self.device),\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.vis_l4_projection = _conv_layer(\n",
        "            input_dim=clip_emb_dim,\n",
        "            output_dim=clip_emb_dim,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            device=self.device,\n",
        "        )[\n",
        "            :2\n",
        "        ]  # Remove ReLU\n",
        "\n",
        "        self.norm_layer = nn.Sequential(\n",
        "            nn.BatchNorm2d(\n",
        "                clip_emb_dim,\n",
        "                device=self.device,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.vis_l3_projection = _conv_layer(\n",
        "            input_dim=clip_emb_dim + clip_emb_dim,\n",
        "            output_dim=emb_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            device=self.device,\n",
        "        )\n",
        "        self.vis_l2_projection = _conv_layer(\n",
        "            input_dim=emb_dim + emb_dim,\n",
        "            output_dim=emb_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            device=self.device,\n",
        "        )\n",
        "        self.aggregation = _conv_layer(\n",
        "            input_dim=clip_emb_dim + emb_dim + emb_dim,\n",
        "            output_dim=emb_dim,\n",
        "            kernel_size=1,\n",
        "            padding=0,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        self.coord_conv = nn.Sequential(\n",
        "            CoordConv(emb_dim + 2, emb_dim),\n",
        "            _conv_layer(\n",
        "                input_dim=emb_dim,\n",
        "                output_dim=emb_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                device=self.device,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, visual_features: Tuple[Tensor, Tensor, Tensor], text_features: Tensor\n",
        "    ) -> Tensor:\n",
        "        visual_l2_features, visual_l3_features, visual_l4_features = visual_features\n",
        "        # Visual and text features projection\n",
        "        text_features_proj: Tensor = (\n",
        "            self.text_projection(text_features).unsqueeze(-1).unsqueeze(-1)\n",
        "        )  # B 1024 1 1\n",
        "        visual_l4_features_proj: Tensor = self.vis_l4_projection(\n",
        "            visual_l4_features\n",
        "        )  # B 1024 7 7\n",
        "\n",
        "        # First fusion l4 (B 1024 7 7) and text (B 1024)\n",
        "        fused_l4: Tensor = self.norm_layer(\n",
        "            visual_l4_features_proj * text_features_proj\n",
        "        )  # B 1024 7 7\n",
        "\n",
        "        # Second fusion l3 (B 512 14 14) and l4 (B 1024 7 7)\n",
        "        fused_l4_upsample: Tensor = nn.Upsample(scale_factor=2, mode=\"nearest\")(\n",
        "            fused_l4\n",
        "        )  # B 1024 14 14\n",
        "        cat_features: Tensor = torch.cat([visual_l3_features, fused_l4_upsample], dim=1)\n",
        "        fused_l3: Tensor = self.vis_l3_projection(cat_features)  # B 512 14 14\n",
        "\n",
        "        # Third fusion l2 (B 512 28 28) and l3 (B 512 14 14)\n",
        "        visual_l2_pooling: Tensor = nn.MaxPool2d(kernel_size=2, stride=2)(\n",
        "            visual_l2_features\n",
        "        )  # B 512 14 14\n",
        "        fused_l2: Tensor = self.vis_l2_projection(\n",
        "            torch.cat([fused_l3, visual_l2_pooling], dim=1)\n",
        "        )  # B 512 14 14\n",
        "\n",
        "        # Aggregate features\n",
        "        cat_visual_features: Tensor = torch.cat(\n",
        "            [fused_l2, fused_l3, fused_l4_upsample], dim=1\n",
        "        )  # B 2048 14 14\n",
        "        aggregated_features: Tensor = self.aggregation(\n",
        "            cat_visual_features\n",
        "        )  # B 512 14 14\n",
        "\n",
        "        # Add coordinate features\n",
        "        final_features: Tensor = self.coord_conv(aggregated_features)  # B 512 14 14\n",
        "        return final_features\n",
        "\n",
        "\n",
        "# Enables the model to \"ignore\" the translation invariant nature of the image\n",
        "class CoordConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels) -> None:\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.conv: nn.Sequential = _conv_layer(\n",
        "            input_dim=in_channels,\n",
        "            output_dim=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "    def add_coord(self, input: Tensor) -> Tensor:\n",
        "        b, _, h, w = input.size()\n",
        "\n",
        "        # Divide [-1, 1] interval into w and h equal parts respectively\n",
        "        x_range = torch.linspace(-1, 1, w, device=self.device)\n",
        "        y_range = torch.linspace(-1, 1, h, device=self.device)\n",
        "\n",
        "        # Similar to taking the cartesian product\n",
        "        y, x = torch.meshgrid(y_range, x_range)\n",
        "        y = y.expand([b, 1, -1, -1])\n",
        "        x = x.expand([b, 1, -1, -1])\n",
        "\n",
        "        coord_feat = torch.cat([x, y], 1)\n",
        "        input = torch.cat([input, coord_feat], 1)\n",
        "        return input\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.add_coord(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "# Utility function to create a convolutional layer with batch normalization and ReLU\n",
        "def _conv_layer(\n",
        "    input_dim: int,\n",
        "    output_dim: int,\n",
        "    kernel_size: int,\n",
        "    padding: int,\n",
        "    device: device,\n",
        ") -> nn.Sequential:\n",
        "    module = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=input_dim,\n",
        "            out_channels=output_dim,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            device=device,\n",
        "        ),\n",
        "        nn.BatchNorm2d(output_dim, device=device),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    nn.init.xavier_uniform_(module[0].weight)\n",
        "    return module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxD55d2tWyFL"
      },
      "source": [
        "### Decoder Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdW2u7M0WyFM"
      },
      "source": [
        "The **Decoder** is a simple transformer decoder [Vaswani, 2017](http://arxiv.org/abs/1706.03762) with 3 layers that takes as input the final representation from the **FusionModule** and computes the cross attention with the transformer output from the backbone **TextEncoder**. To both the text and visual features we add fixed positional embeddings.\n",
        "\n",
        "As the output of the module we don't take the entire transformer output but just the first token (i.e. regression token) that is trained together with the rest of the model similarly to what is done in [Deng, 2022](http://arxiv.org/abs/2104.08541) and should learn to summarize the entire sequence. The output is then fed to a MLP that outputs the final bounding box in xywh format.\n",
        "\n",
        "To help the training of the transformer we adopted the strategy described in [Liu, 2020](https://aclanthology.org/2020.emnlp-main.463.pdf) of using Pre-LN (i.e. the layer norm in each transformer block is moved inside the residual block) which also removes the need of having an initial learning rate warmup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zWTWjd3hWyFM"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        img_size: int,\n",
        "        clip_ctx_length: int,\n",
        "        nheads: int,\n",
        "        nlayers: int,\n",
        "        dim_feedforward: int,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Standard fixed sinusoidal positional encodings\n",
        "        self.pos_embedding_1d = PositionalEncoding1D(d_model, clip_ctx_length).to(\n",
        "            self.device\n",
        "        )\n",
        "        self.pos_embedding_2d = PositionalEncoding2D(d_model, img_size, img_size).to(\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        # Vanilla transformer decoder\n",
        "        # Cross attention between the fusion module output and the backebone text transformer output\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            decoder_layer=nn.TransformerDecoderLayer(\n",
        "                d_model=d_model,\n",
        "                nhead=nheads,\n",
        "                dim_feedforward=dim_feedforward,\n",
        "                batch_first=True,\n",
        "                norm_first=True,  # Less prone to vanishing gradients\n",
        "                device=self.device,\n",
        "            ),\n",
        "            num_layers=nlayers,\n",
        "            norm=nn.LayerNorm(d_model, device=self.device),\n",
        "        )\n",
        "\n",
        "        # Token then used as input to the final mlp\n",
        "        self.reg_token = nn.Parameter(torch.randn((1, 1, d_model), requires_grad=True))\n",
        "        nn.init.kaiming_normal_(self.reg_token, nonlinearity=\"relu\", mode=\"fan_out\")\n",
        "        self.register_parameter(\"reg_token\", self.reg_token)\n",
        "\n",
        "    def forward(self, vis: Tensor, text: Tensor) -> Tensor:\n",
        "        text_features: Tensor = self.pos_embedding_1d(text)\n",
        "\n",
        "        visual_features: Tensor = self.pos_embedding_2d(vis)\n",
        "\n",
        "        visual_features = visual_features.flatten(2).permute(0, 2, 1)  # B HW D\n",
        "\n",
        "        visual_features = torch.cat(\n",
        "            [self.reg_token.expand((vis.shape[0], -1, -1)), visual_features], dim=1\n",
        "        )\n",
        "        x = self.decoder(visual_features, text_features)\n",
        "        return x[:, 0, :]\n",
        "\n",
        "\n",
        "# Positional encodings implemented in separate classes if we want to change them and use learnable positional encodings instead\n",
        "# Dropout added following the original transformer implementation\n",
        "# https://github.com/wzlxjtu/PositionalEncoding2D\n",
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(self, d_model: int, window_len: int) -> None:\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1).to(self.device)\n",
        "        self.pos_encoding = torch.zeros(window_len, d_model, device=self.device)\n",
        "        position = torch.arange(0, window_len, device=self.device).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            (\n",
        "                torch.arange(0, d_model, 2, dtype=torch.float, device=self.device)\n",
        "                * -(math.log(10000.0) / d_model)\n",
        "            )\n",
        "        )\n",
        "        self.pos_encoding[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        self.pos_encoding[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "\n",
        "        self.register_buffer(\"text_pos_encoding\", self.pos_encoding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor) -> Tensor:\n",
        "        out = self.dropout(\n",
        "            token_embedding + self.pos_encoding[: token_embedding.size(1), :]\n",
        "        )\n",
        "        return out\n",
        "\n",
        "\n",
        "# First half of the encodings are used for the height and the second half for the width\n",
        "class PositionalEncoding2D(nn.Module):\n",
        "    def __init__(self, d_model: int, width: int, height: int) -> None:\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1).to(self.device)\n",
        "        self.pe = torch.zeros(d_model, height, width, device=self.device)\n",
        "        # Each dimension use half of d_model\n",
        "        d_model = int(d_model / 2)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0.0, d_model, 2, device=self.device)\n",
        "            * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pos_w = torch.arange(0.0, width, device=self.device).unsqueeze(1)\n",
        "        pos_h = torch.arange(0.0, height, device=self.device).unsqueeze(1)\n",
        "        self.pe[0:d_model:2, :, :] = (\n",
        "            torch.sin(pos_w * div_term)  # H d_model/4\n",
        "            .transpose(0, 1)\n",
        "            .unsqueeze(1)\n",
        "            .repeat(1, height, 1)\n",
        "        )  # d_model/4 H H\n",
        "        self.pe[1:d_model:2, :, :] = (\n",
        "            torch.cos(pos_w * div_term)\n",
        "            .transpose(0, 1)\n",
        "            .unsqueeze(1)\n",
        "            .repeat(1, height, 1)\n",
        "        )\n",
        "        self.pe[d_model::2, :, :] = (\n",
        "            torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "        )  # d_model/4 W W\n",
        "        self.pe[d_model + 1 :: 2, :, :] = (\n",
        "            torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "        )\n",
        "        self.register_buffer(\"visual_pos_encoding\", self.pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxRXjEgAWyFM"
      },
      "source": [
        "## Configurations\n",
        "Here is the code used to parse the config file containing the information about the dataset, the model and the training parameters. The config file is a json file that contains the following fields:\n",
        "- `Model`: contains all information about the layers and input and output sizes of the model\n",
        "- `Train`: contains all information about the training parameters and the hyperparameters. The *sweep* parameter is used to run a hyperparameter sweep on wandb for hyperparameter tuning.\n",
        "- `Logging`: contains the options to customize the logging of the model. In particular, it is possible to choose whether to log using wandb or not.\n",
        "\n",
        "Moreover, by explicitly modelling the field as classes, we can easily access the different fields using the dot notation throughout the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c6SIQcXpWyFM"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Model:\n",
        "    # CLIP parameters\n",
        "    clip_embed_dim: int\n",
        "    clip_ctx_length: int\n",
        "    img_size: int\n",
        "\n",
        "    # fusion and mlp parameters\n",
        "    mlp_hidden_dim: int\n",
        "    activation: str\n",
        "    proj_img_size: int\n",
        "\n",
        "    # Transformer parameters\n",
        "    embed_dim: int\n",
        "    decoder_layers: int\n",
        "    decoder_heads: int\n",
        "    decoder_dim_feedforward: int\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Train:\n",
        "    batch_size: int\n",
        "\n",
        "    # Optimizer parameters\n",
        "    lr: float\n",
        "    lr_backbone: float\n",
        "\n",
        "    # Scheduler parameters\n",
        "    step_size: int\n",
        "\n",
        "    # Loss parameters\n",
        "    l1: float\n",
        "    l2: float\n",
        "\n",
        "    # Hyperparameters search\n",
        "    sweep: bool\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Logging:\n",
        "    path: str\n",
        "    save: bool\n",
        "    resume: bool\n",
        "    wandb: bool\n",
        "\n",
        "\n",
        "class Config:\n",
        "    def __init__(self) -> None:\n",
        "        cfg: Dict[str, Any] = json.load(open(\"../config.json\", \"r\"))\n",
        "        self.dataset_path: str = cfg[\"dataset_path\"]\n",
        "        self.epochs: int = cfg[\"epochs\"]\n",
        "        self.model = Model(**cfg[\"model\"])\n",
        "        self.train = Train(**cfg[\"train\"])\n",
        "        self.logging = Logging(**cfg[\"logging\"])\n",
        "\n",
        "    def as_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"dataset_path\": self.dataset_path,\n",
        "            \"epochs\": self.epochs,\n",
        "            \"model\": self.model.__dict__,\n",
        "            \"train\": self.train.__dict__,\n",
        "        }\n",
        "\n",
        "    # if in other dict there are keys equal to the keys in self, update them\n",
        "    def update(self, other: Dict[str, Any]) -> None:\n",
        "        for k, v in other.items():\n",
        "            if k in self.__dict__:\n",
        "                self.__dict__[k] = v\n",
        "            if k in self.model.__dict__:\n",
        "                self.model.__dict__[k] = v\n",
        "            if k in self.train.__dict__:\n",
        "                self.train.__dict__[k] = v\n",
        "            if k in self.logging.__dict__:\n",
        "                self.logging.__dict__[k] = v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nig34X4qWyFN"
      },
      "source": [
        "### Hyperparameter Tuning\n",
        "We used [Weights and Biases](https://wandb.ai/) to run a hyperparameter sweep. In particular, we used the following hyperparameters:\n",
        "```\n",
        "{\n",
        "    \"batch_size\": { \"values\": [32, 64, 128] },\n",
        "    \"lr\": { \"min\": 1e-5, \"max\": 1e-3 },\n",
        "    \"lr_backbone\": { \"min\": 1e-6, \"max\": 1e-4 },\n",
        "    \"l1\": { \"min\": 3.0, \"max\": 7.0 },\n",
        "    \"l2\": { \"min\": 1.0, \"max\": 3.0 },\n",
        "    \"mlp_hidden_dim\": { \"values\": [64, 128, 256] }\n",
        "}\n",
        "```\n",
        "\n",
        "We conducted two separate sweep, one performing random search over the hyperparameters space and the other one based on Bayesian optimization. The results of the two sweeps are shown below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJmFAhuSWyFN"
      },
      "source": [
        "## Metrics and loss\n",
        "Below we describe which metrics where logged during training and used for the evaluation of the model.\n",
        "- `Loss`: the loss used is a weighted sum of the smooth L1 loss and the generalized intersection over union loss.\n",
        "$$ l = \\lambda_1 \\cdot L_{smoothL1} + \\lambda_2 \\cdot L_{giou} $$\n",
        "<br>The Generalized Intersection over Union [Rezatiofighi, 2019](http://arxiv.org/abs/1902.09630) is an improvement over the IoU as it is different from 0 even when the bounding boxes don't overlap the gradient is non-zero and is defined as:\n",
        "$$ GIoU = IoU - \\frac{|C \\setminus (A \\cup B) |}{|C|} $$\n",
        "where $C$ is the smallest convex box containing both $A$ and $B$.\n",
        "- `Accuracy@N`: the accuracy at level $N$ is the percentage of predictions that have a IoU greater than $N$ with the ground truth bounding box. The accuracy at level $N$ is computed for $N \\in \\{0.25, 0.5, 0.75, 0.9\\}$.\n",
        "- ,'IOU`: the average IoU between the predicted bounding boxes and the ground truth bounding boxes.\n",
        "- `Cosine Similarity`: cosine similarity computed on the normalized text and image features computed from CLIP [(Radford, 2021)](http://arxiv.org/abs/2103.00020). In particular, we compute the similarity between the text features and the image features obtained by cropping the image using both the predicted and the ground truth bounding boxes and then feeding them to CLIP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "i82i2D93WyFN"
      },
      "outputs": [],
      "source": [
        "class Loss:\n",
        "    def __init__(self, l1: float, l2: float) -> None:\n",
        "        self.l1_loss = nn.SmoothL1Loss(reduction=\"mean\")\n",
        "        self.iou_loss = generalized_box_iou_loss\n",
        "        self.l1: float = l1\n",
        "        self.l2: float = l2\n",
        "        self.loss: Tensor\n",
        "\n",
        "    # Both bounding boxex tensors are already converted in xyxy format to use the torchvision giou function\n",
        "    def compute(self, prediction: Tensor, gt_bbox: Tensor) -> Tensor:\n",
        "        self.loss = self.l1 * self.l1_loss(\n",
        "            gt_bbox, prediction\n",
        "        ) + self.l2 * self.iou_loss(gt_bbox, prediction, reduction=\"mean\")\n",
        "        return self.loss\n",
        "\n",
        "    def to_float(self) -> float:\n",
        "        return self.loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "p8QPKZcpWyFN"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Metric(Enum):\n",
        "    LOSS = \"loss\"\n",
        "    ACCURACY_25 = \"accuracy25\"  # IoU > 0.25 -> 1 else 0\n",
        "    ACCURACY_50 = \"accuracy50\"  # IoU > 0.5 -> 1 else 0\n",
        "    ACCURACY_75 = \"accuracy75\"  # IoU > 0.75 -> 1 else 0\n",
        "    ACCURACY_90 = \"accuracy90\"  # IoU > 0.9 -> 1 else 0\n",
        "    IOU = \"avg_iou\"\n",
        "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Reduction(Enum):\n",
        "    MEAN = \"mean\"\n",
        "    SUM = \"sum\"\n",
        "    NONE = \"none\"\n",
        "\n",
        "\n",
        "class MetricsLogger:\n",
        "    def __init__(self, metrics: Dict[str, List[float]] | None = None) -> None:\n",
        "        self.metrics: Dict[str, List[float]] = {}\n",
        "        if metrics is None:\n",
        "            for metric in Metric:\n",
        "                self.metrics[metric.value] = []\n",
        "        else:\n",
        "            self.metrics = metrics\n",
        "\n",
        "    def update_metric(self, metrics: Dict[str, float]) -> None:\n",
        "        for metric, value in metrics.items():\n",
        "            self.metrics[metric].append(value)\n",
        "\n",
        "    def get_metric(\n",
        "        self, metric: Metric, red: Reduction = Reduction.NONE\n",
        "    ) -> float | List[float]:\n",
        "        values: List[float] = self.metrics[metric.value]\n",
        "        match red.name:\n",
        "            case Reduction.MEAN.name:\n",
        "                return sum(values) / len(values)\n",
        "            case Reduction.SUM.name:\n",
        "                return sum(values)\n",
        "            case Reduction.NONE.name:\n",
        "                return values\n",
        "            case _:\n",
        "                raise ValueError(f\"Reduction {red.name} doesn't exists\")\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        res = \"Metrics:\\n\"\n",
        "        for metric, values in self.metrics.items():\n",
        "            res += f\"{metric}: {sum(values) / len(values)}\\n\"\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GKU5cmLWyFS"
      },
      "source": [
        "### VG Model\n",
        "Final model\n",
        "\n",
        "Add image of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "prsp3mkuWyFS"
      },
      "outputs": [],
      "source": [
        "class VGModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cfg: Config,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.cfg: Config = cfg\n",
        "        embed_dim: int = cfg.model.embed_dim\n",
        "        mlp_hidden_dim: int = cfg.model.mlp_hidden_dim\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Load and then build CLIP backbone\n",
        "        self.clip: CLIP = torch.jit.load(\"../RN50.pt\", map_location=\"cpu\").eval()\n",
        "        self.pretrained_model: CLIP = build_model(self.clip.state_dict()).to(\n",
        "            self.device\n",
        "        )\n",
        "        self.pretrained_model.float()\n",
        "        del self.clip\n",
        "\n",
        "        # Freeze all clip parameters except the attention pooling layer positional embeddings\n",
        "        # and the residual connection\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.pretrained_model.visual.attnpool.connect.requires_grad_(True)\n",
        "        self.pretrained_model.visual.attnpool.positional_embedding.requires_grad_(True)\n",
        "\n",
        "        self.fusion_module: FusionModule = FusionModule(\n",
        "            embed_dim, cfg.model.clip_embed_dim, cfg.model.proj_img_size\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.decoder: Decoder = Decoder(\n",
        "            embed_dim,\n",
        "            cfg.model.proj_img_size,\n",
        "            cfg.model.clip_ctx_length,\n",
        "            cfg.model.decoder_heads,\n",
        "            cfg.model.decoder_layers,\n",
        "            cfg.model.decoder_dim_feedforward,\n",
        "        ).to(self.device)\n",
        "\n",
        "        activation: nn.Module = (\n",
        "            nn.Sigmoid() if cfg.model.activation == \"sigmoid\" else nn.Softplus()\n",
        "        )\n",
        "        self.reg_head: MLP = MLP(\n",
        "            input_dim=embed_dim,\n",
        "            output_dim=4,\n",
        "            hidden_dim_1=mlp_hidden_dim,\n",
        "            act_func=activation,\n",
        "        ).to(self.device)\n",
        "\n",
        "    def forward(self, batch: List[BatchSample]) -> Tensor:\n",
        "        # Get text features\n",
        "        text_sequence, global_text_features = self.pretrained_model.encode_text(\n",
        "            torch.stack([sample.caption for sample in batch]).squeeze(1).to(self.device)\n",
        "        )\n",
        "\n",
        "        # Get image features\n",
        "        visual_features = self.pretrained_model.encode_image(\n",
        "            torch.stack([sample.image for sample in batch]).to(self.device)\n",
        "        )\n",
        "\n",
        "        # Fuse features\n",
        "        fused_visual_features: Tensor = self.fusion_module(\n",
        "            visual_features, global_text_features\n",
        "        )\n",
        "\n",
        "        # Transformer decoder\n",
        "        reg_token: Tensor = self.decoder(fused_visual_features, text_sequence)\n",
        "\n",
        "        # Regression head\n",
        "        out: Tensor = self.reg_head(reg_token)\n",
        "        return out\n",
        "\n",
        "# 2-layer MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_dim: int, output_dim: int, hidden_dim_1: int, act_func: nn.Module\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim_1),\n",
        "            nn.BatchNorm1d(hidden_dim_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim_1, output_dim),\n",
        "            act_func,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppCNZ5NbWyFT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okyN6bM0WyFT"
      },
      "source": [
        "The model has been trained for $N$ epochs using the AdamW [Loshchilov, 2019](https://arxiv.org/abs/1711.05101) optmimizer with two different learning rates, one for the backbone non-frozen parameters (i.e. residual connection and positional embeddings in the visual encoder attention pooling) and one for the rest of the model (respectively equal to x, y).\n",
        "\n",
        "The learning rate is then decayed using the [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html) scheduler with a decay rate of 0.1 and a step size of 20 (i.e. the learning rate is divided by 10 every 20 epochs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3by4qEEWyFT"
      },
      "source": [
        "TODO: add graphs from wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2hmDtKIUWyFT"
      },
      "outputs": [],
      "source": [
        "# Training utility functions\n",
        "\n",
        "# Count trainable parameters\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Plot the grad to check for vanishing gradients by adding the function after loss.backward()\n",
        "# Function taken from https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/10\n",
        "def plot_grad_flow(named_parameters: Iterator[Tuple[str, nn.Parameter]]) -> None:\n",
        "    ave_grads = []\n",
        "    max_grads = []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if (p.requires_grad) and (\"bias\" not in n):\n",
        "            if p.grad is None:\n",
        "                raise RuntimeError(f\"Gradient of {n} is None\")\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean().item())\n",
        "            max_grads.append(p.grad.abs().max().item())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=\"k\")\n",
        "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend(\n",
        "        [\n",
        "            Line2D([0], [0], color=\"c\", lw=4),\n",
        "            Line2D([0], [0], color=\"b\", lw=4),\n",
        "            Line2D([0], [0], color=\"k\", lw=4),\n",
        "        ],\n",
        "        [\"max-gradient\", \"mean-gradient\", \"zero-gradient\"],\n",
        "    )\n",
        "\n",
        "# Compute the fraction of samples st IoU > threshold\n",
        "def accuracy(iou: Tensor, threshold: float) -> Tensor:\n",
        "    return torch.tensor(len(iou[iou >= threshold]) / len(iou))\n",
        "\n",
        "# Fix the random seed for reproducibility\n",
        "def init_torch(seed: int = 41) -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4Gh8zuEWyFT"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
        "    model: VGModel,\n",
        "    loss: Loss,\n",
        "    optimizer: optim.Optimizer,\n",
        "    img_size: int,\n",
        "    device: device,\n",
        ") -> Dict[str, float]:\n",
        "    model.train()\n",
        "    loss_list: List[Tensor] = []\n",
        "    iou_list: List[Tensor] = []\n",
        "    acc_25: List[Tensor] = []\n",
        "    acc_50: List[Tensor] = []\n",
        "    acc_75: List[Tensor] = []\n",
        "    acc_90: List[Tensor] = []\n",
        "\n",
        "    for idx, (batch, bbox) in enumerate(tqdm(dataloader, desc=\"Batches\")):\n",
        "        optimizer.zero_grad()\n",
        "        # Move to gpu\n",
        "        for sample in batch:\n",
        "            sample = sample.to(device)\n",
        "        bbox = bbox.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        out: Tensor = model(batch)\n",
        "\n",
        "        # Loss and metrics\n",
        "        out_xyxy = box_convert(out, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        bbox_xyxy = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "        batch_loss: Tensor = loss.compute(out_xyxy, bbox_xyxy)\n",
        "\n",
        "        # Backward pass\n",
        "        batch_loss.backward()\n",
        "        # plot_grad_flow(model.named_parameters())\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Detach the tensors to avoid storing the whole computation graph in the list for each element\n",
        "        out_xyxy_det: Tensor = out_xyxy.detach()\n",
        "        bbox_xyxy_det: Tensor = bbox_xyxy.detach()\n",
        "        batch_iou: Tensor = torch.diagonal(box_iou(out_xyxy_det, bbox_xyxy_det))\n",
        "\n",
        "        # Log metrics\n",
        "        loss_list.append(batch_loss.detach())\n",
        "        iou_list.append(batch_iou.mean())\n",
        "        acc_25.append(accuracy(batch_iou, 0.25))\n",
        "        acc_50.append(accuracy(batch_iou, 0.5))\n",
        "        acc_75.append(accuracy(batch_iou, 0.75))\n",
        "        acc_90.append(accuracy(batch_iou, 0.9))\n",
        "\n",
        "        # Log to console periodically during training to check progress\n",
        "        if (idx * len(batch)) % 4096 == 0:\n",
        "            report: Dict[str, float] = {\n",
        "                \"Train loss\": batch_loss.detach().item(),\n",
        "                \"Train avg iou\": batch_iou.mean().item(),\n",
        "            }\n",
        "            pprint(f\"Batches: {idx}, {report}\")\n",
        "\n",
        "    return {\n",
        "        Metric.LOSS.value: torch.stack(loss_list).mean().item(),\n",
        "        Metric.IOU.value: torch.stack(iou_list).mean().item(),\n",
        "        Metric.ACCURACY_25.value: torch.stack(acc_25).mean().item(),\n",
        "        Metric.ACCURACY_50.value: torch.stack(acc_50).mean().item(),\n",
        "        Metric.ACCURACY_75.value: torch.stack(acc_75).mean().item(),\n",
        "        Metric.ACCURACY_90.value: torch.stack(acc_90).mean().item(),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(\n",
        "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
        "    model: VGModel,\n",
        "    loss: Loss,\n",
        "    img_size: int,\n",
        "    device: torch.device,\n",
        ") -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    loss_list: List[Tensor] = []\n",
        "    iou_list: List[Tensor] = []\n",
        "    acc_25: List[Tensor] = []\n",
        "    acc_50: List[Tensor] = []\n",
        "    acc_75: List[Tensor] = []\n",
        "    acc_90: List[Tensor] = []\n",
        "\n",
        "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
        "        # Move to gpu\n",
        "        for sample in batch:\n",
        "            sample.to(device)\n",
        "        bbox = bbox.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        out: Tensor = model(batch)\n",
        "\n",
        "        # Log metrics\n",
        "        out = box_convert(out, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
        "        bbox = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
        "\n",
        "        batch_loss: Tensor = loss.compute(out, bbox).detach()\n",
        "        batch_iou: Tensor = torch.diagonal(box_iou(out, bbox)).detach()\n",
        "\n",
        "        loss_list.append(batch_loss)\n",
        "        iou_list.append(batch_iou.mean())\n",
        "        acc_25.append(accuracy(batch_iou, 0.25))\n",
        "        acc_50.append(accuracy(batch_iou, 0.5))\n",
        "        acc_75.append(accuracy(batch_iou, 0.75))\n",
        "        acc_90.append(accuracy(batch_iou, 0.9))\n",
        "\n",
        "    return {\n",
        "        Metric.LOSS.value: torch.stack(loss_list).mean().item(),\n",
        "        Metric.IOU.value: torch.stack(iou_list).mean().item(),\n",
        "        Metric.ACCURACY_25.value: torch.stack(acc_25).mean().item(),\n",
        "        Metric.ACCURACY_50.value: torch.stack(acc_50).mean().item(),\n",
        "        Metric.ACCURACY_75.value: torch.stack(acc_75).mean().item(),\n",
        "        Metric.ACCURACY_90.value: torch.stack(acc_90).mean().item(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LgEBKdBWyFT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
        "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
        "    device: torch.device,\n",
        "    cfg: Config,\n",
        ") -> Tuple[MetricsLogger, MetricsLogger]:\n",
        "    train_metrics: MetricsLogger = MetricsLogger()\n",
        "    val_metrics: MetricsLogger = MetricsLogger()\n",
        "\n",
        "    # Loss is the weighted sum of the smooth l1 loss and the GIoU\n",
        "    loss_func = Loss(cfg.train.l1, cfg.train.l2)\n",
        "\n",
        "    model: VGModel = VGModel(cfg).train()\n",
        "\n",
        "    # Separate parameters to train\n",
        "    backbone_params: List[nn.Parameter] = [\n",
        "        p for p in model.pretrained_model.parameters() if p.requires_grad\n",
        "    ]\n",
        "\n",
        "    # All parameters except the backbone parameters to train\n",
        "    non_frozen_params = [\n",
        "        p for p in set(model.parameters()) - set(model.pretrained_model.parameters())\n",
        "    ]\n",
        "\n",
        "    # Optimizer with different learning rates for the backbone and the rest of the model\n",
        "    optimizer = optim.AdamW(\n",
        "        params=[\n",
        "            {\"params\": backbone_params, \"lr\": cfg.train.lr_backbone, \"weight_decay\": 0},\n",
        "            {\"params\": non_frozen_params, \"lr\": cfg.train.lr, \"weight_decay\": 1e-4},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Reduce the learning rate by a factor of 10 every 20 epochs\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=cfg.train.step_size)\n",
        "\n",
        "    print(\"Model parameters: \", count_parameters(model))\n",
        "\n",
        "    if cfg.logging.wandb:\n",
        "        wandb.watch(model, loss_func, log=\"all\", log_freq=100, log_graph=True)\n",
        "\n",
        "    for epoch in tqdm(range(cfg.epochs), desc=\"Epochs\"):\n",
        "        print(\"-------------------- Training --------------------------\")\n",
        "        epoch_train_metrics: Dict[str, float] = train_one_epoch(\n",
        "            dataloader=train_dataloader,\n",
        "            model=model,\n",
        "            loss=loss_func,\n",
        "            optimizer=optimizer,\n",
        "            img_size=cfg.model.img_size,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        train_metrics.update_metric(epoch_train_metrics)\n",
        "        print(\"Training metrics at epoch \", epoch)\n",
        "        print(epoch_train_metrics)\n",
        "\n",
        "        # Evaluate on validation set for hyperparameter tuning and logging\n",
        "        print(\"-------------------- Validation ------------------------\")\n",
        "        epoch_val_metrics: Dict[str, float] = validate(\n",
        "            dataloader=val_dataloader,\n",
        "            model=model,\n",
        "            loss=loss_func,\n",
        "            img_size=cfg.model.img_size,\n",
        "            device=device,\n",
        "        )\n",
        "        val_metrics.update_metric(epoch_val_metrics)\n",
        "        print(\"Validation metrics at epoch \", epoch)\n",
        "        print(epoch_val_metrics)\n",
        "\n",
        "        # Log metric to wandb for hyperparameter tuning\n",
        "        if cfg.train.sweep:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"validation_accuracy\": epoch_val_metrics[Metric.IOU.value],\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Log metrics to wandb putting train and val metrics together\n",
        "        if cfg.logging.wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"Loss\": {\n",
        "                        \"train\": epoch_train_metrics[Metric.LOSS.value],\n",
        "                        \"val\": epoch_val_metrics[Metric.LOSS.value],\n",
        "                    },\n",
        "                    \"Average IOU\": {\n",
        "                        \"train\": epoch_train_metrics[Metric.IOU.value],\n",
        "                        \"val\": epoch_val_metrics[Metric.IOU.value],\n",
        "                    },\n",
        "                    \"Accuracy@25\": {\n",
        "                        \"train\": epoch_train_metrics[Metric.ACCURACY_25.value],\n",
        "                        \"val\": epoch_val_metrics[Metric.ACCURACY_25.value],\n",
        "                    },\n",
        "                    \"Accuracy@50\": {\n",
        "                        \"train\": epoch_train_metrics[Metric.ACCURACY_50.value],\n",
        "                        \"val\": epoch_val_metrics[Metric.ACCURACY_50.value],\n",
        "                    },\n",
        "                    \"Accuracy@75\": {\n",
        "                        \"train\": epoch_train_metrics[Metric.ACCURACY_75.value],\n",
        "                        \"val\": epoch_val_metrics[Metric.ACCURACY_75.value],\n",
        "                    },\n",
        "                    \"Accuracy@90\": {\n",
        "                        \"train\": epoch_train_metrics[Metric.ACCURACY_90.value],\n",
        "                        \"val\": epoch_val_metrics[Metric.ACCURACY_90.value],\n",
        "                    },\n",
        "                },\n",
        "                commit=True,\n",
        "            )\n",
        "\n",
        "        # Save model after each epoch so that we can resume training or take the best model\n",
        "        if cfg.logging.save:\n",
        "            dir: str = cfg.logging.path\n",
        "            if not os.path.exists(dir):\n",
        "                os.makedirs(dir)\n",
        "            torch.save(\n",
        "                obj={\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
        "                    \"loss\": epoch_train_metrics[Metric.LOSS.value],\n",
        "                },\n",
        "                f=f\"{dir}model{epoch}.pth\",\n",
        "            )\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return train_metrics, val_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68N4AlNCWyFU"
      },
      "outputs": [],
      "source": [
        "# Load configuration, create datasets, dataloaders and train the model\n",
        "def initialize_run(sweep: bool = True) -> None:\n",
        "    config = Config()\n",
        "    if sweep:\n",
        "        wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
        "        wandb.init(project=\"vgproject\")\n",
        "\n",
        "        # Set the training configurations we are tuning equal to the sweep run configurations\n",
        "        wandb_cfg = wandb.config\n",
        "        config.update(wandb_cfg)\n",
        "    else:\n",
        "        if config.logging.wandb:\n",
        "            wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
        "            wandb.init(project=\"vgproject\", config=config.as_dict())\n",
        "\n",
        "    train_dataset: VGDataset = VGDataset(\n",
        "        dir_path=config.dataset_path,\n",
        "        split=Split.TRAIN,\n",
        "        output_bbox_type=BboxType.XYWH,\n",
        "        transform=True,\n",
        "        augment=True,\n",
        "        preprocessed=True,\n",
        "    )\n",
        "    print(\"Train dataset created. Dataset length \", len(train_dataset))\n",
        "\n",
        "    # Validation dataset is not augmented to be more similar to the test set\n",
        "    val_dataset: VGDataset = VGDataset(\n",
        "        dir_path=config.dataset_path,\n",
        "        split=Split.VAL,\n",
        "        output_bbox_type=BboxType.XYWH,\n",
        "        transform=True,\n",
        "        augment=False,\n",
        "        preprocessed=True,\n",
        "    )\n",
        "    print(\"Validation dataset created. Dataset length: \", len(val_dataset))\n",
        "\n",
        "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=config.train.batch_size,\n",
        "        collate_fn=custom_collate,\n",
        "        num_workers=2,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=config.train.batch_size,\n",
        "        collate_fn=custom_collate,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_metrics, val_metrics = train(train_dataloader, val_dataloader, device, config)\n",
        "\n",
        "    json.dump(train_metrics.metrics, open(\"../train_metrics.json\", \"w\"))\n",
        "    json.dump(val_metrics.metrics, open(\"../val_metrics.json\", \"w\"))\n",
        "\n",
        "    if config.logging.wandb:\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBtnV-JGWyFU"
      },
      "outputs": [],
      "source": [
        "def main() -> None:\n",
        "    init_torch()\n",
        "    cfg = Config()\n",
        "    if cfg.train.sweep:\n",
        "        sweep_configuration: Dict[str, Any] = json.load(\n",
        "            open(\"../sweep_config.json\", \"r\")\n",
        "        )\n",
        "        sweep: str = wandb.sweep(sweep_configuration, project=\"vgproject\")\n",
        "        wandb.agent(sweep, function=initialize_run, count=10)\n",
        "    else:\n",
        "        initialize_run(cfg.train.sweep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r1CbEe-WyFU"
      },
      "outputs": [],
      "source": [
        "#Start training\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QelPpjQAWyFU"
      },
      "outputs": [],
      "source": [
        "# Used for cleaning up the environment\n",
        "wandb.finish()\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgr6zPCdWyFU"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYdrxuqtWyFV"
      },
      "source": [
        "### Baseline Evaluation\n",
        "Compared to the baseline described in the assignment, the bounding boxes predicted by YOLO are used both for cropping the image and then feed it to CLIP and for blurring the entire image outside of the prdicted bounding box. This way even if the information outside of the box is less important because it's blurred there are still information about the position of the object in the image, similarly to what was done in [Subramanian, 2022](http://arxiv.org/abs/2204.05991)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ul-GB52WyFV"
      },
      "outputs": [],
      "source": [
        "class Baseline:\n",
        "    def __init__(self) -> None:\n",
        "        self.device = torch.device(\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        self.clip_model, self.clip_preprocessor = clip.load(\n",
        "            name=\"RN50\", device=self.device\n",
        "        )\n",
        "        self.yolo: YOLO = YOLO()\n",
        "\n",
        "    def predict(self, batch: List[BatchSample]) -> List[Result]:\n",
        "        images: List[Image.Image] = [T.ToPILImage()(sample.image) for sample in batch]\n",
        "        batch_bbox_predictions: List[Results] = self.yolo(\n",
        "            images, max_det=50, verbose=False, device=self.device\n",
        "        )  # type: ignore\n",
        "        results: List[Result] = []\n",
        "\n",
        "        for sample, image_bboxes in tqdm(\n",
        "            zip(batch, batch_bbox_predictions, strict=True)\n",
        "        ):\n",
        "            image: Tensor = sample.image\n",
        "            crops: List[Tensor] = []\n",
        "            blurs: List[Tensor] = []\n",
        "            boxes: List[Tensor] = []\n",
        "\n",
        "            # For each object proposal, separetely crop and blur the image\n",
        "            for bbox in image_bboxes.boxes:\n",
        "                bbox = bbox.to(self.device)\n",
        "                xmin, ymin, xmax, ymax = bbox.xyxy.int()[0]\n",
        "                boxes.append(torch.tensor([xmin, ymin, xmax, ymax], device=self.device))\n",
        "\n",
        "                blurred: Tensor = T.GaussianBlur(25, 50)(sample.image).to(self.device)\n",
        "                blurred[:, ymin:ymax, xmin:xmax] = image[:, ymin:ymax, xmin:xmax]\n",
        "                blurs.append(blurred)\n",
        "                crops.append(image[:, ymin:ymax, xmin:xmax])\n",
        "\n",
        "            # If no object proposals are found, return a bounding box with all zeros\n",
        "            if len(crops) == 0:\n",
        "                results.append(\n",
        "                    Result(\n",
        "                        torch.tensor([0, 0, 0, 0], device=self.device),\n",
        "                        torch.tensor([0], device=self.device),\n",
        "                    )\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            scores: Tensor = self.compute_clip_similarity(\n",
        "                crops, blurs, sample.caption.to(self.device)\n",
        "            )\n",
        "\n",
        "            # Get the object proposal with the highest score\n",
        "            max_score: Tensor = torch.argmax(scores)\n",
        "            results.append(Result(boxes[max_score], scores[max_score]))\n",
        "        return results\n",
        "\n",
        "    # Combination of cropping and blurring for the object proposals\n",
        "    @torch.no_grad()\n",
        "    def compute_clip_similarity(\n",
        "        self,\n",
        "        crops: List[Tensor],\n",
        "        blurs: List[Tensor],\n",
        "        caption: Tensor,\n",
        "    ) -> Tensor:\n",
        "        crops = [self.clip_preprocessor(T.ToPILImage()(crop)) for crop in crops]\n",
        "        images_crops: Tensor = torch.stack(tensors=crops).to(device=self.device)\n",
        "        logits_per_image_crops, _ = self.clip_model(images_crops, caption)\n",
        "\n",
        "        blurs = [self.clip_preprocessor(T.ToPILImage()(blur)) for blur in blurs]\n",
        "        images_blurs: Tensor = torch.stack(\n",
        "            tensors=blurs,\n",
        "        ).to(device=self.device)\n",
        "\n",
        "        logits_per_image_blurs, _ = self.clip_model(images_blurs, caption)\n",
        "\n",
        "        return logits_per_image_crops + logits_per_image_blurs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQgyM-h_WyFV"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval_baseline() -> None:\n",
        "    cfg = Config()\n",
        "    test_data = VGDataset(\n",
        "        dir_path=cfg.dataset_path,\n",
        "        split=Split.TEST,\n",
        "        output_bbox_type=BboxType.XYXY,\n",
        "        transform=False,\n",
        "        augment=False,\n",
        "        preprocessed=True,\n",
        "    )\n",
        "\n",
        "    dataloader: DataLoader[Any] = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=custom_collate,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    baseline = Baseline()\n",
        "\n",
        "    metrics: MetricsLogger = MetricsLogger()\n",
        "    iou_list: List[Tensor] = []\n",
        "    acc_25: List[Tensor] = []\n",
        "    acc_50: List[Tensor] = []\n",
        "    acc_75: List[Tensor] = []\n",
        "    acc_90: List[Tensor] = []\n",
        "    cos_sim_pred: List[Tensor] = []\n",
        "    cos_sim_gt: List[Tensor] = []\n",
        "\n",
        "    transformation = T.Compose(\n",
        "        [\n",
        "            T.Resize(cfg.model.img_size),\n",
        "            T.CenterCrop(cfg.model.img_size),\n",
        "            lambda x: x / 255.0,\n",
        "            T.Normalize(\n",
        "                (0.48145466, 0.4578275, 0.40821073),\n",
        "                (0.26862954, 0.26130258, 0.27577711),\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    for batch, bboxes in tqdm(dataloader):\n",
        "        for sample in batch:\n",
        "            sample = sample.to(baseline.device)\n",
        "        bboxes = bboxes.to(baseline.device)\n",
        "\n",
        "        prediction: List[Result] = baseline.predict(batch)\n",
        "        bbox_pred: Tensor = torch.stack([p.bounding_box for p in prediction]).to(\n",
        "            baseline.device\n",
        "        )\n",
        "        bbox_gt: Tensor = bboxes.clone().detach().squeeze(1).to(baseline.device)\n",
        "\n",
        "        batch_iou: Tensor = torch.diagonal(box_iou(bbox_gt, bbox_pred))\n",
        "\n",
        "        iou_list.append(batch_iou.mean())\n",
        "        acc_25.append(accuracy(batch_iou, 0.25))\n",
        "        acc_50.append(accuracy(batch_iou, 0.5))\n",
        "        acc_75.append(accuracy(batch_iou, 0.75))\n",
        "        acc_90.append(accuracy(batch_iou, 0.9))\n",
        "\n",
        "        box_gt_xywh = box_convert(bbox_gt, in_fmt=\"xyxy\", out_fmt=\"xywh\")\n",
        "        image_features_gt = torch.stack(\n",
        "            [\n",
        "                FT.crop(\n",
        "                    sample.image,\n",
        "                    int(bbox[1].item()),\n",
        "                    int(bbox[0].item()),\n",
        "                    int(bbox[3].item()),\n",
        "                    int(bbox[2].item()),\n",
        "                )\n",
        "                for sample, bbox in zip(batch, box_gt_xywh)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        box_pred_xywh = box_convert(bbox_pred, in_fmt=\"xyxy\", out_fmt=\"xywh\")\n",
        "        image_features_pred = torch.stack(\n",
        "            [\n",
        "                FT.crop(\n",
        "                    sample.image,\n",
        "                    int(bbox[1].item()),\n",
        "                    int(bbox[0].item()),\n",
        "                    int(bbox[3].item()),\n",
        "                    int(bbox[2].item()),\n",
        "                )\n",
        "                for sample, bbox in zip(batch, box_pred_xywh)\n",
        "            ]\n",
        "        )\n",
        "        text_features = torch.stack([sample.caption for sample in batch])\n",
        "        cos_sim_pred.append(\n",
        "            compute_clip_cosine_similarity(\n",
        "                image_features_pred, text_features, transformation\n",
        "            )\n",
        "        )\n",
        "        cos_sim_gt.append(\n",
        "            compute_clip_cosine_similarity(image_features_gt, text_features, transformation)\n",
        "        )\n",
        "\n",
        "    json.dump(\n",
        "        {\n",
        "            Metric.IOU.value: iou_list,\n",
        "            Metric.ACCURACY_25.value: acc_25,\n",
        "            Metric.ACCURACY_50.value: acc_50,\n",
        "            Metric.ACCURACY_75.value: acc_75,\n",
        "            Metric.ACCURACY_90.value: acc_90,\n",
        "            Metric.COSINE_SIMILARITY.value + \" prediction\": cos_sim_pred,\n",
        "            Metric.COSINE_SIMILARITY.value + \" ground truth\": cos_sim_gt,\n",
        "        },\n",
        "        open(\"../test_metrics_baseline.json\", \"w\"),\n",
        "    )\n",
        "\n",
        "    pprint(\n",
        "        object={\n",
        "            Metric.IOU.value: torch.stack(iou_list).mean().item(),\n",
        "            Metric.ACCURACY_25.value: torch.stack(acc_25).mean().item(),\n",
        "            Metric.ACCURACY_50.value: torch.stack(acc_50).mean().item(),\n",
        "            Metric.ACCURACY_75.value: torch.stack(acc_75).mean().item(),\n",
        "            Metric.ACCURACY_90.value: torch.stack(acc_90).mean().item(),\n",
        "            Metric.COSINE_SIMILARITY.value\n",
        "            + \" prediction\": torch.stack(cos_sim_pred).mean().item(),\n",
        "            Metric.COSINE_SIMILARITY.value\n",
        "            + \" ground truth\": torch.stack(cos_sim_gt).mean().item(),\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RTPI8hPWyFV"
      },
      "outputs": [],
      "source": [
        "eval_baseline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lizePj56WyFV"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "A-moysKOWyFW"
      },
      "outputs": [],
      "source": [
        "def compute_clip_cosine_similarity(\n",
        "    image_features: Tensor, text_features: Tensor, transform_func: T.Compose\n",
        ") -> Tensor:\n",
        "    clip_model, _ = clip.load(\"RN50\")\n",
        "    image_features = clip_model.encode_image(transform_func(image_features))\n",
        "    text_features = clip_model.encode_text(text_features.squeeze_(1))\n",
        "\n",
        "    image_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    text_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "    return torch.nn.functional.cosine_similarity(image_norm, text_norm, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3MztMN4OWyFW"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval(model_file: str) -> None:\n",
        "    cfg = Config()\n",
        "    dataset = VGDataset(\n",
        "        dir_path=cfg.dataset_path,\n",
        "        split=Split.TEST,\n",
        "        output_bbox_type=BboxType.XYXY,\n",
        "        transform=True,\n",
        "        augment=False,\n",
        "        preprocessed=True,\n",
        "    )\n",
        "\n",
        "    dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=1,\n",
        "        collate_fn=custom_collate,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    checkpoint: Dict[str, Any] = torch.load(\n",
        "        f\"../runs/{model_file}\", map_location=device\n",
        "    )\n",
        "    model: VGModel = VGModel(cfg)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.eval()\n",
        "    loss = Loss(cfg.train.l1, cfg.train.l2)\n",
        "\n",
        "    loss_list: List[Tensor] = []\n",
        "    iou_list: List[Tensor] = []\n",
        "    acc_25: List[Tensor] = []\n",
        "    acc_50: List[Tensor] = []\n",
        "    acc_75: List[Tensor] = []\n",
        "    acc_90: List[Tensor] = []\n",
        "    cos_sim_pred: List[Tensor] = []\n",
        "    cos_sim_gt: List[Tensor] = []\n",
        "\n",
        "    transformation = T.Compose(\n",
        "        [\n",
        "            T.Resize(cfg.model.img_size),\n",
        "            T.CenterCrop(cfg.model.img_size),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    for idx, (batch, bbox) in enumerate(tqdm(dataloader, desc=\"Batches\")):\n",
        "        # Move to gpu\n",
        "        for sample in batch:\n",
        "            sample = sample.to(device)\n",
        "        bboxes: Tensor = bbox.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        out: Tensor = model(batch)\n",
        "\n",
        "        # Loss and metrics\n",
        "        out_xyxy: Tensor = box_convert(\n",
        "            out * cfg.model.img_size, in_fmt=\"xywh\", out_fmt=\"xyxy\"\n",
        "        )\n",
        "        bbox_xyxy: Tensor = box_convert(\n",
        "            bboxes * cfg.model.img_size, in_fmt=\"xywh\", out_fmt=\"xyxy\"\n",
        "        )\n",
        "        batch_loss: Tensor = loss.compute(out_xyxy, bbox_xyxy)\n",
        "\n",
        "        out_xyxy_det: Tensor = out_xyxy.detach()\n",
        "        bbox_xyxy_det: Tensor = bbox_xyxy.detach()\n",
        "        batch_iou: Tensor = torch.diagonal(box_iou(out_xyxy_det, bbox_xyxy_det))\n",
        "\n",
        "        loss_list.append(batch_loss.detach())\n",
        "        iou_list.append(batch_iou.mean())\n",
        "\n",
        "        acc_25.append(accuracy(batch_iou, 0.25))\n",
        "        acc_50.append(accuracy(batch_iou, 0.5))\n",
        "        acc_75.append(accuracy(batch_iou, 0.75))\n",
        "        acc_90.append(accuracy(batch_iou, 0.9))\n",
        "\n",
        "        image_features_gt = torch.stack(\n",
        "            [\n",
        "                FT.crop(\n",
        "                    sample.image,\n",
        "                    int(bbox[1].item()),\n",
        "                    int(bbox[0].item()),\n",
        "                    int(bbox[3].item()),\n",
        "                    int(bbox[2].item()),\n",
        "                )\n",
        "                for sample, bbox in zip(batch, bboxes * cfg.model.img_size)\n",
        "            ]\n",
        "        )\n",
        "        image_features_pred = torch.stack(\n",
        "            [\n",
        "                FT.crop(\n",
        "                    sample.image,\n",
        "                    int(bbox[1].item()),\n",
        "                    int(bbox[0].item()),\n",
        "                    int(bbox[3].item()),\n",
        "                    int(bbox[2].item()),\n",
        "                )\n",
        "                for sample, bbox in zip(batch, out * cfg.model.img_size)\n",
        "            ]\n",
        "        )\n",
        "        text_features = torch.stack([sample.caption for sample in batch])\n",
        "        cos_sim_pred.append(\n",
        "            compute_clip_cosine_similarity(\n",
        "                image_features_pred, text_features, transformation\n",
        "            )\n",
        "        )\n",
        "        cos_sim_gt.append(\n",
        "            compute_clip_cosine_similarity(image_features_gt, text_features, transformation)\n",
        "        )\n",
        "\n",
        "    json.dump(\n",
        "        {\n",
        "            Metric.LOSS.value: loss_list,\n",
        "            Metric.IOU.value: iou_list,\n",
        "            Metric.ACCURACY_25.value: acc_25,\n",
        "            Metric.ACCURACY_50.value: acc_50,\n",
        "            Metric.ACCURACY_75.value: acc_75,\n",
        "            Metric.ACCURACY_90.value: acc_90,\n",
        "            Metric.COSINE_SIMILARITY.value + \" prediction\": cos_sim_pred,\n",
        "            Metric.COSINE_SIMILARITY.value + \" ground truth\": cos_sim_gt,\n",
        "        },\n",
        "        open(\"../test_metrics.json\", \"w\"),\n",
        "    )\n",
        "\n",
        "    pprint(\n",
        "        object={\n",
        "            Metric.LOSS.value: torch.stack(loss_list).mean().item(),\n",
        "            Metric.IOU.value: torch.stack(iou_list).mean().item(),\n",
        "            Metric.ACCURACY_25.value: torch.stack(acc_25).mean().item(),\n",
        "            Metric.ACCURACY_50.value: torch.stack(acc_50).mean().item(),\n",
        "            Metric.ACCURACY_75.value: torch.stack(acc_75).mean().item(),\n",
        "            Metric.ACCURACY_90.value: torch.stack(acc_90).mean().item(),\n",
        "            Metric.COSINE_SIMILARITY.value\n",
        "            + \" prediction\": torch.stack(cos_sim_pred).mean().item(),\n",
        "            Metric.COSINE_SIMILARITY.value\n",
        "            + \" ground truth\": torch.stack(cos_sim_gt).mean().item(),\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h88onZ8lWyFW",
        "outputId": "2e47227f-c7fd-4366-ead0-d386b071d550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509,
          "referenced_widgets": [
            "9752edd35b9a40b48abe9caa13d49453",
            "db0a5eba55c84ffc816c84e3f337e1df",
            "0722e963395646c8a7c303b09d4161c2",
            "292b070a6f7c4fa9b50241be6c837e94",
            "42fb348182514797a1055187f3c8560b",
            "87ce04b1519a47e3a0d9dd8ef66e6d55",
            "70cec5b6097b492b886ed22d7b89cbbc",
            "aa62e436dbaf4545ba3dd702647205d3",
            "0e92c75802774f7a85c55e10dda5d1cf",
            "46c815d0f40844beb23c2e66fa79d32a",
            "c958afa3fca5407fb3830f1a4f61d54c"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "Resume: ../runs/model53.pth\n",
            "From: https://drive.google.com/uc?export=download&id=1v5zU-OfZeCi_R9eHhm0LBOVGRSQyn3T9\n",
            "To: /content/runs/model53.pth\n",
            "0.00B [00:00, ?B/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/5023 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9752edd35b9a40b48abe9caa13d49453"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-41f17a03e6df>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../runs/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model53.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-d3f0615c5033>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         cos_sim_pred.append(\n\u001b[0;32m--> 102\u001b[0;31m             compute_clip_cosine_similarity(\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mimage_features_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-22-1a47267585d0>\u001b[0m in \u001b[0;36mcompute_clip_cosine_similarity\u001b[0;34m(image_features, text_features, transform_func)\u001b[0m\n\u001b[1;32m      3\u001b[0m ) -> Tensor:\n\u001b[1;32m      4\u001b[0m     \u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RN50\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mencode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)"
          ]
        }
      ],
      "source": [
        "# Download the model best checkpoint from the drive\n",
        "if not os.path.exists(\"runs/model53.pth\"):\n",
        "  print(\"Downloading model...\")\n",
        "  url = \"https://drive.google.com/uc?export=download&id=1v5zU-OfZeCi_R9eHhm0LBOVGRSQyn3T9\"\n",
        "  gdown.download(url=url, output=\"../runs/\", quiet=False, resume=True)\n",
        "\n",
        "eval(\"model53.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_0UPx2ANcNYs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9752edd35b9a40b48abe9caa13d49453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db0a5eba55c84ffc816c84e3f337e1df",
              "IPY_MODEL_0722e963395646c8a7c303b09d4161c2",
              "IPY_MODEL_292b070a6f7c4fa9b50241be6c837e94"
            ],
            "layout": "IPY_MODEL_42fb348182514797a1055187f3c8560b"
          }
        },
        "db0a5eba55c84ffc816c84e3f337e1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87ce04b1519a47e3a0d9dd8ef66e6d55",
            "placeholder": "​",
            "style": "IPY_MODEL_70cec5b6097b492b886ed22d7b89cbbc",
            "value": "Batches:   0%"
          }
        },
        "0722e963395646c8a7c303b09d4161c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa62e436dbaf4545ba3dd702647205d3",
            "max": 5023,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e92c75802774f7a85c55e10dda5d1cf",
            "value": 0
          }
        },
        "292b070a6f7c4fa9b50241be6c837e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46c815d0f40844beb23c2e66fa79d32a",
            "placeholder": "​",
            "style": "IPY_MODEL_c958afa3fca5407fb3830f1a4f61d54c",
            "value": " 0/5023 [00:04&lt;?, ?it/s]"
          }
        },
        "42fb348182514797a1055187f3c8560b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ce04b1519a47e3a0d9dd8ef66e6d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70cec5b6097b492b886ed22d7b89cbbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa62e436dbaf4545ba3dd702647205d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e92c75802774f7a85c55e10dda5d1cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46c815d0f40844beb23c2e66fa79d32a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c958afa3fca5407fb3830f1a4f61d54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}