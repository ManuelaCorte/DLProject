{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ftfy regex tqdm ultralytics wandb albumentations\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import gzip\n",
    "import html\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "from typing import (Any, Callable, Dict, List, Optional, OrderedDict, Tuple,\n",
    "                    Iterable, Iterable, Union)\n",
    "\n",
    "import albumentations as A\n",
    "import clip\n",
    "import cv2\n",
    "import ftfy\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pkg_resources as p\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from clip import tokenize\n",
    "from clip.model import CLIP, ModifiedResNet\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from PIL import Image\n",
    "from torch import Tensor, device, tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops import box_convert, box_iou, generalized_box_iou_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "# Download dataset and save under data/raw/ only if not already downloaded\n",
    "url = \"https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\"\n",
    "if not os.path.exists(\"data/raw/refcocog.tar.gz\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(url=url, output=\"data/raw/\", quiet=False, resume=True)\n",
    "if not os.path.exists(\"data/raw/refcocog/\"):\n",
    "    print(\"Extracting dataset...\")\n",
    "    !tar -xf data/raw/refcocog.tar.gz -C data/raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed samples can be downloaded from Google Drive by executing the following cell. Otherwise, the preprocessing wil be done saving the file only temporarly in the Colab environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download preprocessed dataset\n",
    "url = \"https://drive.google.com/drive/folders/1jaJV40dneOckZn7WHMQyd2jBh7A8534N\"\n",
    "gdown.download_folder(url=url, output=\"data/\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ManuelaCorte/DLProject/develop/config.json\n",
    "!wget https://raw.githubusercontent.com/ManuelaCorte/DLProject/develop/sweep_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir src\n",
    "%cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, image_path: str, caption: str, bounding_box: Tensor) -> None:\n",
    "        self.image_path = image_path\n",
    "        self.caption = caption\n",
    "        self.bounding_box = bounding_box\n",
    "\n",
    "    def as_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"image_path\": self.image_path,\n",
    "            \"caption\": self.caption,\n",
    "            \"bounding_box\": self.bounding_box.tolist(),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def fromJSON(json: dict[str, Any]) -> Any:\n",
    "        return Sample(json[\"image_path\"], json[\"caption\"], Tensor(json[\"bounding_box\"]))\n",
    "\n",
    "\n",
    "class BatchSample:\n",
    "    def __init__(self, image: Tensor, caption: Tensor) -> None:\n",
    "        self.image: Tensor = image\n",
    "        self.caption: Tensor = caption\n",
    "\n",
    "    def to(self, device: device | str) -> Any:\n",
    "        return self.__class__(self.image.to(device), self.caption.to(device))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BatchSample(image={self.image.shape}, caption={self.caption.shape})\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Result:\n",
    "    bounding_box: Tensor\n",
    "    score: Tensor\n",
    "\n",
    "\n",
    "# XYXY: top left and bottom right corners\n",
    "# XYWH: top left corner, width and height\n",
    "# CXCWH: center coordinates, width and height\n",
    "@dataclass(frozen=True)\n",
    "class BboxType(Enum):\n",
    "    XYXY = \"xyxy\"\n",
    "    XYWH = \"xywh\"\n",
    "    CXCWH = \"cxcwh\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return super().__str__()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Model:\n",
    "    clip_embed_dim: int\n",
    "    clip_ctx_length: int\n",
    "    embed_dim: int\n",
    "    mlp_hidden_dim: int\n",
    "    img_size: int\n",
    "    proj_img_size: int\n",
    "    decoder_layers: int\n",
    "    decoder_heads: int\n",
    "    decoder_dim_feedforward: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Train:\n",
    "    batch_size: int\n",
    "    lr: float\n",
    "    l1: float\n",
    "    l2: float\n",
    "    sweep: bool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Logging:\n",
    "    path: str\n",
    "    save: bool\n",
    "    resume: bool\n",
    "    wandb: bool\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        cfg: Dict[str, Any] = json.load(open(\"../config.json\", \"r\"))\n",
    "        self.dataset_path: str = cfg[\"dataset_path\"]\n",
    "        self.epochs: int = cfg[\"epochs\"]\n",
    "        self.model = Model(**cfg[\"model\"])\n",
    "        self.train = Train(**cfg[\"train\"])\n",
    "        self.logging = Logging(**cfg[\"logging\"])\n",
    "\n",
    "    def as_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dataset_path\": self.dataset_path,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"model\": self.model.__dict__,\n",
    "            \"train\": self.train.__dict__,\n",
    "        }\n",
    "\n",
    "    # if in other dict there are keys equal to the keys in self, update them\n",
    "    def update(self, other: Dict[str, Any]):\n",
    "        for k, v in other.items():\n",
    "            if k in self.__dict__:\n",
    "                self.__dict__[k] = v\n",
    "            if k in self.model.__dict__:\n",
    "                self.model.__dict__[k] = v\n",
    "            if k in self.train.__dict__:\n",
    "                self.train.__dict__[k] = v\n",
    "            if k in self.logging.__dict__:\n",
    "                self.logging.__dict__[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Metric(Enum):\n",
    "    LOSS = \"loss\"\n",
    "    ACCURACY_50 = \"accuracy\"  # IoU > 0.5 -> 1 else 0\n",
    "    ACCURACY_75 = \"accuracy75\"  # IoU > 0.75 -> 1 else 0\n",
    "    ACCURACY_90 = \"accuracy90\"  # IoU > 0.9 -> 1 else 0\n",
    "    IOU = \"iou\"\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Reduction(Enum):\n",
    "    MEAN = \"mean\"\n",
    "    SUM = \"sum\"\n",
    "    NONE = \"none\"\n",
    "\n",
    "\n",
    "class MetricsLogger:\n",
    "    def __init__(self, metrics: Dict[str, List[float]] | None = None) -> None:\n",
    "        self.metrics: Dict[str, List[float]] = {}\n",
    "        if metrics is None:\n",
    "            for metric in Metric:\n",
    "                self.metrics[metric.value] = []\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "\n",
    "    def update_metric(self, metrics: Dict[str, float]) -> None:\n",
    "        for metric, value in metrics.items():\n",
    "            self.metrics[metric].append(value)\n",
    "\n",
    "    def get_metric(\n",
    "        self, metric: Metric, red: Reduction = Reduction.NONE\n",
    "    ) -> float | List[float]:\n",
    "        values: List[float] = self.metrics[metric.value]\n",
    "        match red.name:\n",
    "            case Reduction.MEAN.name:\n",
    "                return sum(values) / len(values)\n",
    "            case Reduction.SUM.name:\n",
    "                return sum(values)\n",
    "            case Reduction.NONE.name:\n",
    "                return values\n",
    "            case _:\n",
    "                raise ValueError(f\"Reduction {red.name} doesn't exists\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        res = \"Metrics:\\n\"\n",
    "        for metric, values in self.metrics.items():\n",
    "            res += f\"{metric}: {sum(values) / len(values)}\\n\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset contains samples with an image with a bounding box and a caption associated with the bounding box.\n",
    "\n",
    "\n",
    "class VGDataset(Dataset[Tuple[BatchSample, Tensor]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        split: Split,\n",
    "        output_bbox_type: BboxType,\n",
    "        augment: bool,\n",
    "        transform: bool = True,\n",
    "        preprocessed: bool = False,\n",
    "        preprocessed_path: str = \"../data/processed/\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dir_path: str = dir_path\n",
    "        self.split: Split = split\n",
    "        self.output_bbox_type: BboxType = output_bbox_type\n",
    "        self.augment: bool = augment\n",
    "        self.transform: bool = transform\n",
    "        self.device: device = torch.device(\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        if preprocessed:\n",
    "            preprocess(dir_path, preprocessed_path, output_bbox_type)\n",
    "            with open(\n",
    "                preprocessed_path + f\"{self.split.value}_samples.json\", \"rb\"\n",
    "            ) as samples:\n",
    "                self.samples: List[Sample] = json.load(\n",
    "                    samples, object_hook=Sample.fromJSON\n",
    "                )\n",
    "        else:\n",
    "            self.samples: List[Sample] = self.get_samples()  # type: ignore\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, ref_id: int) -> Tuple[BatchSample, Tensor]:\n",
    "        # extended_caption: str = f\"find the region that corresponds to the description {self.samples[ref_id].caption}\"\n",
    "        caption: Tensor = tokenize(self.samples[ref_id].caption, truncate=True)  # type: ignore\n",
    "        if self.transform:\n",
    "            image, bbox = transform_sample(\n",
    "                Image.open(self.samples[ref_id].image_path),\n",
    "                self.samples[ref_id].bounding_box,\n",
    "                self.augment,\n",
    "            )\n",
    "        else:\n",
    "            image = read_image(self.samples[ref_id].image_path)\n",
    "            bbox = torch.tensor([self.samples[ref_id].bounding_box])\n",
    "        return BatchSample(image, caption), bbox\n",
    "\n",
    "    def get_samples(self) -> List[Sample]:\n",
    "        with open(self.dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "            self.dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "        ) as refs:\n",
    "            instances = json.load(inst)\n",
    "            references = pickle.load(refs)\n",
    "        samples: List[Sample] = []\n",
    "        for ref in references:\n",
    "            if self.split.value == ref[\"split\"]:\n",
    "                image_path = self.get_image_path(ref[\"image_id\"], instances)\n",
    "                caption = self.get_caption(ref[\"sentences\"])\n",
    "                bbox = self.get_bounding_box(ref[\"ann_id\"], instances)\n",
    "                samples.append(Sample(image_path, caption, bbox))\n",
    "        return samples\n",
    "\n",
    "    def get_image_path(self, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "        image_name = next(\n",
    "            image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "        )\n",
    "        path = self.dir_path + \"images/\" + image_name\n",
    "        return path\n",
    "\n",
    "    def get_caption(self, captions: List[Dict[str, Any]]) -> str:\n",
    "        longest_caption = captions[0]\n",
    "        for caption in captions:\n",
    "            if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "                longest_caption = caption\n",
    "        return f\"find the region that corresponds to the description {longest_caption['sent']}\"\n",
    "\n",
    "    # Bounding boxed converted to format compatible with yolo or torchvision\n",
    "    def get_bounding_box(self, ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
    "        bbox = next(\n",
    "            ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id\n",
    "        )\n",
    "        bounding_box: Tensor = tensor([])\n",
    "        match self.output_bbox_type.name:\n",
    "            case BboxType.XYXY.name:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "                )\n",
    "            case BboxType.XYWH.name:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
    "                )\n",
    "            case BboxType.CXCWH.name:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
    "                )\n",
    "\n",
    "        return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(\n",
    "    dir_path: str, bbox_type: BboxType\n",
    ") -> Tuple[List[Sample], List[Sample], List[Sample]]:\n",
    "    with open(dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "        dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "    ) as refs:\n",
    "        instances = json.load(inst)\n",
    "        references = pickle.load(refs)\n",
    "    train_samples: List[Sample] = []\n",
    "    val_samples: List[Sample] = []\n",
    "    test_samples: List[Sample] = []\n",
    "    for ref in tqdm(references, desc=f\"Processing dataset\"):\n",
    "        image_path = get_image_path(dir_path, ref[\"image_id\"], instances)\n",
    "        caption = get_caption(ref[\"sentences\"])\n",
    "        bbox = get_bounding_box(ref[\"ann_id\"], instances, bbox_type)\n",
    "        split = ref[\"split\"]\n",
    "        # print(split)\n",
    "        match split:\n",
    "            case Split.TRAIN.value:\n",
    "                train_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.VAL.value:\n",
    "                val_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.TEST.value:\n",
    "                test_samples.append(Sample(image_path, caption, bbox))\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid split: {split}\")\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "def get_image_path(dir_path: str, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "    image_name = next(\n",
    "        image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "    )\n",
    "    path = dir_path + \"images/\" + image_name\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_caption(captions: List[Dict[str, Any]]) -> str:\n",
    "    longest_caption = captions[0]\n",
    "    for caption in captions:\n",
    "        if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "            longest_caption = caption\n",
    "    return longest_caption[\"sent\"]\n",
    "\n",
    "\n",
    "# Bounding boxed converted to format compatible with yolo or torchvision\n",
    "def get_bounding_box(\n",
    "    ann_id: int, instances: Dict[str, Any], bbox_type: BboxType\n",
    ") -> Tensor:\n",
    "    bbox = next(ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id)\n",
    "    bounding_box: Tensor = tensor([])\n",
    "    match bbox_type.name:\n",
    "        case BboxType.XYXY.name:\n",
    "            bounding_box = box_convert(\n",
    "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "            )\n",
    "        case BboxType.XYWH.name:\n",
    "            bounding_box = box_convert(\n",
    "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
    "            )\n",
    "        case BboxType.CXCWH.name:\n",
    "            bounding_box = box_convert(\n",
    "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
    "            )\n",
    "\n",
    "    return bounding_box\n",
    "\n",
    "\n",
    "# If the files already exist, don't preprocess again\n",
    "def preprocess(in_path: str, out_path: str, bbox_type: BboxType) -> None:\n",
    "    if (\n",
    "        os.path.exists(f\"{out_path}train_samples.json\")\n",
    "        and os.path.exists(f\"{out_path}val_samples.json\")\n",
    "        and os.path.exists(f\"{out_path}test_samples.json\")\n",
    "    ):\n",
    "        return\n",
    "    train_samples, val_samples, test_samples = get_samples(in_path, bbox_type)\n",
    "\n",
    "    json.dump(\n",
    "        train_samples,\n",
    "        open(f\"{out_path}train_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "    json.dump(\n",
    "        val_samples,\n",
    "        open(f\"{out_path}val_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "    json.dump(\n",
    "        test_samples,\n",
    "        open(f\"{out_path}test_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(\"../data/raw/refcocog/\", \"../data/processed/\", BboxType.XYWH)\n",
    "\n",
    "    train: List[Sample] = json.load(\n",
    "        open(\"../data/processed/train_samples.json\", \"r\"), object_hook=Sample.fromJSON\n",
    "    )\n",
    "    val: List[Sample] = json.load(\n",
    "        open(\"../data/processed/val_samples.json\", \"r\"), object_hook=Sample.fromJSON\n",
    "    )\n",
    "    test: List[Sample] = json.load(\n",
    "        open(\"../data/processed/test_samples.json\", \"r\"), object_hook=Sample.fromJSON\n",
    "    )\n",
    "    print(len(train), len(val), len(test))\n",
    "    print(train[0].image_path, train[0].caption, train[0].bounding_box.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(\n",
    "    batch: List[Tuple[BatchSample, torch.Tensor]]\n",
    ") -> Tuple[List[BatchSample], torch.Tensor]:\n",
    "    bboxes: List[torch.Tensor] = []\n",
    "    samples: List[BatchSample] = []\n",
    "    for sample, bbox in batch:\n",
    "        samples.append(BatchSample(sample.image, sample.caption))\n",
    "        bboxes.append(bbox)\n",
    "    return samples, torch.stack(bboxes)\n",
    "\n",
    "\n",
    "# Transform image according to CLIP preprocess function\n",
    "# Normalize bounding box coordinates to be independent of image size\n",
    "def transform_sample(\n",
    "    image: Image.Image,\n",
    "    box: Tensor,\n",
    "    augment: bool,\n",
    "    target_size: int = 224,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Same transformation as in the CLIP preprocess function\n",
    "    if augment:\n",
    "        trans = A.Compose(\n",
    "            transforms=[\n",
    "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
    "                A.CenterCrop(\n",
    "                    target_size,\n",
    "                    target_size,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                    max_pixel_value=255.0,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0),\n",
    "                A.GaussianBlur(p=1),\n",
    "                A.PixelDropout(dropout_prob=0.02),\n",
    "                A.Rotate(limit=20),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"coco\", label_fields=[]),\n",
    "        )\n",
    "    else:\n",
    "        trans = A.Compose(\n",
    "            transforms=[\n",
    "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
    "                A.CenterCrop(\n",
    "                    target_size,\n",
    "                    target_size,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                    max_pixel_value=255.0,\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"coco\", label_fields=[]),\n",
    "        )\n",
    "\n",
    "    transformed_sample: Dict[str, Any] = trans(\n",
    "        image=np.array(image), bboxes=box.tolist()\n",
    "    )\n",
    "\n",
    "    bbox_tensor: Tensor = torch.tensor(transformed_sample[\"bboxes\"][0]) / target_size\n",
    "    # print(bbox_tensor)\n",
    "    return transformed_sample[\"image\"], bbox_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "def init_torch(seed: int = 41) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP transformer encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, batch_size: int, clip_ctx_length, embed_dim) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: CLIP = clip.load(\"RN50\", device=self.device)[0]\n",
    "        self.pretrained_model.float()\n",
    "\n",
    "        # Freeze the backbone\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pretrained_model.transformer.register_forward_hook(self.hook_fn())\n",
    "        self.transformer_output: Tensor = torch.empty(\n",
    "            (batch_size, clip_ctx_length, embed_dim),\n",
    "            requires_grad=True,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def forward(self, tokenized_caption: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        out: Tensor = self.pretrained_model.encode_text(tokenized_caption).to(\n",
    "            self.device\n",
    "        )\n",
    "        # .unsqueeze(1)\n",
    "        return (\n",
    "            self.transformer_output,\n",
    "            out,\n",
    "        )\n",
    "\n",
    "    def hook_fn(self) -> Callable[[nn.Module, Tensor, Tensor], None]:\n",
    "        def hook(module: nn.Module, input: Tensor, output: Tensor) -> None:\n",
    "            self.transformer_output = output.permute(1, 0, 2)  # L B D -> B L D\n",
    "\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that gets output for all layes of the backbone\n",
    "# CLIP backbone is a modified ResNet with an attention layer for global pooling\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pretrained_model: ModifiedResNet = clip.load(\"RN50\", device=self.device)[\n",
    "            0\n",
    "        ].visual  # type: ignore\n",
    "        self.pretrained_model.float()\n",
    "        assert isinstance(self.pretrained_model, ModifiedResNet)\n",
    "\n",
    "        # Freeze the backbone\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Register hooks to get the output of all layers\n",
    "        self.layers_outputs: OrderedDict[str, Tensor] = OrderedDict()\n",
    "        self.pretrained_model.layer1.register_forward_hook(self.hook_fn(\"layer1\"))  # type: ignore\n",
    "        self.pretrained_model.layer2.register_forward_hook(self.hook_fn(\"layer2\"))  # type: ignore\n",
    "        self.pretrained_model.layer3.register_forward_hook(self.hook_fn(\"layer3\"))  # type: ignore\n",
    "        self.pretrained_model.layer4.register_forward_hook(self.hook_fn(\"layer4\"))  # type: ignore\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def forward(self, batch: Tensor) -> OrderedDict[str, Tensor]:\n",
    "        out: Tensor = self.pretrained_model(batch)\n",
    "        return self.layers_outputs\n",
    "\n",
    "    def hook_fn(self, layer: str) -> Callable[[nn.Module, Tensor, Tensor], None]:\n",
    "        def hook(module: nn.Module, input: Tensor, output: Tensor) -> None:\n",
    "            # print(f\"Module: {[module for  module in module.modules()]}\")\n",
    "            self.layers_outputs[layer] = output\n",
    "\n",
    "        return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModule(nn.Module):\n",
    "    def __init__(self, emb_dim: int, clip_emb_dim: int, proj_img_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(in_features=clip_emb_dim, out_features=clip_emb_dim),\n",
    "            nn.BatchNorm1d(clip_emb_dim, device=self.device),\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.vis_l4_projection = _conv_layer(\n",
    "            input_dim=clip_emb_dim * 2,\n",
    "            output_dim=clip_emb_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            device=self.device,\n",
    "        )[\n",
    "            :2\n",
    "        ]  # Remove ReLU\n",
    "        self.norm_layer = nn.Sequential(\n",
    "            nn.BatchNorm2d(\n",
    "                clip_emb_dim,\n",
    "                device=self.device,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "        self.vis_l3_projection = _conv_layer(\n",
    "            input_dim=clip_emb_dim + clip_emb_dim,\n",
    "            output_dim=emb_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.vis_l2_projection = _conv_layer(\n",
    "            input_dim=emb_dim + emb_dim,\n",
    "            output_dim=emb_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.aggregation = _conv_layer(\n",
    "            input_dim=clip_emb_dim + emb_dim + emb_dim,\n",
    "            output_dim=emb_dim,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        self.coord_conv = nn.Sequential(\n",
    "            CoordConv(emb_dim + 2, emb_dim),\n",
    "            _conv_layer(\n",
    "                input_dim=emb_dim,\n",
    "                output_dim=emb_dim,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                device=self.device,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, visual_features: OrderedDict[str, Tensor], text_features: Tensor\n",
    "    ) -> Tensor:\n",
    "        visual_l2_features, visual_l3_features, visual_l4_features = (\n",
    "            visual_features[\"layer2\"],\n",
    "            visual_features[\"layer3\"],\n",
    "            visual_features[\"layer4\"],\n",
    "        )\n",
    "        # Visual and text features projection\n",
    "        text_features_proj: Tensor = (\n",
    "            self.text_projection(text_features).unsqueeze(-1).unsqueeze(-1)\n",
    "        )  # B 1024 1 1\n",
    "        visual_l4_features_proj: Tensor = self.vis_l4_projection(\n",
    "            visual_l4_features\n",
    "        )  # B 1024 7 7\n",
    "\n",
    "        # First fusion l4 (B 1024 7 7) and text (B 1024)\n",
    "        fused_l4: Tensor = self.norm_layer(\n",
    "            visual_l4_features_proj * text_features_proj\n",
    "        )  # B 1024 7 7\n",
    "\n",
    "        # Second fusion l3 (B 512 14 14) and l4 (B 1024 7 7)\n",
    "        fused_l4_upsample: Tensor = nn.Upsample(scale_factor=2, mode=\"nearest\")(\n",
    "            fused_l4\n",
    "        )  # B 1024 14 14\n",
    "        cat_features: Tensor = torch.cat([visual_l3_features, fused_l4_upsample], dim=1)\n",
    "        fused_l3: Tensor = self.vis_l3_projection(cat_features)  # B 512 14 14\n",
    "\n",
    "        # Third fusion l2 (B 512 28 28) and l3 (B 512 14 14)\n",
    "        visual_l2_pooling = nn.MaxPool2d(kernel_size=2, stride=2)(\n",
    "            visual_l2_features\n",
    "        )  # B 512 14 14\n",
    "        fused_l2: Tensor = self.vis_l2_projection(\n",
    "            torch.cat([fused_l3, visual_l2_pooling], dim=1)\n",
    "        )  # B 512 14 14\n",
    "\n",
    "        # Aggregate features\n",
    "        cat_visual_features: Tensor = torch.cat(\n",
    "            [fused_l2, fused_l3, fused_l4_upsample], dim=1\n",
    "        )  # B 2048 14 14\n",
    "        aggregated_features: Tensor = self.aggregation(\n",
    "            cat_visual_features\n",
    "        )  # B 512 14 14\n",
    "\n",
    "        # Add coordinate features\n",
    "        final_features: Tensor = self.coord_conv(aggregated_features)  # B 512 14 14\n",
    "        return final_features\n",
    "\n",
    "\n",
    "class CoordConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.conv: nn.Sequential = _conv_layer(\n",
    "            input_dim=in_channels,\n",
    "            output_dim=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def add_coord(self, input: Tensor) -> Tensor:\n",
    "        b, _, h, w = input.size()\n",
    "        x_range = torch.linspace(-1, 1, w, device=self.device)\n",
    "        y_range = torch.linspace(-1, 1, h, device=self.device)\n",
    "\n",
    "        y, x = torch.meshgrid(y_range, x_range)\n",
    "        y = y.expand([b, 1, -1, -1])\n",
    "        x = x.expand([b, 1, -1, -1])\n",
    "        coord_feat = torch.cat([x, y], 1)\n",
    "        input = torch.cat([input, coord_feat], 1)\n",
    "        return input\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.add_coord(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _conv_layer(\n",
    "    input_dim: int,\n",
    "    output_dim: int,\n",
    "    kernel_size: int,\n",
    "    padding: int,\n",
    "    device: device,\n",
    ") -> nn.Sequential:\n",
    "    module = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            device=device,\n",
    "        ),\n",
    "        nn.BatchNorm2d(output_dim, device=device),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    nn.init.xavier_uniform_(module[0].weight)\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        img_size: int,\n",
    "        clip_ctx_length: int,\n",
    "        nheads: int,\n",
    "        nlayers: int,\n",
    "        dim_feedforward: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.d_model = d_model\n",
    "        self.pos_embedding_1d = PositionalEncoding1D(d_model, clip_ctx_length).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.pos_embedding_2d = PositionalEncoding2D(d_model, img_size, img_size).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nheads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                batch_first=True,\n",
    "                norm_first=True,\n",
    "                device=self.device,\n",
    "            ),\n",
    "            num_layers=nlayers,\n",
    "            norm=nn.LayerNorm(d_model, device=self.device),\n",
    "        )\n",
    "        self.reg_token = nn.Parameter(\n",
    "            torch.randn((1, 1, d_model), requires_grad=True)\n",
    "        ).to(self.device)\n",
    "        nn.init.kaiming_normal_(self.reg_token, nonlinearity=\"relu\", mode=\"fan_out\")\n",
    "\n",
    "    def forward(self, vis: Tensor, text: Tensor) -> Tensor:\n",
    "        text_features: Tensor = self.pos_embedding_1d(text)\n",
    "\n",
    "        visual_features: Tensor = self.pos_embedding_2d(vis)\n",
    "\n",
    "        visual_features = visual_features.flatten(2).permute(0, 2, 1)  # B HW D\n",
    "\n",
    "        visual_features = torch.cat(\n",
    "            [self.reg_token.expand((vis.shape[0], -1, -1)), visual_features], dim=1\n",
    "        )\n",
    "        x: Tensor = self.decoder(visual_features, text_features)\n",
    "        reg_token: Tensor = x[:, 0, :]\n",
    "        return reg_token\n",
    "\n",
    "\n",
    "# Positional encodings implemented in separate classes if we want to change them and use learnable positional encodings instead\n",
    "# Dropout added following the original transformer implementation\n",
    "# https://github.com/wzlxjtu/PositionalEncoding2D\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, d_model: int, window_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1).to(self.device)\n",
    "        self.pos_encoding = torch.zeros(window_len, d_model, device=self.device)\n",
    "        position = torch.arange(0, window_len, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            (\n",
    "                torch.arange(0, d_model, 2, dtype=torch.float, device=self.device)\n",
    "                * -(math.log(10000.0) / d_model)\n",
    "            )\n",
    "        )\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "        self.register_buffer(\"text_pos_encoding\", self.pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor) -> Tensor:\n",
    "        out = self.dropout(\n",
    "            token_embedding + self.pos_encoding[: token_embedding.size(1), :]\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "# First half of the encodings are used for the height and the second half for the width\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, d_model: int, width: int, height: int) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1).to(self.device)\n",
    "        self.pe = torch.zeros(d_model, height, width, device=self.device)\n",
    "        # Each dimension use half of d_model\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0.0, d_model, 2, device=self.device)\n",
    "            * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_w = torch.arange(0.0, width, device=self.device).unsqueeze(1)\n",
    "        pos_h = torch.arange(0.0, height, device=self.device).unsqueeze(1)\n",
    "        self.pe[0:d_model:2, :, :] = (\n",
    "            torch.sin(pos_w * div_term)  # H d_model/4\n",
    "            .transpose(0, 1)\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, height, 1)\n",
    "        )  # d_model/4 H H\n",
    "        self.pe[1:d_model:2, :, :] = (\n",
    "            torch.cos(pos_w * div_term)\n",
    "            .transpose(0, 1)\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, height, 1)\n",
    "        )\n",
    "        self.pe[d_model::2, :, :] = (\n",
    "            torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        )  # d_model/4 W W\n",
    "        self.pe[d_model + 1 :: 2, :, :] = (\n",
    "            torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"visual_pos_encoding\", self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg: Config = cfg\n",
    "        embed_dim: int = cfg.model.embed_dim\n",
    "        mlp_hidden_dim: int = cfg.model.mlp_hidden_dim\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.visual_encoder = VisualEncoder()\n",
    "\n",
    "        self.text_encoder = TextEncoder(\n",
    "            cfg.train.batch_size, cfg.model.clip_ctx_length, embed_dim\n",
    "        )\n",
    "\n",
    "        self.fusion_module: FusionModule = FusionModule(\n",
    "            embed_dim, cfg.model.clip_embed_dim, cfg.model.proj_img_size\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.decoder: Decoder = Decoder(\n",
    "            embed_dim,\n",
    "            cfg.model.proj_img_size,\n",
    "            cfg.model.clip_ctx_length,\n",
    "            cfg.model.decoder_heads,\n",
    "            cfg.model.decoder_layers,\n",
    "            cfg.model.decoder_dim_feedforward,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.reg_head: MLP = MLP(\n",
    "            input_dim=embed_dim, output_dim=4, hidden_dim_1=mlp_hidden_dim\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, batch: List[BatchSample]) -> Tensor:\n",
    "        # Get text features\n",
    "        text_sequence, global_text_features = self.text_encoder(\n",
    "            torch.stack([sample.caption for sample in batch]).squeeze(1).to(self.device)\n",
    "        )\n",
    "\n",
    "        # Get image features\n",
    "        visual_features: OrderedDict[str, Tensor] = self.visual_encoder(\n",
    "            torch.stack([sample.image for sample in batch]).to(self.device)\n",
    "        )\n",
    "\n",
    "        # Fuse features\n",
    "        fused_visual_features: Tensor = self.fusion_module(\n",
    "            visual_features, global_text_features\n",
    "        )\n",
    "\n",
    "        # Transformer decoder\n",
    "        reg_token: Tensor = self.decoder(fused_visual_features, text_sequence)\n",
    "\n",
    "        # Regression head\n",
    "        out: Tensor = self.reg_head(reg_token)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim_1: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim_1),\n",
    "            nn.BatchNorm1d(hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_1, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, l1: float, l2: float) -> None:\n",
    "        self.l1_loss = nn.SmoothL1Loss(reduction=\"mean\")\n",
    "        self.giou_loss = generalized_box_iou_loss\n",
    "        self.l1: float = l1\n",
    "        self.l2: float = l2\n",
    "        self.loss: Tensor\n",
    "\n",
    "    # Both bounding boxex tensors are in xyxy format\n",
    "    def compute(self, prediction: Tensor, gt_bbox: Tensor) -> Tensor:\n",
    "        self.loss = self.l1 * self.l1_loss(\n",
    "            gt_bbox, prediction\n",
    "        ) + self.l2 * self.giou_loss(gt_bbox, prediction, reduction=\"mean\")\n",
    "        return self.loss\n",
    "\n",
    "    def to_float(self) -> float:\n",
    "        return self.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters: Iterator[Tuple[str, nn.Parameter]]) -> None:\n",
    "    \"\"\"Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "\n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as\n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow\"\"\"\n",
    "    ave_grads = []\n",
    "    max_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if (p.requires_grad) and (\"bias\" not in n):\n",
    "            if p.grad is None:\n",
    "                print(f\"None gradient for {n}\")\n",
    "            else:\n",
    "                layers.append(n)\n",
    "                ave_grads.append(p.grad.abs().mean().item())\n",
    "                max_grads.append(p.grad.abs().max().item())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=\"k\")\n",
    "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(\n",
    "        [\n",
    "            Line2D([0], [0], color=\"c\", lw=4),\n",
    "            Line2D([0], [0], color=\"b\", lw=4),\n",
    "            Line2D([0], [0], color=\"k\", lw=4),\n",
    "        ],\n",
    "        [\"max-gradient\", \"mean-gradient\", \"zero-gradient\"],\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    epoch: int,\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    loss: Loss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler.OneCycleLR,\n",
    "    device: device,\n",
    "    cfg: Config,\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_list: List[Tensor] = []\n",
    "    iou_list: List[Tensor] = []\n",
    "    acc_50: List[Tensor] = []\n",
    "    acc_75: List[Tensor] = []\n",
    "    acc_90: List[Tensor] = []\n",
    "\n",
    "    for idx, (batch, bbox) in enumerate(tqdm(dataloader, desc=\"Batches\")):\n",
    "        optimizer.zero_grad()\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample = sample.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        # Loss and metrics\n",
    "        out_xyxy = box_convert(out, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
    "        bbox_xyxy = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
    "        batch_loss: Tensor = loss.compute(out_xyxy, bbox_xyxy)\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        plot_grad_flow(model.named_parameters())\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        out_xyxy = out_xyxy.detach()\n",
    "        bbox_xyxy = bbox_xyxy.detach()\n",
    "        batch_iou: Tensor = torch.diagonal(box_iou(out_xyxy, bbox_xyxy))\n",
    "\n",
    "        loss_list.append(batch_loss.detach())\n",
    "        iou_list.append(batch_iou.mean())\n",
    "        acc_50.append(accuracy(batch_iou, 0.5))\n",
    "        acc_75.append(accuracy(batch_iou, 0.75))\n",
    "        acc_90.append(accuracy(batch_iou, 0.9))\n",
    "\n",
    "        if (idx * len(batch)) % 4096 == 0:\n",
    "            report: Dict[str, float] = {\n",
    "                \"Train loss\": batch_loss.detach().item(),\n",
    "                \"Train accurracy\": batch_iou.mean().item(),\n",
    "            }\n",
    "            pprint(f\"Batches: {idx}, {report}\")\n",
    "\n",
    "    return {\n",
    "        Metric.LOSS.value: torch.stack(loss_list).mean().item(),\n",
    "        Metric.IOU.value: torch.stack(iou_list).mean().item(),\n",
    "        Metric.ACCURACY_50.value: torch.stack(acc_50).mean().item(),\n",
    "        Metric.ACCURACY_75.value: torch.stack(acc_75).mean().item(),\n",
    "        Metric.ACCURACY_90.value: torch.stack(acc_90).mean().item(),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    loss: Loss,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    # As accuracy we take the average IoU\n",
    "    model.eval()\n",
    "    loss_list: List[Tensor] = []\n",
    "    iou_list: List[Tensor] = []\n",
    "    acc_50: List[Tensor] = []\n",
    "    acc_75: List[Tensor] = []\n",
    "    acc_90: List[Tensor] = []\n",
    "\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        out = box_convert(out, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
    "        bbox = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
    "\n",
    "        batch_loss: Tensor = loss.compute(out, bbox).detach()\n",
    "        batch_iou: Tensor = torch.diagonal(box_iou(out, bbox)).detach()\n",
    "\n",
    "        loss_list.append(batch_loss)\n",
    "        iou_list.append(batch_iou.mean())\n",
    "        acc_50.append(accuracy(batch_iou, 0.5))\n",
    "        acc_75.append(accuracy(batch_iou, 0.75))\n",
    "        acc_90.append(accuracy(batch_iou, 0.9))\n",
    "\n",
    "    return {\n",
    "        Metric.LOSS.value: torch.stack(loss_list).mean().item(),\n",
    "        Metric.IOU.value: torch.stack(iou_list).mean().item(),\n",
    "        Metric.ACCURACY_50.value: torch.stack(acc_50).mean().item(),\n",
    "        Metric.ACCURACY_75.value: torch.stack(acc_75).mean().item(),\n",
    "        Metric.ACCURACY_90.value: torch.stack(acc_90).mean().item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def accuracy(iou: Tensor, threshold: float) -> Tensor:\n",
    "    return torch.tensor(len(iou[iou >= threshold]) / len(iou))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    device: torch.device,\n",
    "    cfg: Config,\n",
    ") -> Tuple[MetricsLogger, MetricsLogger]:\n",
    "    train_metrics: MetricsLogger = MetricsLogger()\n",
    "    val_metrics: MetricsLogger = MetricsLogger()\n",
    "\n",
    "    # Loss is the weighted sum of the smooth l1 loss and the GIoU\n",
    "    loss_func = Loss(cfg.train.l1, cfg.train.l2)\n",
    "\n",
    "    model: VGModel = VGModel(cfg).train()\n",
    "\n",
    "    # Separate parameters to train\n",
    "    params: List[nn.Parameter] = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        params=[\n",
    "            {\"params\": params, \"lr\": cfg.train.lr, \"weight_decay\": 1e-4},\n",
    "        ]\n",
    "    )\n",
    "    lr_scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=optimizer,\n",
    "        max_lr=[cfg.train.lr],\n",
    "        epochs=cfg.epochs,\n",
    "        steps_per_epoch=len(train_dataloader),\n",
    "    )\n",
    "\n",
    "    if cfg.logging.wandb:\n",
    "        wandb.watch(model, loss_func, log=\"all\", log_freq=100, log_graph=True)\n",
    "\n",
    "    for epoch in tqdm(range(cfg.epochs), desc=\"Epochs\"):\n",
    "        print(\"-------------------- Training --------------------------\")\n",
    "        epoch_train_metrics: Dict[str, float] = train_one_epoch(\n",
    "            epoch=epoch,\n",
    "            dataloader=train_dataloader,\n",
    "            model=model,\n",
    "            loss=loss_func,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=lr_scheduler,\n",
    "            device=device,\n",
    "            cfg=cfg,\n",
    "        )\n",
    "        train_metrics.update_metric(epoch_train_metrics)\n",
    "        print(\"Training metrics at epoch \", epoch)\n",
    "        print(epoch_train_metrics)\n",
    "\n",
    "        # Evaluate on validation set for hyperparameter tuning\n",
    "        print(\"-------------------- Validation ------------------------\")\n",
    "        epoch_val_metrics: Dict[str, float] = validate(\n",
    "            val_dataloader, model, loss_func, device\n",
    "        )\n",
    "        val_metrics.update_metric(epoch_val_metrics)\n",
    "        print(\"Validation metrics at epoch \", epoch)\n",
    "        print(epoch_val_metrics)\n",
    "\n",
    "        # Log metrics to wandb putting train and val metrics together\n",
    "        if cfg.logging.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"Loss\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.LOSS.value],\n",
    "                        \"val\": epoch_val_metrics[Metric.LOSS.value],\n",
    "                    },\n",
    "                    \"Average IOU\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.IOU.value],\n",
    "                        \"val\": epoch_val_metrics[Metric.IOU.value],\n",
    "                    },\n",
    "                    \"Accuracy@50\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.ACCURACY_50.value],\n",
    "                        \"val\": epoch_val_metrics[Metric.ACCURACY_50.value],\n",
    "                    },\n",
    "                    \"Accuracy@75\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.ACCURACY_75.value],\n",
    "                        \"val\": epoch_val_metrics[Metric.ACCURACY_75.value],\n",
    "                    },\n",
    "                    \"Accuracy@90\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.ACCURACY_90.value],\n",
    "                        \"val\": epoch_val_metrics[Metric.ACCURACY_90.value],\n",
    "                    },\n",
    "                },\n",
    "                commit=True,\n",
    "            )\n",
    "\n",
    "        # Save model after each epoch\n",
    "        if cfg.logging.save:\n",
    "            dir: str = cfg.logging.path\n",
    "            if not os.path.exists(dir):\n",
    "                os.makedirs(dir)\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "                    \"loss\": epoch_train_metrics[Metric.LOSS.value],\n",
    "                },\n",
    "                f=f\"{dir}model{epoch}.pth\",\n",
    "            )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return train_metrics, val_metrics\n",
    "\n",
    "\n",
    "def initialize_run(sweep: bool = True) -> None:\n",
    "    config = Config()\n",
    "    if sweep:\n",
    "        wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "        wandb.init(project=\"vgproject\")\n",
    "        wandb_cfg = wandb.config\n",
    "        config.update(wandb_cfg)\n",
    "    else:\n",
    "        if config.logging.wandb:\n",
    "            wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "            wandb.init(project=\"vgproject\", config=config.as_dict())\n",
    "\n",
    "    train_dataset: VGDataset = VGDataset(\n",
    "        dir_path=config.dataset_path,\n",
    "        split=Split.TRAIN,\n",
    "        output_bbox_type=BboxType.XYWH,\n",
    "        augment=True,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Train dataset created. Dataset length \", len(train_dataset))\n",
    "\n",
    "    val_dataset: VGDataset = VGDataset(\n",
    "        dir_path=config.dataset_path,\n",
    "        split=Split.VAL,\n",
    "        output_bbox_type=BboxType.XYWH,\n",
    "        augment=False,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Validation dataset created. Dataset length: \", len(val_dataset))\n",
    "\n",
    "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.train.batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=config.train.batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_metrics, val_metrics = train(train_dataloader, val_dataloader, device, config)\n",
    "\n",
    "    json.dump(train_metrics.metrics, open(\"../train_metrics.json\", \"w\"))\n",
    "    json.dump(val_metrics.metrics, open(\"../val_metrics.json\", \"w\"))\n",
    "\n",
    "    if config.logging.wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    init_torch()\n",
    "    cfg = Config()\n",
    "    if cfg.train.sweep:\n",
    "        sweep_configuration: Dict[str, Any] = json.load(\n",
    "            open(\"../sweep_config.json\", \"r\")\n",
    "        )\n",
    "        sweep: str = wandb.sweep(sweep_configuration, project=\"vgproject\")\n",
    "        wandb.agent(sweep, function=initialize_run, count=10)\n",
    "    else:\n",
    "        initialize_run(cfg.train.sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean environment\n",
    "wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
