{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ftfy regex tqdm ultralytics wandb albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import gzip\n",
    "import html\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, List, Optional, OrderedDict, Tuple, Union\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import ftfy\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pkg_resources as p\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch import Tensor, device, tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops import box_convert, box_iou, generalized_box_iou_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "# Download dataset and save under data/raw/ only if not already downloaded\n",
    "url = \"https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\"\n",
    "if not os.path.exists(\"data/raw/refcocog.tar.gz\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(url=url, output=\"data/raw/\", quiet=False, resume=True)\n",
    "if not os.path.exists(\"data/raw/refcocog/\"):\n",
    "    print(\"Extracting dataset...\")\n",
    "    !tar -xf data/raw/refcocog.tar.gz -C data/raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed samples can be downloaded from Google Drive by executing the following cell. Otherwise, the preprocessing wil be done saving the file only temporarly in the Colab environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download preprocessed dataset\n",
    "url = \"https://drive.google.com/drive/folders/1jaJV40dneOckZn7WHMQyd2jBh7A8534N\"\n",
    "gdown.download_folder(url=url, output=\"data/\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ManuelaCorte/DLProject/decoder/config.json\n",
    "!wget https://raw.githubusercontent.com/ManuelaCorte/DLProject/decoder/sweep_config.json\n",
    "!wget https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\n",
    "!wget https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir src\n",
    "%cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, image_path: str, caption: str, bounding_box: Tensor) -> None:\n",
    "        self.image_path = image_path\n",
    "        self.caption = caption\n",
    "        self.bounding_box = bounding_box\n",
    "\n",
    "    def as_dict(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"image_path\": self.image_path,\n",
    "            \"caption\": self.caption,\n",
    "            \"bounding_box\": self.bounding_box.tolist(),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def fromJSON(json: dict[str, Any]) -> Any:\n",
    "        return Sample(json[\"image_path\"], json[\"caption\"], Tensor(json[\"bounding_box\"]))\n",
    "\n",
    "\n",
    "class BatchSample:\n",
    "    def __init__(self, image: Tensor, caption: Tensor) -> None:\n",
    "        self.image: Tensor = image\n",
    "        self.caption: Tensor = caption\n",
    "\n",
    "    def to(self, device: device | str) -> Any:\n",
    "        return self.__class__(self.image.to(device), self.caption.to(device))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BatchSample(image={self.image.shape}, caption={self.caption.shape})\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Result:\n",
    "    bounding_box: Tensor\n",
    "    score: Tensor\n",
    "\n",
    "\n",
    "# XYXY: top left and bottom right corners\n",
    "# XYWH: top left corner, width and height\n",
    "# CXCWH: center coordinates, width and height\n",
    "@dataclass(frozen=True)\n",
    "class BboxType(Enum):\n",
    "    XYXY = \"xyxy\"\n",
    "    XYWH = \"xywh\"\n",
    "    CXCWH = \"cxcwh\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return super().__str__()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Model:\n",
    "    clip_embed_dim: int\n",
    "    clip_ctx_length: int\n",
    "    embed_dim: int\n",
    "    mlp_hidden_dim: int\n",
    "    img_size: int\n",
    "    proj_img_size: int\n",
    "    decoder_layers: int\n",
    "    decoder_heads: int\n",
    "    decoder_dim_feedforward: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Train:\n",
    "    batch_size: int\n",
    "    lr: float\n",
    "    lr_backbone: float\n",
    "    gamma: float\n",
    "    l1: float\n",
    "    l2: float\n",
    "    sweep: bool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Logging:\n",
    "    path: str\n",
    "    save: bool\n",
    "    resume: bool\n",
    "    wandb: bool\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        cfg: Dict[str, Any] = json.load(open(\"../config.json\", \"r\"))\n",
    "        self.dataset_path: str = cfg[\"dataset_path\"]\n",
    "        self.epochs: int = cfg[\"epochs\"]\n",
    "        self.model = Model(**cfg[\"model\"])\n",
    "        self.train = Train(**cfg[\"train\"])\n",
    "        self.logging = Logging(**cfg[\"logging\"])\n",
    "\n",
    "    def as_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dataset_path\": self.dataset_path,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"model\": self.model.__dict__,\n",
    "            \"train\": self.train.__dict__,\n",
    "        }\n",
    "\n",
    "    # if in other dict there are keys equal to the keys in self, update them\n",
    "    def update(self, other: Dict[str, Any]):\n",
    "        for k, v in other.items():\n",
    "            if k in self.__dict__:\n",
    "                self.__dict__[k] = v\n",
    "            if k in self.model.__dict__:\n",
    "                self.model.__dict__[k] = v\n",
    "            if k in self.train.__dict__:\n",
    "                self.train.__dict__[k] = v\n",
    "            if k in self.logging.__dict__:\n",
    "                self.logging.__dict__[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Metric(Enum):\n",
    "    LOSS = \"loss\"\n",
    "    ACCURACY_50 = \"accuracy\"  # IoU > 0.5 -> 1 else 0\n",
    "    ACCURACY_75 = \"accuracy75\"  # IoU > 0.75 -> 1 else 0\n",
    "    ACCURACY_90 = \"accuracy90\"  # IoU > 0.9 -> 1 else 0\n",
    "    IOU = \"iou\"\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Reduction(Enum):\n",
    "    MEAN = \"mean\"\n",
    "    SUM = \"sum\"\n",
    "    NONE = \"none\"\n",
    "\n",
    "\n",
    "class MetricsLogger:\n",
    "    def __init__(self, metrics: Dict[Metric, List[float]] | None = None) -> None:\n",
    "        self.metrics: Dict[Metric, List[float]] = {}\n",
    "        if metrics is None:\n",
    "            for metric in Metric:\n",
    "                self.metrics[metric] = []\n",
    "        else:\n",
    "            self.metrics: Dict[Metric, List[float]] = metrics\n",
    "\n",
    "    def update_metric(self, metrics: Dict[Metric, float]) -> None:\n",
    "        for metric, value in metrics.items():\n",
    "            self.metrics[metric].append(value)\n",
    "\n",
    "    def get_metric(\n",
    "        self, metric: Metric, red: Reduction = Reduction.NONE\n",
    "    ) -> float | List[float]:\n",
    "        values: List[float] = self.metrics[metric]\n",
    "        match red.name:\n",
    "            case Reduction.MEAN.name:\n",
    "                return sum(values) / len(values)\n",
    "            case Reduction.SUM.name:\n",
    "                return sum(values)\n",
    "            case Reduction.NONE.name:\n",
    "                return values\n",
    "            case _:\n",
    "                raise ValueError(f\"Reduction {red.name} doesn't exists\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        res = \"Metrics:\\n\"\n",
    "        for metric, values in self.metrics.items():\n",
    "            res += f\"{metric.value}: {sum(values) / len(values)}\\n\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def default_bpe():\n",
    "    return os.path.join(globals()[\"_dh\"][0], \"bpe_simple_vocab_16e6.txt.gz\")\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1))\n",
    "        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
    "        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = default_bpe()):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges_lst = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n",
    "        merges_lst = merges_lst[1 : 49152 - 256 - 2 + 1]\n",
    "        merges = [tuple(merge.split()) for merge in merges_lst]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v + \"</w>\" for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(\"\".join(merge))\n",
    "        vocab.extend([\"<|startoftext|>\", \"<|endoftext|>\"])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {\n",
    "            \"<|startoftext|>\": \"<|startoftext|>\",\n",
    "            \"<|endoftext|>\": \"<|endoftext|>\",\n",
    "        }\n",
    "        self.pat = re.compile(\n",
    "            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\\\p{L}]+|[\\\\p{N}]|[^\\s\\\\p{L}\\\\p{N}]+\"\"\",\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token + \"</w>\"\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "            bpe_tokens.extend(\n",
    "                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n",
    "            )\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \"\".join([self.decoder[token] for token in tokens])\n",
    "        text = (\n",
    "            bytearray([self.byte_decoder[c] for c in text])\n",
    "            .decode(\"utf-8\", errors=\"replace\")\n",
    "            .replace(\"</w>\", \" \")\n",
    "        )\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = SimpleTokenizer()\n",
    "\n",
    "\n",
    "def tokenize(\n",
    "    texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False\n",
    ") -> Union[torch.IntTensor, torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Returns the tokenized representation of given input string(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : Union[str, List[str]]\n",
    "        An input string or a list of input strings to tokenize\n",
    "\n",
    "    context_length : int\n",
    "        The context length to use; all CLIP models use 77 as the context length\n",
    "\n",
    "    truncate: bool\n",
    "        Whether to truncate the text in case its encoding is longer than the context length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].\n",
    "    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "\n",
    "    result: Union[torch.IntTensor, torch.LongTensor]\n",
    "    if p.parse_version(torch.__version__) < p.parse_version(\"1.8.0\"):\n",
    "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)  # type: ignore\n",
    "    else:\n",
    "        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)  # type: ignore\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    f\"Input {texts[i]} is too long for context length {context_length}\"\n",
    "                )\n",
    "        result[i, : len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset contains samples with an image with a bounding box and a caption associated with the bounding box.\n",
    "\n",
    "\n",
    "class VGDataset(Dataset[Tuple[BatchSample, Tensor]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        split: Split,\n",
    "        output_bbox_type: BboxType,\n",
    "        augment: bool,\n",
    "        transform: bool = True,\n",
    "        preprocessed: bool = False,\n",
    "        preprocessed_path: str = \"../data/processed/\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dir_path: str = dir_path\n",
    "        self.split: Split = split\n",
    "        self.output_bbox_type: BboxType = output_bbox_type\n",
    "        self.augment: bool = augment\n",
    "        self.transform: bool = transform\n",
    "        self.device: device = torch.device(\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        if preprocessed:\n",
    "            preprocess(dir_path, preprocessed_path, output_bbox_type)\n",
    "            with open(\n",
    "                preprocessed_path + f\"{self.split.value}_samples.json\", \"rb\"\n",
    "            ) as samples:\n",
    "                self.samples: List[Sample] = json.load(\n",
    "                    samples, object_hook=Sample.fromJSON\n",
    "                )\n",
    "        else:\n",
    "            self.samples: List[Sample] = self.get_samples()  # type: ignore\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, ref_id: int) -> Tuple[BatchSample, Tensor]:\n",
    "        # extended_caption: str = f\"find the region that corresponds to the description {self.samples[ref_id].caption}\"\n",
    "        caption: Tensor = tokenize(self.samples[ref_id].caption, truncate=True)  # type: ignore\n",
    "        if self.transform:\n",
    "            image, bbox = transform_sample(\n",
    "                Image.open(self.samples[ref_id].image_path),\n",
    "                self.samples[ref_id].bounding_box,\n",
    "                self.augment,\n",
    "            )\n",
    "        else:\n",
    "            image = read_image(self.samples[ref_id].image_path)\n",
    "            bbox = torch.tensor([self.samples[ref_id].bounding_box])\n",
    "        return BatchSample(image, caption), bbox\n",
    "\n",
    "    def get_samples(self) -> List[Sample]:\n",
    "        with open(self.dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "            self.dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "        ) as refs:\n",
    "            instances = json.load(inst)\n",
    "            references = pickle.load(refs)\n",
    "        samples: List[Sample] = []\n",
    "        for ref in references:\n",
    "            if self.split.value == ref[\"split\"]:\n",
    "                image_path = self.get_image_path(ref[\"image_id\"], instances)\n",
    "                caption = self.get_caption(ref[\"sentences\"])\n",
    "                bbox = self.get_bounding_box(ref[\"ann_id\"], instances)\n",
    "                samples.append(Sample(image_path, caption, bbox))\n",
    "        return samples\n",
    "\n",
    "    def get_image_path(self, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "        image_name = next(\n",
    "            image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "        )\n",
    "        path = self.dir_path + \"images/\" + image_name\n",
    "        return path\n",
    "\n",
    "    def get_caption(self, captions: List[Dict[str, Any]]) -> str:\n",
    "        longest_caption = captions[0]\n",
    "        for caption in captions:\n",
    "            if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "                longest_caption = caption\n",
    "        return f\"find the region that corresponds to the description {longest_caption['sent']}\"\n",
    "\n",
    "    # Bounding boxed converted to format compatible with yolo or torchvision\n",
    "    def get_bounding_box(self, ann_id: int, instances: Dict[str, Any]) -> Tensor:\n",
    "        bbox = next(\n",
    "            ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id\n",
    "        )\n",
    "        bounding_box: Tensor = tensor([])\n",
    "        match self.output_bbox_type.name:\n",
    "            case BboxType.XYXY.name:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "                )\n",
    "            case BboxType.XYWH.name:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
    "                )\n",
    "            case BboxType.CXCWH.name:\n",
    "                bounding_box = box_convert(\n",
    "                    tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
    "                )\n",
    "\n",
    "        return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(\n",
    "    dir_path: str, bbox_type: BboxType\n",
    ") -> Tuple[List[Sample], List[Sample], List[Sample]]:\n",
    "    with open(dir_path + \"annotations/instances.json\", \"r\") as inst, open(\n",
    "        dir_path + \"annotations/refs(umd).p\", \"rb\"\n",
    "    ) as refs:\n",
    "        instances = json.load(inst)\n",
    "        references = pickle.load(refs)\n",
    "    train_samples: List[Sample] = []\n",
    "    val_samples: List[Sample] = []\n",
    "    test_samples: List[Sample] = []\n",
    "    for ref in tqdm(references, desc=f\"Processing dataset\"):\n",
    "        image_path = get_image_path(dir_path, ref[\"image_id\"], instances)\n",
    "        caption = get_caption(ref[\"sentences\"])\n",
    "        bbox = get_bounding_box(ref[\"ann_id\"], instances, bbox_type)\n",
    "        split = ref[\"split\"]\n",
    "        # print(split)\n",
    "        match split:\n",
    "            case Split.TRAIN.value:\n",
    "                train_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.VAL.value:\n",
    "                val_samples.append(Sample(image_path, caption, bbox))\n",
    "            case Split.TEST.value:\n",
    "                test_samples.append(Sample(image_path, caption, bbox))\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid split: {split}\")\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "def get_image_path(dir_path: str, img_id: int, instances: Dict[str, Any]) -> str:\n",
    "    image_name = next(\n",
    "        image[\"file_name\"] for image in instances[\"images\"] if image[\"id\"] == img_id\n",
    "    )\n",
    "    path = dir_path + \"images/\" + image_name\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_caption(captions: List[Dict[str, Any]]) -> str:\n",
    "    longest_caption = captions[0]\n",
    "    for caption in captions:\n",
    "        if len(caption[\"sent\"]) > len(longest_caption[\"sent\"]):\n",
    "            longest_caption = caption\n",
    "    return longest_caption[\"sent\"]\n",
    "\n",
    "\n",
    "# Bounding boxed converted to format compatible with yolo or torchvision\n",
    "def get_bounding_box(\n",
    "    ann_id: int, instances: Dict[str, Any], bbox_type: BboxType\n",
    ") -> Tensor:\n",
    "    bbox = next(ann[\"bbox\"] for ann in instances[\"annotations\"] if ann[\"id\"] == ann_id)\n",
    "    bounding_box: Tensor = tensor([])\n",
    "    match bbox_type.name:\n",
    "        case BboxType.XYXY.name:\n",
    "            bounding_box = box_convert(\n",
    "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYXY.value\n",
    "            )\n",
    "        case BboxType.XYWH.name:\n",
    "            bounding_box = box_convert(\n",
    "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.XYWH.value\n",
    "            )\n",
    "        case BboxType.CXCWH.name:\n",
    "            bounding_box = box_convert(\n",
    "                tensor([bbox]), in_fmt=\"xywh\", out_fmt=BboxType.CXCWH.value\n",
    "            )\n",
    "\n",
    "    return bounding_box\n",
    "\n",
    "\n",
    "# If the files already exist, don't preprocess again\n",
    "def preprocess(in_path: str, out_path: str, bbox_type: BboxType) -> None:\n",
    "    if (\n",
    "        os.path.exists(f\"{out_path}train_samples.json\")\n",
    "        and os.path.exists(f\"{out_path}val_samples.json\")\n",
    "        and os.path.exists(f\"{out_path}test_samples.json\")\n",
    "    ):\n",
    "        return\n",
    "    train_samples, val_samples, test_samples = get_samples(in_path, bbox_type)\n",
    "\n",
    "    json.dump(\n",
    "        train_samples,\n",
    "        open(f\"{out_path}train_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "    json.dump(\n",
    "        val_samples,\n",
    "        open(f\"{out_path}val_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "    json.dump(\n",
    "        test_samples,\n",
    "        open(f\"{out_path}test_samples.json\", \"w\"),\n",
    "        default=Sample.as_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(\"../data/raw/refcocog/\", \"../data/processed/\", BboxType.XYWH)\n",
    "\n",
    "    train: List[Sample] = json.load(\n",
    "        open(\"../data/processed/train_samples.json\", \"r\"), object_hook=Sample.fromJSON\n",
    "    )\n",
    "    val: List[Sample] = json.load(\n",
    "        open(\"../data/processed/val_samples.json\", \"r\"), object_hook=Sample.fromJSON\n",
    "    )\n",
    "    test: List[Sample] = json.load(\n",
    "        open(\"../data/processed/test_samples.json\", \"r\"), object_hook=Sample.fromJSON\n",
    "    )\n",
    "    print(len(train), len(val), len(test))\n",
    "    print(train[0].image_path, train[0].caption, train[0].bounding_box.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(\n",
    "    batch: List[Tuple[BatchSample, torch.Tensor]]\n",
    ") -> Tuple[List[BatchSample], torch.Tensor]:\n",
    "    bboxes: List[torch.Tensor] = []\n",
    "    samples: List[BatchSample] = []\n",
    "    for sample, bbox in batch:\n",
    "        samples.append(BatchSample(sample.image, sample.caption))\n",
    "        bboxes.append(bbox)\n",
    "    return samples, torch.stack(bboxes)\n",
    "\n",
    "\n",
    "# Transform image according to CLIP preprocess function\n",
    "# Normalize bounding box coordinates to be independent of image size\n",
    "def transform_sample(\n",
    "    image: Image.Image,\n",
    "    box: Tensor,\n",
    "    augment: bool,\n",
    "    target_size: int = 224,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Same transformation as in the CLIP preprocess function\n",
    "    if augment:\n",
    "        trans = A.Compose(\n",
    "            transforms=[\n",
    "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
    "                A.CenterCrop(\n",
    "                    target_size,\n",
    "                    target_size,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                    max_pixel_value=255.0,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0),\n",
    "                A.GaussianBlur(p=1),\n",
    "                A.PixelDropout(dropout_prob=0.02),\n",
    "                A.Rotate(limit=20),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"coco\", label_fields=[]),\n",
    "        )\n",
    "    else:\n",
    "        trans = A.Compose(\n",
    "            transforms=[\n",
    "                A.Resize(target_size, target_size, interpolation=cv2.INTER_CUBIC, p=1),\n",
    "                A.CenterCrop(\n",
    "                    target_size,\n",
    "                    target_size,\n",
    "                    always_apply=True,\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                    max_pixel_value=255.0,\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            bbox_params=A.BboxParams(format=\"coco\", label_fields=[]),\n",
    "        )\n",
    "\n",
    "    transformed_sample: Dict[str, Any] = trans(\n",
    "        image=np.array(image), bboxes=box.tolist()\n",
    "    )\n",
    "\n",
    "    bbox_tensor: Tensor = torch.tensor(transformed_sample[\"bboxes\"][0]) / target_size\n",
    "    # print(bbox_tensor)\n",
    "    return transformed_sample[\"image\"], bbox_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "def init_torch(seed: int = 41) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes: int, planes: int, stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(\n",
    "                OrderedDict(\n",
    "                    [\n",
    "                        (\"-1\", nn.AvgPool2d(stride)),\n",
    "                        (\n",
    "                            \"0\",\n",
    "                            nn.Conv2d(\n",
    "                                inplanes,\n",
    "                                planes * self.expansion,\n",
    "                                1,\n",
    "                                stride=1,\n",
    "                                bias=False,\n",
    "                            ),\n",
    "                        ),\n",
    "                        (\"1\", nn.BatchNorm2d(planes * self.expansion)),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.spacial_dim = spacial_dim\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.randn(spacial_dim**2 + 1, embed_dim) / embed_dim**0.5\n",
    "        )\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        # residual\n",
    "        self.connect = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, output_dim, 1),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "        )\n",
    "\n",
    "    def resize_pos_embed(self, pos_embed, input_shape):\n",
    "        pos_h = pos_w = self.spacial_dim\n",
    "        # Skip the first position embedding as it is the CLS token\n",
    "        pos_embed_weight = pos_embed[:, (-1 * pos_h * pos_w) :, :]  # 1 HW C\n",
    "        pos_embed_weight = pos_embed_weight.permute(0, 2, 1)\n",
    "        return pos_embed_weight\n",
    "\n",
    "        # pos_embed_weight = pos_embed_weight.reshape(\n",
    "        #     1, pos_h, pos_w, pos_embed.shape[2]\n",
    "        # ).permute(\n",
    "        #     0, 3, 1, 2\n",
    "        # )  # 1 C H W\n",
    "        # # pos_embed_weight_int = F.interpolate(\n",
    "        # #     pos_embed_weight, size=input_shape, align_corners=False, mode=\"bicubic\"\n",
    "        # # )\n",
    "        # pos_embed_weight = torch.flatten(pos_embed_weight, 2).transpose(1, 2)  # 1 HW C\n",
    "        # return pos_embed_weight.transpose(-2, -1)  # 1 C HW\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        # Residual connection\n",
    "        residual = self.connect(x)\n",
    "\n",
    "        x = x.reshape(B, C, -1)  # B C HW\n",
    "        pos_embed = self.positional_embedding.unsqueeze(0)\n",
    "        pos_embed = self.resize_pos_embed(pos_embed, (H, W))  # 1 C HW\n",
    "        x = x + pos_embed.to(x.dtype)  # B C HW\n",
    "        x = x.permute(2, 0, 1)  # HW B C (seq_len, batch, embed_dim)\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat(\n",
    "                [self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]\n",
    "            ),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        x = x.permute(1, 2, 0).reshape(B, -1, H, W)  # B C H W\n",
    "        x = x + residual\n",
    "        x = F.relu(x, True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: List[int] | Tuple[int, int, int, int],\n",
    "        output_dim: int,\n",
    "        heads: int,\n",
    "        input_resolution: int = 224,\n",
    "        width: int = 64,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, width // 2, kernel_size=3, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            width // 2, width // 2, kernel_size=3, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(\n",
    "            input_resolution // 32, embed_dim, heads, output_dim\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, planes: int, blocks: int, stride: int = 1) -> nn.Sequential:\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            def stem(x: Tensor) -> Tensor:\n",
    "                x = self.relu1(self.bn1(self.conv1(x)))\n",
    "                x = self.relu2(self.bn2(self.conv2(x)))\n",
    "                x = self.relu3(self.bn3(self.conv3(x)))\n",
    "                x = self.avgpool(x)\n",
    "                return x\n",
    "\n",
    "            x = x.type(self.conv1.weight.dtype)\n",
    "            x_stem: Tensor = stem(x)\n",
    "            x1: Tensor = self.layer1(x_stem)\n",
    "            x2: Tensor = self.layer2(x1)\n",
    "            x3: Tensor = self.layer3(x2)\n",
    "            x4: Tensor = self.layer4(x3)\n",
    "\n",
    "        x_pooled: Tensor = self.attnpool(x4)\n",
    "\n",
    "        return (\n",
    "            x2,\n",
    "            x3,\n",
    "            x_pooled,\n",
    "        )  # B 512 H/8 W/8 (B, 1024, H/16, W/16) (B, 1024, H/32, W/32)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        orig_type = input.dtype\n",
    "        ret = super().forward(input.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model: int, n_head: int, attn_mask: Optional[torch.Tensor] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "                    (\"gelu\", QuickGELU()),\n",
    "                    (\"c_proj\", nn.Linear(d_model * 4, d_model)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = (\n",
    "            self.attn_mask.to(dtype=x.dtype, device=x.device)\n",
    "            if self.attn_mask is not None\n",
    "            else None\n",
    "        )\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        # vision\n",
    "        image_resolution: int,\n",
    "        vision_layers: Tuple[int, int, int, int],\n",
    "        vision_width: int,\n",
    "        # text\n",
    "        context_length: int,\n",
    "        vocab_size: int,\n",
    "        transformer_width: int,\n",
    "        transformer_heads: int,\n",
    "        transformer_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        vision_heads = vision_width * 32 // 64\n",
    "        self.visual = ModifiedResNet(\n",
    "            layers=vision_layers,\n",
    "            output_dim=embed_dim,\n",
    "            heads=vision_heads,\n",
    "            input_resolution=image_resolution,\n",
    "            width=vision_width,\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask(),\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.empty(self.context_length, transformer_width)\n",
    "        )\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features**-0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "                # nn.init.uniform_(self.visual.attnpool.connect.weight)\n",
    "\n",
    "            for resnet_block in [\n",
    "                self.visual.layer1,\n",
    "                self.visual.layer2,\n",
    "                self.visual.layer3,\n",
    "                self.visual.layer4,\n",
    "            ]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width**-0.5) * (\n",
    "            (2 * self.transformer.layers) ** -0.5\n",
    "        )\n",
    "        attn_std = self.transformer.width**-0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, text) -> Tuple[Tensor, Tensor]:\n",
    "        x = self.token_embedding(text).type(\n",
    "            self.dtype\n",
    "        )  # B L D (batch size, sequence length=77, embed dim=512)\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # L B D\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # B L D\n",
    "        x = self.ln_final(x).type(self.dtype)  # B L D\n",
    "\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # Embedding corresponding to the higher value token for each element in the sequence (eot_token)\n",
    "        global_repr: Tensor = (\n",
    "            x[torch.arange(x.shape[0]), text.argmax(dim=-1), :] @ self.text_projection\n",
    "        )  # [B, d_model=1024] @ [d_model, D] = [B, D]\n",
    "\n",
    "        # Transformer output, EOT embeddings\n",
    "        return x, global_repr\n",
    "\n",
    "    def forward(self, image, text) -> Tuple[Tensor, Tensor]:\n",
    "        image_features: Tensor = self.encode_image(image)[2]\n",
    "        text_features: Tensor = self.encode_text(text)[1]\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "\n",
    "def convert_weights(model: nn.Module):\n",
    "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp16(l: nn.Module) -> None:\n",
    "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            l.weight.data = l.weight.data.half()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data = l.bias.data.half()\n",
    "\n",
    "        if isinstance(l, nn.MultiheadAttention):\n",
    "            for attr in [\n",
    "                *[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]],\n",
    "                \"in_proj_bias\",\n",
    "                \"bias_k\",\n",
    "                \"bias_v\",\n",
    "            ]:\n",
    "                tensor = getattr(l, attr)\n",
    "                if tensor is not None:\n",
    "                    tensor.data = tensor.data.half()\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\"]:\n",
    "            if hasattr(l, name):\n",
    "                attr_module: nn.Module = getattr(l, name)\n",
    "                if attr_module is not None:\n",
    "                    attr_module.data = attr_module.data.half()\n",
    "\n",
    "    model.apply(_convert_weights_to_fp16)\n",
    "\n",
    "\n",
    "def build_model(state_dict: dict[str, Any]):\n",
    "    \"\"\"Build a CLIP model from a state dict\"\"\"\n",
    "    counts: List[int] = [\n",
    "        len(\n",
    "            set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))\n",
    "        )\n",
    "        for b in [1, 2, 3, 4]\n",
    "    ]\n",
    "    vision_layers: Tuple[int, int, int, int] = tuple(counts)  # type: ignore\n",
    "    vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "    output_width = round(\n",
    "        (state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5\n",
    "    )\n",
    "    assert (\n",
    "        output_width**2 + 1\n",
    "        == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "    )\n",
    "    image_resolution = output_width * 32\n",
    "\n",
    "    embed_dim: int = state_dict[\"text_projection\"].shape[1]\n",
    "    context_length: int = state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size: int = state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width: int = state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads: int = transformer_width // 64\n",
    "    transformer_layers: int = len(\n",
    "        set(\n",
    "            k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = CLIP(\n",
    "        embed_dim,\n",
    "        image_resolution,\n",
    "        vision_layers,\n",
    "        vision_width,\n",
    "        context_length,\n",
    "        vocab_size,\n",
    "        transformer_width,\n",
    "        transformer_heads,\n",
    "        transformer_layers,\n",
    "    )\n",
    "\n",
    "    # Add the new positional embedding to the state dict\n",
    "    state_dict.update(\n",
    "        model.visual.attnpool.connect.state_dict(prefix=\"visual.attnpool.connect.\")\n",
    "    )\n",
    "\n",
    "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "        if key in state_dict:\n",
    "            del state_dict[key]\n",
    "\n",
    "    convert_weights(model)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModule(nn.Module):\n",
    "    def __init__(self, emb_dim: int, clip_emb_dim: int, proj_img_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(in_features=clip_emb_dim, out_features=clip_emb_dim),\n",
    "            nn.BatchNorm1d(clip_emb_dim, device=self.device),\n",
    "        ).to(self.device)\n",
    "        self.vis_l4_projection = _conv_layer(\n",
    "            input_dim=clip_emb_dim,\n",
    "            output_dim=clip_emb_dim,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            device=self.device,\n",
    "        )[\n",
    "            :2\n",
    "        ]  # Remove ReLU\n",
    "        self.norm_layer = nn.Sequential(\n",
    "            nn.BatchNorm2d(\n",
    "                clip_emb_dim,\n",
    "                device=self.device,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "        self.vis_l3_projection = _conv_layer(\n",
    "            input_dim=clip_emb_dim + clip_emb_dim,\n",
    "            output_dim=emb_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.vis_l2_projection = _conv_layer(\n",
    "            input_dim=emb_dim + emb_dim,\n",
    "            output_dim=emb_dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.aggregation = _conv_layer(\n",
    "            input_dim=clip_emb_dim + emb_dim + emb_dim,\n",
    "            output_dim=emb_dim,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, visual_features: Tuple[Tensor, Tensor, Tensor], text_features: Tensor\n",
    "    ) -> Tensor:\n",
    "        visual_l2_features, visual_l3_features, visual_l4_features = visual_features\n",
    "        # Visual and text features projection\n",
    "        text_features_proj: Tensor = (\n",
    "            self.text_projection(text_features).unsqueeze(-1).unsqueeze(-1)\n",
    "        )  # B 1024 1 1\n",
    "        visual_l4_features_proj: Tensor = self.vis_l4_projection(\n",
    "            visual_l4_features\n",
    "        )  # B 1024 7 7\n",
    "\n",
    "        # First fusion l4 (B 1024 7 7) and text (B 1024)\n",
    "        fused_l4: Tensor = self.norm_layer(\n",
    "            visual_l4_features_proj * text_features_proj\n",
    "        )  # B 1024 7 7\n",
    "\n",
    "        # Second fusion l3 (B 512 14 14) and l4 (B 1024 7 7)\n",
    "        fused_l4_upsample: Tensor = nn.Upsample(scale_factor=2, mode=\"nearest\")(\n",
    "            fused_l4\n",
    "        )  # B 1024 14 14\n",
    "        cat_features: Tensor = torch.cat([visual_l3_features, fused_l4_upsample], dim=1)\n",
    "        fused_l3: Tensor = self.vis_l3_projection(cat_features)  # B 512 14 14\n",
    "\n",
    "        # Third fusion l2 (B 512 28 28) and l3 (B 512 14 14)\n",
    "        visual_l2_pooling: Tensor = nn.MaxPool2d(kernel_size=2, stride=2)(\n",
    "            visual_l2_features\n",
    "        )  # B 512 14 14\n",
    "        fused_l2: Tensor = self.vis_l2_projection(\n",
    "            torch.cat([fused_l3, visual_l2_pooling], dim=1)\n",
    "        )  # B 512 14 14\n",
    "\n",
    "        # Aggregate features\n",
    "        cat_visual_features: Tensor = torch.cat(\n",
    "            [fused_l2, fused_l3, fused_l4_upsample], dim=1\n",
    "        )  # B 2048 14 14\n",
    "        aggregated_features: Tensor = self.aggregation(\n",
    "            cat_visual_features\n",
    "        )  # B 512 14 14\n",
    "        # TODO: Add spatial coords?\n",
    "        return aggregated_features\n",
    "\n",
    "\n",
    "def _conv_layer(\n",
    "    input_dim: int,\n",
    "    output_dim: int,\n",
    "    kernel_size: int,\n",
    "    padding: int,\n",
    "    device: device,\n",
    ") -> nn.Sequential:\n",
    "    module = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            device=device,\n",
    "        ),\n",
    "        nn.BatchNorm2d(output_dim, device=device),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    nn.init.xavier_uniform_(module[0].weight)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        img_size: int,\n",
    "        clip_ctx_length: int,\n",
    "        nheads: int,\n",
    "        nlayers: int,\n",
    "        dim_feedforward: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.d_model = d_model\n",
    "        self.pos_embedding_1d = PositionalEncoding1D(d_model, clip_ctx_length).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.pos_embeddinf_2d = PositionalEncoding2D(d_model, img_size, img_size).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nheads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                batch_first=True,\n",
    "                norm_first=True,  # Less prone to vanishing gradients??\n",
    "                device=self.device,\n",
    "            ),\n",
    "            num_layers=nlayers,\n",
    "            norm=nn.LayerNorm(d_model, device=self.device),\n",
    "        )\n",
    "        self.reg_token = nn.Parameter(torch.randn(1, 1, d_model)).to(self.device)\n",
    "        nn.init.kaiming_normal_(self.reg_token, nonlinearity=\"relu\", mode=\"fan_out\")\n",
    "\n",
    "    def forward(self, vis: Tensor, text: Tensor) -> Tensor:\n",
    "        text_features: Tensor = self.pos_embedding_1d(text)\n",
    "\n",
    "        visual_features: Tensor = self.pos_embeddinf_2d(vis)\n",
    "\n",
    "        visual_features = visual_features.flatten(2).permute(0, 2, 1)  # B HW D\n",
    "\n",
    "        visual_features = torch.cat(\n",
    "            [self.reg_token.expand((vis.shape[0], -1, -1)), visual_features], dim=1\n",
    "        )\n",
    "        x = self.decoder(visual_features, text_features)\n",
    "        return x[:, 0, :]\n",
    "\n",
    "\n",
    "# Positional encodings implemented in separate classes if we want to change them and use learnable positional encodings instead\n",
    "# Dropout added following the original transformer implementation\n",
    "# https://github.com/wzlxjtu/PositionalEncoding2D\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, d_model: int, window_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1).to(self.device)\n",
    "        self.pos_encoding = torch.zeros(window_len, d_model, device=self.device)\n",
    "        position = torch.arange(0, window_len, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            (\n",
    "                torch.arange(0, d_model, 2, dtype=torch.float, device=self.device)\n",
    "                * -(math.log(10000.0) / d_model)\n",
    "            )\n",
    "        )\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "        self.register_buffer(\"text_pos_encoding\", self.pos_encoding)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, token_embedding: Tensor) -> Tensor:\n",
    "        out = self.dropout(\n",
    "            token_embedding + self.pos_encoding[: token_embedding.size(1), :]\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "# First half of the encodings are used for the height and the second half for the width\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, d_model: int, width: int, height: int) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1).to(self.device)\n",
    "        self.pe = torch.zeros(d_model, height, width, device=self.device)\n",
    "        # Each dimension use half of d_model\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0.0, d_model, 2, device=self.device)\n",
    "            * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_w = torch.arange(0.0, width, device=self.device).unsqueeze(1)\n",
    "        pos_h = torch.arange(0.0, height, device=self.device).unsqueeze(1)\n",
    "        self.pe[0:d_model:2, :, :] = (\n",
    "            torch.sin(pos_w * div_term)  # H d_model/4\n",
    "            .transpose(0, 1)\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, height, 1)\n",
    "        )  # d_model/4 H H\n",
    "        self.pe[1:d_model:2, :, :] = (\n",
    "            torch.cos(pos_w * div_term)\n",
    "            .transpose(0, 1)\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, height, 1)\n",
    "        )\n",
    "        self.pe[d_model::2, :, :] = (\n",
    "            torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        )  # d_model/4 W W\n",
    "        self.pe[d_model + 1 :: 2, :, :] = (\n",
    "            torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        )\n",
    "        self.register_buffer(\"visual_pos_encoding\", self.pe)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg: Config = cfg\n",
    "        embed_dim: int = cfg.model.embed_dim\n",
    "        mlp_hidden_dim: int = cfg.model.mlp_hidden_dim\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.clip: CLIP = torch.jit.load(\"../RN50.pt\", map_location=\"cpu\").eval()\n",
    "        self.pretrained_model: CLIP = build_model(self.clip.state_dict()).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.pretrained_model.float()\n",
    "        del self.clip\n",
    "\n",
    "        # Freeze all clip parameters except the attention pooling layer\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.pretrained_model.visual.attnpool.requires_grad_(True)\n",
    "\n",
    "        self.fusion_module: FusionModule = FusionModule(\n",
    "            embed_dim, cfg.model.clip_embed_dim, cfg.model.proj_img_size\n",
    "        ).to(self.device)\n",
    "        self.decoder: Decoder = Decoder(\n",
    "            embed_dim,\n",
    "            cfg.model.proj_img_size,\n",
    "            cfg.model.clip_ctx_length,\n",
    "            cfg.model.decoder_heads,\n",
    "            cfg.model.decoder_layers,\n",
    "            cfg.model.decoder_dim_feedforward,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.reg_head: MLP = MLP(\n",
    "            input_dim=embed_dim, output_dim=4, hidden_dim_1=mlp_hidden_dim\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, batch: List[BatchSample]) -> Tensor:\n",
    "        # Get text features\n",
    "        text_sequence, global_text_features = self.pretrained_model.encode_text(\n",
    "            torch.stack([sample.caption for sample in batch]).squeeze(1).to(self.device)\n",
    "        )\n",
    "\n",
    "        # Get image features\n",
    "        visual_features = self.pretrained_model.encode_image(\n",
    "            torch.stack([sample.image for sample in batch]).to(self.device)\n",
    "        )\n",
    "\n",
    "        # Fuse features\n",
    "        fused_visual_features: Tensor = self.fusion_module(\n",
    "            visual_features, global_text_features\n",
    "        )\n",
    "\n",
    "        # Transformer decoder\n",
    "        reg_token = self.decoder(fused_visual_features, text_sequence)\n",
    "\n",
    "        # Regression head\n",
    "        out = self.reg_head(reg_token)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim_1: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_1, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, l1: float, l2: float) -> None:\n",
    "        self.l1_loss = nn.SmoothL1Loss(reduction=\"mean\")\n",
    "        self.giou_loss = generalized_box_iou_loss\n",
    "        self.l1: float = l1\n",
    "        self.l2: float = l2\n",
    "        self.loss: Tensor\n",
    "\n",
    "    def compute(self, prediction: Tensor, gt_bbox: Tensor) -> Tensor:\n",
    "        prediction = box_convert(prediction, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
    "        gt_bbox = box_convert(gt_bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
    "        self.loss = self.l1 * self.l1_loss(\n",
    "            gt_bbox, prediction\n",
    "        ) + self.l2 * self.giou_loss(gt_bbox, prediction, reduction=\"mean\")\n",
    "        return self.loss\n",
    "\n",
    "    def to_float(self) -> float:\n",
    "        return self.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    epoch: int,\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    loss: Loss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler.OneCycleLR,\n",
    "    device: device,\n",
    "    cfg: Config,\n",
    ") -> Dict[Metric, float]:\n",
    "    model.train()\n",
    "    loss_list: List[Tensor] = []\n",
    "    iou_list: List[Tensor] = []\n",
    "    acc_50: List[Tensor] = []\n",
    "    acc_75: List[Tensor] = []\n",
    "    acc_90: List[Tensor] = []\n",
    "\n",
    "    for idx, (batch, bbox) in enumerate(tqdm(dataloader, desc=\"Batches\")):\n",
    "        optimizer.zero_grad()\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample = sample.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        # Loss and metrics\n",
    "        batch_loss: Tensor = loss.compute(out, bbox)\n",
    "\n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        out = box_convert(out, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
    "        bbox = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
    "        batch_iou: Tensor = torch.diagonal(box_iou(out, bbox))\n",
    "\n",
    "        loss_list.append(batch_loss.detach())\n",
    "        iou_list.append(batch_iou.mean())\n",
    "        acc_50.append(accuracy(batch_iou, 0.5))\n",
    "        acc_75.append(accuracy(batch_iou, 0.75))\n",
    "        acc_90.append(accuracy(batch_iou, 0.9))\n",
    "\n",
    "        if (idx * len(batch)) % 4096 == 0:\n",
    "            report: Dict[str, float] = {\n",
    "                \"Train loss\": batch_loss.detach().item(),\n",
    "                \"Train accurracy\": batch_iou.mean().item(),\n",
    "            }\n",
    "            pprint(f\"Batches: {idx}, {report}\")\n",
    "\n",
    "    return {\n",
    "        Metric.LOSS: torch.stack(loss_list).mean().item(),\n",
    "        Metric.IOU: torch.stack(iou_list).mean().item(),\n",
    "        Metric.ACCURACY_50: torch.stack(acc_50).mean().item(),\n",
    "        Metric.ACCURACY_75: torch.stack(acc_75).mean().item(),\n",
    "        Metric.ACCURACY_90: torch.stack(acc_90).mean().item(),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    model: VGModel,\n",
    "    loss: Loss,\n",
    "    device: torch.device,\n",
    ") -> Dict[Metric, float]:\n",
    "    # As accuracy we take the average IoU\n",
    "    model.eval()\n",
    "    loss_list: List[Tensor] = []\n",
    "    iou_list: List[Tensor] = []\n",
    "    acc_50: List[Tensor] = []\n",
    "    acc_75: List[Tensor] = []\n",
    "    acc_90: List[Tensor] = []\n",
    "\n",
    "    for batch, bbox in tqdm(dataloader, desc=\"Batches\"):\n",
    "        # Move to gpu\n",
    "        for sample in batch:\n",
    "            sample.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out: Tensor = model(batch)\n",
    "\n",
    "        out = box_convert(out, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
    "        bbox = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\").detach()\n",
    "\n",
    "        batch_loss: Tensor = loss.compute(out, bbox).detach()\n",
    "        batch_iou: Tensor = torch.diagonal(box_iou(out, bbox)).detach()\n",
    "\n",
    "        loss_list.append(batch_loss)\n",
    "        iou_list.append(batch_iou.mean())\n",
    "        acc_50.append(accuracy(batch_iou, 0.5))\n",
    "        acc_75.append(accuracy(batch_iou, 0.75))\n",
    "        acc_90.append(accuracy(batch_iou, 0.9))\n",
    "\n",
    "    return {\n",
    "        Metric.LOSS: torch.stack(loss_list).mean().item(),\n",
    "        Metric.IOU: torch.stack(iou_list).mean().item(),\n",
    "        Metric.ACCURACY_50: torch.stack(acc_50).mean().item(),\n",
    "        Metric.ACCURACY_75: torch.stack(acc_75).mean().item(),\n",
    "        Metric.ACCURACY_90: torch.stack(acc_90).mean().item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def accuracy(iou: Tensor, threshold: float) -> Tensor:\n",
    "    return torch.tensor(len(iou[iou >= threshold]) / len(iou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]],\n",
    "    device: torch.device,\n",
    "    cfg: Config,\n",
    ") -> Tuple[MetricsLogger, MetricsLogger]:\n",
    "    train_metrics: MetricsLogger = MetricsLogger()\n",
    "    val_metrics: MetricsLogger = MetricsLogger()\n",
    "\n",
    "    # Loss is the weighted sum of the smooth l1 loss and the GIoU\n",
    "    loss_func = Loss(cfg.train.l1, cfg.train.l2)\n",
    "\n",
    "    model: VGModel = VGModel(cfg).train()\n",
    "\n",
    "    # Separate parameters to train\n",
    "    backbone_params: List[nn.Parameter] = [\n",
    "        p for p in model.pretrained_model.parameters() if p.requires_grad\n",
    "    ]\n",
    "\n",
    "    # All parameters except the backbone\n",
    "    non_frozen_params: List[nn.Parameter] = []\n",
    "    non_frozen_params.extend(model.fusion_module.parameters())\n",
    "    non_frozen_params.extend(model.decoder.parameters())\n",
    "    non_frozen_params.extend(model.reg_head.parameters())\n",
    "    print(len(backbone_params), len(non_frozen_params))\n",
    "    optimizer = optim.AdamW(\n",
    "        params=[\n",
    "            {\"params\": backbone_params, \"lr\": cfg.train.lr_backbone, \"weight_decay\": 0},\n",
    "            {\"params\": non_frozen_params, \"lr\": cfg.train.lr, \"weight_decay\": 1e-4},\n",
    "        ]\n",
    "    )\n",
    "    lr_scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=optimizer,\n",
    "        max_lr=[cfg.train.lr_backbone, cfg.train.lr],\n",
    "        epochs=cfg.epochs,\n",
    "        steps_per_epoch=len(train_dataloader),\n",
    "    )\n",
    "\n",
    "    if cfg.logging.wandb:\n",
    "        wandb.watch(model, loss_func, log=\"all\", log_freq=100, log_graph=True)\n",
    "\n",
    "    for epoch in tqdm(range(cfg.epochs), desc=\"Epochs\"):\n",
    "        print(\"-------------------- Training --------------------------\")\n",
    "        epoch_train_metrics: Dict[Metric, float] = train_one_epoch(\n",
    "            epoch=epoch,\n",
    "            dataloader=train_dataloader,\n",
    "            model=model,\n",
    "            loss=loss_func,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=lr_scheduler,\n",
    "            device=device,\n",
    "            cfg=cfg,\n",
    "        )\n",
    "        train_metrics.update_metric(epoch_train_metrics)\n",
    "        print(\"Training metrics at epoch \", epoch)\n",
    "        pprint(epoch_train_metrics)\n",
    "\n",
    "        # Evaluate on validation set for hyperparameter tuning\n",
    "        print(\"-------------------- Validation ------------------------\")\n",
    "        epoch_val_metrics: Dict[Metric, float] = validate(\n",
    "            val_dataloader, model, loss_func, device\n",
    "        )\n",
    "        val_metrics.update_metric(epoch_val_metrics)\n",
    "        print(\"Validation metrics at epoch \", epoch)\n",
    "        pprint(epoch_val_metrics)\n",
    "\n",
    "        # Log metrics to wandb putting train and val metrics together\n",
    "        if cfg.logging.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"Loss\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.LOSS],\n",
    "                        \"val\": epoch_val_metrics[Metric.LOSS],\n",
    "                    },\n",
    "                    \"Average IOU\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.IOU],\n",
    "                        \"val\": epoch_val_metrics[Metric.IOU],\n",
    "                    },\n",
    "                    \"Accuracy@50\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.ACCURACY_50],\n",
    "                        \"val\": epoch_val_metrics[Metric.ACCURACY_50],\n",
    "                    },\n",
    "                    \"Accuracy@75\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.ACCURACY_75],\n",
    "                        \"val\": epoch_val_metrics[Metric.ACCURACY_75],\n",
    "                    },\n",
    "                    \"Accuracy@90\": {\n",
    "                        \"train\": epoch_train_metrics[Metric.ACCURACY_90],\n",
    "                        \"val\": epoch_val_metrics[Metric.ACCURACY_90],\n",
    "                    },\n",
    "                },\n",
    "                commit=True,\n",
    "            )\n",
    "\n",
    "        # Save model after each epoch\n",
    "        if cfg.logging.save:\n",
    "            dir: str = cfg.logging.path\n",
    "            if not os.path.exists(dir):\n",
    "                os.makedirs(dir)\n",
    "            torch.save(\n",
    "                obj={\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "                    \"loss\": epoch_train_metrics[Metric.LOSS],\n",
    "                },\n",
    "                f=f\"{dir}model{epoch}.pth\",\n",
    "            )\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return train_metrics, val_metrics\n",
    "\n",
    "\n",
    "def initialize_run(sweep: bool = True) -> None:\n",
    "    config = Config()\n",
    "    if sweep:\n",
    "        wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "        wandb.init(project=\"vgproject\")\n",
    "        wandb_cfg = wandb.config\n",
    "        config.update(wandb_cfg)\n",
    "    else:\n",
    "        if config.logging.wandb:\n",
    "            wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "            wandb.init(project=\"vgproject\", config=config.as_dict())\n",
    "\n",
    "    train_dataset: VGDataset = VGDataset(\n",
    "        dir_path=config.dataset_path,\n",
    "        split=Split.TRAIN,\n",
    "        output_bbox_type=BboxType.XYWH,\n",
    "        augment=True,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Train dataset created. Dataset length \", len(train_dataset))\n",
    "\n",
    "    val_dataset: VGDataset = VGDataset(\n",
    "        dir_path=config.dataset_path,\n",
    "        split=Split.VAL,\n",
    "        output_bbox_type=BboxType.XYWH,\n",
    "        augment=False,\n",
    "        preprocessed=True,\n",
    "    )\n",
    "    print(\"Validation dataset created. Dataset length: \", len(val_dataset))\n",
    "\n",
    "    train_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.train.batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_dataloader: DataLoader[Tuple[BatchSample, Tensor]] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=config.train.batch_size,\n",
    "        collate_fn=custom_collate,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_metrics, val_metrics = train(train_dataloader, val_dataloader, device, config)\n",
    "\n",
    "    json.dump(train_metrics.metrics, open(\"../train_metrics.json\", \"w\"))\n",
    "    json.dump(val_metrics.metrics, open(\"../val_metrics.json\", \"w\"))\n",
    "\n",
    "    if config.logging.wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    init_torch()\n",
    "    cfg = Config()\n",
    "    if cfg.train.sweep:\n",
    "        sweep_configuration: Dict[str, Any] = json.load(\n",
    "            open(\"../sweep_config.json\", \"r\")\n",
    "        )\n",
    "        sweep: str = wandb.sweep(sweep_configuration, project=\"vgproject\")\n",
    "        wandb.agent(sweep, function=initialize_run, count=10)\n",
    "    else:\n",
    "        initialize_run(cfg.train.sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
