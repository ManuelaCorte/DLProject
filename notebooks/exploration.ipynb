{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# file = open('/home/manuela/Downloads/refcocog-20230409T170358Z-001/refcocog/annotations/refs(umd).p', 'rb')\n",
    "# obj = pickle.load(file)\n",
    "\n",
    "path = '../data/raw/refcocog/'\n",
    "obj = pd.read_pickle(path + 'annotations/refs(umd).p')\n",
    "refs = json.loads(json.dumps(obj))\n",
    "\n",
    "with open(path + 'annotations/instances.json', 'r') as file:\n",
    "    inst = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = pd.DataFrame(inst['images'])\n",
    "# print(images.head())\n",
    "print(refs[1])\n",
    "print(inst['info'])\n",
    "print(inst['images'][0])\n",
    "print(inst['annotations'][0])\n",
    "print(inst['categories'])\n",
    "print(inst['licenses'])\n",
    "splits = [refs[i]['split'] for i in range(len(refs))]\n",
    "print(set(splits))\n",
    "for i in inst:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of images in images, annotations and references\n",
    "images = set()\n",
    "for image in inst['images']:\n",
    "    images.add(image['id'])\n",
    "\n",
    "img_ann = set()\n",
    "for ann in inst['annotations']:\n",
    "    img_ann.add(ann['image_id'])\n",
    "\n",
    "for ann in inst['annotations']:\n",
    "    img_ann.add(ann['image_id'])\n",
    "\n",
    "# img_refs = set(refs['image_id'].unique())\n",
    "\n",
    "print(len(images), len(img_ann),  len(inst['annotations']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms as t\n",
    "from torchvision.ops import box_convert\n",
    "ref = refs[157]\n",
    "\n",
    "# print(ref['file_name']) remove the annotation id from name\n",
    "bbox = [ann['bbox'] for ann in inst['annotations'] if ann['id'] == ref['ann_id']][0]\n",
    "# print(bbox)\n",
    "image_name = '_'.join(ref['file_name'].split('_')[0:-1]) + '.jpg'\n",
    "print(image_name)\n",
    "print(ref['split'])\n",
    "box = box_convert(torch.tensor([bbox]), in_fmt='xywh', out_fmt='xyxy')\n",
    "# print(box.shape)\n",
    "print(ref['sentences'])\n",
    "image = read_image(path + 'images/'+image_name)\n",
    "res = draw_bounding_boxes(image, boxes=box, colors='red', width=5)\n",
    "t.ToPILImage()(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible class structure for dataset sample\n",
    "class Example():\n",
    "    def __init__(self, id:str, split:str, sentences:list, image_id:str, file_name:str, category:dict, bbox:torch.Tensor) -> None:\n",
    "        self.id = id\n",
    "        self.split = split\n",
    "        self.sentences = sentences\n",
    "        self.image_id = image_id\n",
    "        self.file_name = file_name\n",
    "        self.category = category\n",
    "        self.bbox = bbox\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'''\n",
    "[\n",
    "    Id: {self.id},\n",
    "    Split: {self.split},\n",
    "    Sentences: {self.sentences},\n",
    "    Image: {self.file_name},\n",
    "    Category: {self.category['name']}\n",
    "]'''\n",
    "\n",
    "# example:Example = Example()\n",
    "examples = []\n",
    "for ref in refs[0:100]:\n",
    "    sentences = []\n",
    "    for sentence in ref['sentences']:\n",
    "        sentences.append({\n",
    "            'tokens': sentence['tokens'], \n",
    "            'sent': sentence['sent']\n",
    "        })\n",
    "    file_name = '_'.join(ref['file_name'].split('_')[0:-1]) + '.jpg'\n",
    "    category = [{'category_id': cat['id'], 'name': cat['name']} for cat in inst['categories'] if cat['id'] == ref['category_id']][0]\n",
    "    bbox = [ann['bbox'] for ann in inst['annotations'] if ann['id'] == ref['ann_id']][0]\n",
    "    box = box_convert(torch.tensor([bbox]), in_fmt='xywh', out_fmt='xyxy')\n",
    "    \n",
    "    e = Example(ref['ref_id'], \n",
    "                    ref['split'], \n",
    "                    sentences,\n",
    "                    ref['image_id'],\n",
    "                    file_name,\n",
    "                    category,\n",
    "                    box)\n",
    "    examples.append(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check images width and height to find the best one for resizing for batches\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "for image in inst['images']:\n",
    "    widths.append(image['width'])\n",
    "    heights.append(image['height'])\n",
    "\n",
    "print(f'Max width: {max(widths)}, Min width: {min(widths)}, Avg width: {sum(widths)/len(widths)}')\n",
    "print(f'Max height: {max(heights)}, Min height: {min(heights)}, Avg height: {sum(heights)/len(heights)}')\n",
    "\n",
    "splits = [ref['split'] for ref in refs]\n",
    "from collections import Counter\n",
    "print(Counter(splits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing with dependency graphs\n",
    "sents = []\n",
    "for ref in refs[0:200]:\n",
    "    sents.append(ref['sentences'][0]['sent'])\n",
    "    # print(ref['sentences'][0]['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_phrase(doc):\n",
    "    for token in doc:\n",
    "        # Mainly -ing verbs\n",
    "        if(\"acl\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            end = subtree[0].i\n",
    "            sent = doc[0:end]\n",
    "            if len(sent) > 1:\n",
    "                return sent\n",
    "\n",
    "    # subject which/that something\n",
    "    for token in doc:\n",
    "        if(\"relcl\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            end = subtree[0].i\n",
    "            sent = doc[0:end]\n",
    "            if len(sent) > 1:\n",
    "                return sent\n",
    "            \n",
    "    # Subjects\n",
    "    for token in doc:\n",
    "        if (\"subj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            sent = doc[start:end]\n",
    "            if len(sent) > 1:\n",
    "                return sent\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing with name chunks\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "for sentence in sents:\n",
    "    doc = nlp(sentence)\n",
    "    # subject_phrase = get_subject_phrase(doc)\n",
    "    print(sentence)\n",
    "    # print(subject_phrase)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(chunk.text)\n",
    "    print('----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"a lady pouring wine in the bigger glass\")\n",
    "displacy.render(doc, style='dep')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for image blurring\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms as T\n",
    "from torchvision.ops import box_convert\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ref = refs[51]\n",
    "\n",
    "# print(ref['file_name']) remove the annotation id from name\n",
    "bbox = [ann['bbox'] for ann in inst['annotations'] if ann['id'] == ref['ann_id']][0]\n",
    "# print(bbox)\n",
    "image_name = '_'.join(ref['file_name'].split('_')[0:-1]) + '.jpg'\n",
    "print(image_name)\n",
    "box = box_convert(torch.tensor([bbox]), in_fmt='xywh', out_fmt='xyxy')\n",
    "# print(box.shape)\n",
    "print(ref['sentences'][0]['sent'])\n",
    "original = read_image(path + 'images/'+image_name)\n",
    "\n",
    "# cv2.imshow('image', original)\n",
    "blurred = T.GaussianBlur(25, 20)(original)\n",
    "# blurred = cv2.GaussianBlur(original, (25,25), 0)\n",
    "box = box.squeeze(0).numpy().astype(int)\n",
    "blurred[:,box[1]:box[3], box[0]:box[2]] = original[:,box[1]:box[3], box[0]:box[2]]\n",
    "print(blurred.shape)\n",
    "plt.imshow(blurred.permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for bounding boxes resizing\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.ops import box_convert \n",
    "import torch\n",
    "\n",
    "\n",
    "def transform_sample(image: Image.Image , box: List[int], target_size: int = 224)-> Tuple[Tensor, Tensor]:\n",
    "    x, y = image.size\n",
    "\n",
    "    x_scale = target_size / x\n",
    "    y_scale = target_size / y\n",
    "\n",
    "    trans = T.Compose(transforms=[\n",
    "        T.Resize((target_size, target_size)),\n",
    "        T.CenterCrop(target_size),\n",
    "        T.PILToTensor()\n",
    "    ])\n",
    "    image_tensor: Tensor = trans(image)\n",
    "\n",
    "    # original frame as named values\n",
    "    box_tensor = box_convert(torch.tensor([box]), in_fmt='xywh', out_fmt='xyxy').squeeze(0)\n",
    "    xmin, ymin, xmax, ymax = box_tensor\n",
    "\n",
    "    xmin = int(np.round(xmin * x_scale))\n",
    "    ymin = int(np.round(ymin* y_scale))\n",
    "    xmax = int(np.round(xmax * x_scale))\n",
    "    ymax = int(np.round(ymax * y_scale))\n",
    "\n",
    "    bbox_tensor: Tensor = torch.tensor([[xmin, ymin, xmax, ymax]])\n",
    "    return image_tensor, bbox_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "ref = refs[157]\n",
    "\n",
    "image_name: str = '_'.join(ref['file_name'].split('_')[0:-1]) + '.jpg'\n",
    "image: Image.Image = Image.open(path + 'images/'+image_name)\n",
    "bbox:List[int] = [ann['bbox'] for ann in inst['annotations'] if ann['id'] == ref['ann_id']][0]\n",
    "\n",
    "image_tensor, bbox_tensor = transform_sample(image, bbox, target_size=224)\n",
    "# bbox_tensor = torch.as_tensor(data=bbox_tensor)\n",
    "result = draw_bounding_boxes(image_tensor, bbox_tensor, width=2, colors=['red'])\n",
    "plt.imshow(result.permute(1,2,0))\n",
    "print(ref['sentences'][0]['sent'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
